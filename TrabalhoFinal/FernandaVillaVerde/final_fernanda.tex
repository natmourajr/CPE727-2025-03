\documentclass[10pt,aspectratio=169]{beamer}

% =======================
% Pacotes básicos
% =======================
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

\title[Predição de Mudança de Estado]{Utilização de RNNs para Predição de mudança de estado em compressor industrial}
\author{Fernanda Mickosz Villa Verde}
\institute{CPE727 -- Deep Learning}
\date{\today}

\begin{document}

% =======================
% Capa
% =======================
\begin{frame}
  \titlepage
\end{frame}

% =======================
% Sumário
% =======================
\begin{frame}{Sumário}
  \tableofcontents
\end{frame}

% =======================
% Seção: Motivação
% =======================
\section{Motivação e Objetivos}

\begin{frame}{Motivação e Objetivos do trabalho}
\noindent \textbf{Motivação:}
    \begin{itemize}
      \item Compressores industriais são ativos críticos (segurança, disponibilidade, custo).
      \item Mudanças de regime operacional (estados) afetam: Desempenho energético; Risco de falha; Planejamento de manutenção.
        \item Predizer \textbf{quando} ocorrerá a próxima mudança de estado é útil para:
        \begin{itemize}
          \item Detecção precoce de desvios;
          \item Replanejamento operacional;
          \item Suporte à decisão para operadores.
        \end{itemize}
    \end{itemize}

\noindent \textbf{Objetivos do trabalho:}

    \begin{itemize}
    
  \item Modelar séries temporais industriais (compressores) com amostragem irregular, dados faltantes e alta dimensionalidade utilizando arquiteturas recorrentes.

      \item Predição dos \textbf{estados} em janelas futuras;
    \item Predição do \textbf{tempo até a próxima mudança de estado}.
        \end{itemize}
   \noindent \textbf{Comparar arquiteturas:}
        \begin{itemize}
          \item Modelos DEEP, LSTM, GRU, BiLSTM e BiGRU com GRU fuser, todos com otimização Adam;
          \item Variações de treinamento (warmup, scheduler, RAdam, AdamW, variational dropout, difusão, log-likelihood e layernorm).
        \end{itemize}


\end{frame}

% =======================
% Seção: Base de dados e pré-processamento
% =======================
\section{Base de dados e pré-processamento}


\begin{frame}{Base de dados e pré-processamento}

\begin{columns}[t]
  
  \begin{column}[t]{0.48\textwidth}
    \textbf{Base de dados}

    \vspace{0.2cm}
    Cognite — sinais reais de sensores de um compressor industrial offshore.

    \vspace{0.3cm}
    \centering
    \includegraphics[width=\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/FernandaVillaVerde/12series.png}

    \vspace{0.2cm}
    \scriptsize
    \textit{Exemplo de séries reais utilizadas no estudo (12 sinais simultâneos).}
  \end{column}

  \begin{column}[t]{0.48\textwidth}
    \textbf{Pré-processamento}

    \vspace{0.2cm}
    \begin{itemize}
      \item Estados operacionais:
        \begin{itemize}
          \item Estados originais: 6 (\(0\) a \(5\));
          \item Estados finais: 3 (normal, alerta, falha).
        \end{itemize}

      \item Normalização robusta;
      \item Amostragem a cada 5 minutos;
      \item Janelas temporais:
        \begin{itemize}
          \item \(L = 40\) amostras (\(\approx 3\)h20min);
          \item Deslocamento de 40 amostras (janelas não sobrepostas).
        \end{itemize}
    \end{itemize}
  \end{column}

\end{columns}
\end{frame}


% =======================
% Seção: Formulação Teórica
% =======================
\section{Formulação do Problema}

\begin{frame}{Definição do problema}
  \begin{itemize}
    \item Série multivariada: \(\mathbf{x}_t \in \mathbb{R}^{d}\), \(t = 1,\dots,T\), com \(d = 11\) sensores.
    \item Estados discretos: \(s_t \in \{0,1,4\}\) (após fusão dos 6 estados iniciais).
    \item Para cada janela \(\mathbf{X} = (x_{t-L+1}, \dots, x_t)\), queremos:
      \begin{itemize}
        \item Predizer a distribuição sobre estados futuros:
          \[
            p(s_{t+\Delta} \mid \mathbf{X})
          \]
        \item Predizer o tempo até a próxima mudança de estado:
          \[
            \hat{\tau} = f_{\theta}(\mathbf{X})
          \]
      \end{itemize}
    \item Problema multi-tarefa:
      \begin{itemize}
        \item Classificação de estados (estado futuro) + regressão de tempo (tempo até a mudança).
      \end{itemize}
  \end{itemize}
\vspace{0.3cm}
  \noindent A partir de janelas de histórico de sensores, o modelo aprende simultaneamente a prever o próximo regime operacional e o tempo restante até a próxima mudança de estado.
\end{frame}


\begin{frame}{Função de custo}
\small
\begin{itemize}
  \item Função de custo multi-tarefa:
  \[
  \mathcal{L} =
  \lambda_m \mathcal{L}_{\text{miss}}
  + \lambda_v \mathcal{L}_{\text{vae}}
  \]


\small
\[
\mathcal{L}_{\text{miss}} = \mathrm{BCE}(m,\hat{m})
\quad\text{com}\quad
\hat{m} = \sigma(f_{\text{miss}}(h))
\]
\(\mathcal{L}_{\text{miss}}\): aprendizado explícito do padrão de dados ausentes.

  \[
  \mathcal{L}_{\text{vae-}}
  =
  \mathbb{E}_{q(z_\tau|\mathbf{X})}
  \big[ - \log p(\tau \mid z_\tau) \big]
  +
  \beta\,\mathrm{KL}\big(q(z_\tau) \,\|\, \mathcal{N}(0,I)\big)
  \]

  \(\mathcal{L}_{\text{vae}}\): \textbf{modelagem probabilística do tempo até falha}, penalizando mudanças próximas e calibrando incerteza.
   
  \item Peso maior atribuído a instantes próximos à mudança de estado:
  \[
  \log p(\tau \mid z_\tau)
  \;\propto\;
  w_t \cdot \|\hat{\tau} - \tau\|^2,
  \quad
  w_t \gg 1
  \]

  \item Permite estimar \textbf{incerteza}, \textbf{intervalos de confiança} e
  \textbf{cobertura probabilística}.
\end{itemize}
\end{frame}


\begin{frame}{Arquiteturas avaliadas}
  \begin{itemize}
    \item \textbf{TSDF\_DEEP}:
      \begin{itemize}
        \item Modelo feedforward sobre janelas (baseline não-recorrente).
      \end{itemize}
    \item \textbf{TSDF\_LSTM}:
      \begin{itemize}
        \item RNN LSTM unidirecional;
        \item Versão \textbf{BiLSTM}.
      \end{itemize}
    \item \textbf{TSDF\_GRU}:
      \begin{itemize}
        \item RNN GRU unidirecional;
        \item Versão \textbf{BiGRU}.
      \end{itemize}
  \end{itemize}

  
\end{frame}


\begin{frame}{Arquitetura baseline: TSDF\_DEEP (feedforward)}
  \begin{itemize}
    \item Entrada: janela temporal \(\mathbf{X} = (x_1,\dots,x_L)\),
          com \(x_t \in \mathbb{R}^{11}\) e máscara \(m_t \in \{0,1\}^{11}\).
    \item Codificação inicial por MLP (por timestep):
      \[
        e_t = \phi\left( W_e [x_t \,\Vert\, m_t] + b_e \right),
        \quad t = 1,\dots,L
      \]
      onde \(\phi(\cdot)\) é uma ativação não linear (ReLU/GELU).
    \item Incorporação explícita do tempo:
      \[
        e'_t = \mathrm{MLP}\big([e_t \,\Vert\, t_t]\big)
      \]
      com \(t_t\) o timestamp normalizado do instante \(t\).
    \item Representação latente da janela:
      \begin{itemize}
        \item Obtida independentemente por timestep (sem estados recorrentes);
        \item Integra informação temporal apenas via timestamps explícitos.
      \end{itemize}
    \item Cabeças de saída (multi-tarefa):
      \begin{align*}
        \hat{x}_t &= f_{\theta}(e'_t) \quad \text{(reconstrução / difusão)} \\
        \hat{\tau} &= g_{\theta}(e'_t) \quad \text{(tempo até mudança de estado)}
      \end{align*}
    \item Treinamento:
      \begin{itemize}
        \item Otimizador Adam;
        \item Mesmas funções de perda usadas nos modelos recorrentes.
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Arquitetura 1: LSTM}
  \begin{itemize}
    \item Entrada: janela temporal \(\mathbf{X} = (x_1,\dots,x_L)\), \(x_t \in \mathbb{R}^{11}\).
    \item Codificação temporal com LSTM unidirecional:
      \[
        (h_t, c_t) = \mathrm{LSTM}(x_t, h_{t-1}, c_{t-1}), \quad t = 1,\dots,L
      \]
    \item Representação da janela:
      \[
        z = h_L \in \mathbb{R}^{H}
      \]
    \item Cabeças de saída (multi-tarefa):
      \begin{align*}
        \hat{y}_{\text{state}} &= \mathrm{softmax}(W_s z + b_s) \\
        \hat{\tau} &= W_\tau z + b_\tau
      \end{align*}
      
    \item Otimização:
      \begin{itemize}
        \item Otimizador Adam, minimizando perda conjunta (classificação de estado + regressão de tempo).
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Arquitetura 2: GRU}
  \begin{itemize}
     \item Mesma entrada:
  \[
    \mathbf{X} = (x_1, \dots, x_L), \quad x_t \in \mathbb{R}^{11}
  \]

  \item Codificação temporal recorrente:
  \[
    \{h_t\}_{t=1}^L = \mathrm{GRU}(\mathbf{X})
  \]

  \item Representação latente da janela:
  \[
    z = h_L \in \mathbb{R}^{H}
  \]
    \item Cabeças de saída:
      \begin{align*}
        \hat{y}_{\text{state}} &= \mathrm{softmax}(W_s z + b_s) \\
        \hat{\tau} &= W_\tau z + b_\tau
      \end{align*}
    \item Otimização:
      \begin{itemize}
        \item Otimizador Adam, com os mesmos hiperparâmetros do baseline Deep (learning rate, batch, etc.).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Arquitetura 3: BiGRU + GRU fuser}
  \begin{itemize}
    \item Codificação da janela com BiGRU:
      \[
        h_t^{\rightarrow}, h_t^{\leftarrow} = \mathrm{BiGRU}(x_t, h_{t-1}^{\rightarrow}, h_{t+1}^{\leftarrow})
      \]
      \[
        z_k = [h_L^{\rightarrow} \,\Vert\, h_1^{\leftarrow}] \in \mathbb{R}^{2H}
      \]
      onde \(z_k\) é o embedding da \(k\)-ésima janela.
    \item Fusão temporal entre janelas com GRU fuser:
      \[
        m_k = \mathrm{GRU}_{\text{fuser}}(z_k, m_{k-1}), \quad k = 1,\dots,K
      \]
      \[
        m_K \in \mathbb{R}^{H_f} \text{ resume a história recente de janelas.}
      \]
    \item Cabeças de saída a partir de \(m_K\):
      \begin{align*}
        \hat{y}_{\text{state}} &= \mathrm{softmax}(W_s m_K + b_s) \\
        \hat{\tau} &= W_\tau m_K + b_\tau
      \end{align*}
    \item Otimização:
      \begin{itemize}
        \item Adam sobre todos os parâmetros (BiGRU + GRU fuser + cabeças).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Arquitetura 4: BiLSTM + GRU fuser}
  \begin{itemize}
    \item Codificação da janela com BiLSTM:
      \[
        (h_t^{\rightarrow}, c_t^{\rightarrow}), (h_t^{\leftarrow}, c_t^{\leftarrow})
          = \mathrm{BiLSTM}(x_t,\dots)
      \]
      \[
        z_k = [h_L^{\rightarrow} \,\Vert\, h_1^{\leftarrow}] \in \mathbb{R}^{2H}
      \]
    \item Fusão temporal entre janelas com GRU fuser:
      \[
        m_k = \mathrm{GRU}_{\text{fuser}}(z_k, m_{k-1}), \quad k = 1,\dots,K
      \]
    \item Cabeças de saída:
      \begin{align*}
        \hat{y}_{\text{state}} &= \mathrm{softmax}(W_s m_K + b_s) \\
        \hat{\tau} &= W_\tau m_K + b_\tau
      \end{align*}
    \item Otimização:
      \begin{itemize}
        \item Treino conjunto com Adam, mesma função de perda multi-tarefa.
      \end{itemize}
  \end{itemize}
\end{frame}


% =======================
% Seção: Arquiteturas
% =======================
\section{Arquiteturas e Variações}

\begin{frame}{Variações de treinamento - Otimização}
    

  \begin{itemize}
    \item \textbf{BiLSTM  + Warmup \& Scheduler}
      \begin{itemize}
        \item Aumenta gradualmente a taxa de aprendizado no início do treino, 
              evitando instabilidades e melhorando a convergência inicial.
        \item \texttt{warmup\_steps = 50}, \texttt{min\_lr\_factor = 0.01}
      \end{itemize}

    \item \textbf{BiLSTM + RAdam}
      \begin{itemize}
        \item Reduz a variância adaptativa do Adam nos primeiros passos, 
              tornando o treinamento mais estável.
        \item \texttt{optimizer\_name = 'radam'}
      \end{itemize}
 

     \end{itemize}
\end{frame}


\begin{frame}{Variações de treinamento — Regularização}
 \begin{itemize}

    \item \textbf{BiLSTM + Variational Dropout}
      \begin{itemize}
        \item Regularização explícita no tempo,
              reduzindo overfitting sem quebrar dependências temporais.
        \item \texttt{variational\_dropout = 0.2}
      \end{itemize}

    \item \textbf{BiLSTM + Difusão (missingness)}
      \begin{itemize}
        \item Regularização implícita via objetivos probabilísticos,
              forçando robustez a dados ausentes.
        \item \texttt{lam = [0., 0.0, 0.0, 0.05, 0.0, 0.95]}, \texttt{rebuild=True}
      \end{itemize}

  
    \item \textbf{BiLSTM + LayerNorm}
      \begin{itemize}
        \item Estabiliza as ativações internas,
              com efeito regularizante indireto.
        \item \texttt{use\_layernorm = True}
      \end{itemize}

    \item \textbf{BiLSTM + AdamW (Weight Decay)}
      \begin{itemize}
        \item Regularização explícita dos pesos,
              melhorando generalização.
        \item \texttt{optimizer\_name = 'adamw'}, \texttt{weight\_decay = 1e-4}
      \end{itemize}

  \end{itemize}
\end{frame}




% =======================
% Seção: Configuração Experimental
% =======================
\section{Configuração Experimental}

\begin{frame}{Configuração Experimental}
  \begin{itemize}
    \item Divisão treino/validação/teste: 60\% / 20\% / 20\%.
    \item Treinamento por 500 épocas, batch size 256.
    \item Paciênica de 50 épocas para early stopping.
    \item Função de perda multi-tarefa conforme descrito.
    \item Otimizador: Adam (exceto variações).
    \item Taxa de aprendizado inicial: \(1 \times 10^{-3}\).
    \item Early stopping baseado em MSE de validação.
    \item Avaliação final no conjunto de teste.
  \end{itemize}
\end{frame}
% =======================
% Seção: Resultados
% =======================
\section{Resultados}


\begin{frame}{Figuras de Mérito utilizadas}
  \begin{itemize}
   \item \textbf{NELBO} (Negative Evidence Lower Bound):
\begin{itemize}
  \item Loss probabilística minimizada no treinamento;
  \item Maximiza implicitamente a verossimilhança (ELBO);
  \item Balanceia reconstrução e regularização latente (KL).
\end{itemize}

    \item \textbf{MSE} (Mean Squared Error):
      \begin{itemize}
        \item Mede o erro médio quadrático na predição do tempo até a próxima mudança;
        \item Avalia precisão pontual das estimativas.
      \end{itemize}

    \item \textbf{NLL} (Negative Log-Likelihood):
      \begin{itemize}
        \item Avalia a qualidade probabilística das previsões;
        \item Penaliza incertezas mal calibradas (sub ou superestimação).
      \end{itemize}

    \item \textbf{Cobertura 90\%} (\texttt{cov\_90}):
      \begin{itemize}
        \item Fração de observações reais contidas no intervalo preditivo de 90\%;
        \item Mede a calibração do modelo.
      \end{itemize}

    \item \textbf{Largura do intervalo 90\%} (\texttt{width\_90}):
      \begin{itemize}
        \item Largura média dos intervalos preditivos de 90\%;
        \item Reflete o compromisso entre precisão e incerteza (sharpness).
      \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}{Resultados quantitativos}
  \begin{table}[h]
    \centering
    \small
\begin{tabular}{lccccc}
\toprule
\textbf{Modelo} & \textbf{NELBO $\downarrow$} & \textbf{MSE (micro) $\downarrow$} 
& \textbf{NLL $\downarrow$} & \textbf{Cov90 $\rightarrow$ 0.90} & \textbf{Width90 $\downarrow$} \\
\midrule
BiLSTM RAdam & \textbf{0.023} & 0.002 $\pm$ 0.000 & 0.023 & 0.946 & 0.270 \\
BiLSTM Warmup/Sched. & 0.033 & 0.002 $\pm$ 0.000 & 0.033 & 0.946 & 0.270 \\
LSTM & 0.064 & 0.002 $\pm$ 0.000 & 0.063 & 0.937 & 0.270 \\
BiLSTM & 0.150 & 0.002 $\pm$ 0.000 & 0.150 & 0.920 & 0.270 \\
BiGRU & 0.203 & 0.001 $\pm$ 0.000 & 0.202 & 0.875 & 0.270 \\
BiLSTM AdamW & 0.230 & 0.001 $\pm$ 0.001 & 0.229 & 0.866 & 0.270 \\
BiLSTM Difusão & 0.329 & 0.002 $\pm$ 0.001 & 0.328 & 0.839 & 0.270 \\
BiLSTM LayerNorm & 0.383 & 0.054 $\pm$ 0.001 & 0.332 & 0.839 & 0.270 \\
GRU & 0.464 & 0.003 $\pm$ 0.001 & 0.463 & 0.812 & 0.270 \\
BiLSTM VarDrop 0.2 & 0.992 & 0.002 $\pm$ 0.001 & 0.991 & 0.705 & 0.270 \\
DEEP & 8016.760 & 0.819 $\pm$ 0.011 & 33.711 & 0.250 & 0.270 \\
\bottomrule
\end{tabular}

    \caption{Comparação de desempenho entre modelos e variações. }
  \end{table}
\end{frame}

\begin{frame}{Curva de treinamento — Modelo \textbf{BiLSTM RAdam}}
\begin{itemize}
      \item Curva de NELBO bastante oscilatória, refletindo o
              desbalanceamento entre grupos e a raridade de mudanças de estado;
        \item Após as primeiras dezenas de épocas, o modelo passa a operar
              em um patamar de NELBO baixo, com mínimos recorrentes;
        \item A melhor época (\textbf{epoch 54}) atinge o menor NELBO entre as
              épocas avaliadas, sendo o melhor compromisso entre ajuste e
              complexidade para a tarefa de mudança de estado.   
\end{itemize}

  \centering
  \begin{figure}
  \includegraphics[width=0.70\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/FernandaVillaVerde/nelbo.png}
  \caption{Evolução do NELBO durante o treinamento do modelo
  BiLSTM RAdam.}
  \end{figure}
\end{frame}
% =======================
% Seção: Discussão e Conclusões
% =======================
\section{Discussão e Conclusões}

\begin{frame}{Discussão dos resultados — Mudança de estado}
  \begin{itemize}
    \item \textbf{Qualidade probabilística}:
      \begin{itemize}
        \item O \textbf{BiLSTM RAdam} apresenta o menor NELBO,
              indicando melhor ajuste probabilístico global;
        \item Bons valores de NLL e MSE micro confirmam alta precisão
              na predição de mudanças de estado raras.
      \end{itemize}

    \item \textbf{Calibração das incertezas}:
      \begin{itemize}
        \item Cobertura próxima de 0.90 nos melhores modelos
              indica boa calibração probabilística;
        \item Width90 constante sugere intervalos estáveis e comparáveis
              entre arquiteturas.
      \end{itemize}

    \item \textbf{Impacto arquitetural}:
      \begin{itemize}
        \item Modelos bidirecionais superam LSTM/GRU unidirecionais;
      \item Em cenários fortemente desbalanceados, o principal desafio é a
      \textbf{otimização sob gradientes raros}, e não o overfitting;
\item Otimizadores adaptativos (RAdam, warmup) estabilizam o treinamento
      e facilitam a captura de eventos de mudança raros, enquanto
      regularização excessiva tende a diluir esses sinais.

      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Conclusões — Mudança de estado}
  \begin{itemize}
    \item A predição de mudanças de estado é um problema
          fortemente desbalanceado e sensível à calibração probabilística.

    \item Arquiteturas recorrentes probabilísticas são essenciais:
      \begin{itemize}
        \item Capturam dependências temporais longas;
        \item Modelam explicitamente incertezas associadas ao evento de transição.
      \end{itemize}

    \item O \textbf{BiLSTM RAdam} é o modelo mais adequado:
      \begin{itemize}
        \item Menor NELBO entre os modelos avaliados;
        \item Boa calibração (Cov90 $\approx$ 0.95) e baixo erro micro.
      \end{itemize}

    \item Esses resultados indicam potencial para:
      \begin{itemize}
        \item Detecção antecipada de mudanças de regime;
        \item Suporte a sistemas de monitoramento e alerta industrial.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Referências}

\begin{itemize}
  \item Kingma, D. P.; Welling, M. \textit{Auto-Encoding Variational Bayes}. ICLR, 2014.
  \item Hochreiter, S.; Schmidhuber, J. \textit{Long Short-Term Memory}. Neural Computation, 1997.
  \item Cho et al. \textit{Learning Phrase Representations using RNN Encoder–Decoder}. EMNLP, 2014.
  \item Ho, J.; Jain, A.; Abbeel, P. \textit{Denoising Diffusion Probabilistic Models}. NeurIPS, 2020.
  \item Che et al. \textit{Recurrent Neural Networks for Multivariate Time Series with Missing Values}. Sci Rep, 2018.
\end{itemize}

\end{frame}


\end{document}
