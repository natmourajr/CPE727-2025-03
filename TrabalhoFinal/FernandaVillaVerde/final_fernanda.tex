\documentclass[10pt,aspectratio=169]{beamer}

% =======================
% Pacotes básicos
% =======================
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{positioning}
% -------- Pipeline base: Change Prediction --------
\newcommand{\pipelinebase}[1]{%
\centering
\scriptsize
\begin{tikzpicture}[
  x=1cm, y=1cm,
  node distance=1.6cm and 1.2cm,
  every node/.style={transform shape},
  scale=0.85
]

\tikzstyle{box}=[%
  draw, thick, rounded corners=3pt,
  minimum width=2.4cm, minimum height=0.75cm,
  align=center, fill=gray!10
];
\tikzstyle{enc}=[box, fill=blue!12];
\tikzstyle{dec}=[box, fill=orange!18];
\tikzstyle{extra}=[box, fill=green!12];
\tikzstyle{circ}=[draw, thick, circle, minimum size=0.6cm];
\tikzstyle{arrow}=[->, thick];

% ---- Entrada ----
\node[box] (x) {
  Entrada\\
  $(\mathbf{x},\,\mathbf{c})_{1:T}$
};

% ---- Encoder ----
\node[enc, right=of x] (emb) {
  Encoder\\
  (MLP / Embedding)
};

% ---- RNN Encoder ----
\node[enc, right=of emb] (rnn) {
  RNN Encoder\\
  $\mu_z,\ \log\sigma_z^2$
};

% ---- Amostragem latente ----
\node[circ, right=of rnn] (z) {$z$};

% ---- Decoder ----
\node[dec, right=of z] (dec) {
  Decoder\\
  $\hat{c},\ \log\sigma_c^2$
};

% ---- Variação arquitetural ----
\node[extra, below=1.1cm of rnn, text width=4.4cm] (var) {#1};
\draw[arrow, dashed] (var) -- (rnn);

% ---- Setas ----
\draw[arrow] (x)   -- (emb);
\draw[arrow] (emb) -- (rnn);
\draw[arrow] (rnn) -- (z);
\draw[arrow] (z)   -- (dec);

\end{tikzpicture}%
}

\title[Predição de Mudança de Estado]{Utilização de RNNs para Predição de mudança de estado em compressor industrial}
\author{Fernanda Mickosz Villa Verde}
\institute{CPE727 -- Deep Learning}
\date{\today}

\begin{document}

% =======================
% Capa
% =======================
\begin{frame}
  \titlepage
\end{frame}

% =======================
% Sumário
% =======================
\begin{frame}{Sumário}
  \tableofcontents
\end{frame}

% =======================
% Seção: Motivação
% =======================
\section{Motivação e Objetivos}

\begin{frame}{Motivação e Objetivos do trabalho}
\noindent \textbf{Motivação:}
    \begin{itemize}
      \item Compressores industriais são ativos críticos (segurança, disponibilidade, custo).
      \item Mudanças de regime operacional (estados) afetam: Desempenho energético; Risco de falha; Planejamento de manutenção.
        \item Predizer \textbf{quando} ocorrerá a próxima mudança de estado é útil para:
        \begin{itemize}
          \item Detecção precoce de desvios;
          \item Replanejamento operacional;
          \item Suporte à decisão para operadores.
        \end{itemize}
    \end{itemize}

\noindent \textbf{Objetivos do trabalho:}

    \begin{itemize}
    
  \item Modelar séries temporais industriais (compressores) com amostragem irregular, dados faltantes e alta dimensionalidade utilizando arquiteturas recorrentes.
 \item Aprender uma \textbf{modelagem probabilística do tempo até a
        próxima mudança de estado}, estimando distribuições preditivas
        condicionadas ao histórico observado.
        \end{itemize}
   \noindent \textbf{Comparar arquiteturas:}
        \begin{itemize}
          \item Modelos DEEP, LSTM, GRU, BiLSTM e BiGRU com GRU fuser, todos com otimização Adam;
          \item Variações de treinamento (warmup, scheduler, RAdam, AdamW, variational dropout, difusão, log-likelihood e layernorm).
        \end{itemize}


\end{frame}

% =======================
% Seção: Base de dados e pré-processamento
% =======================
\section{Base de dados e pré-processamento}


\begin{frame}{Base de dados e pré-processamento}

\begin{columns}[t]
  
  \begin{column}[t]{0.48\textwidth}
    \textbf{Base de dados}

    \vspace{0.2cm}
    Cognite — sinais reais de sensores de um compressor industrial offshore.

    \vspace{0.3cm}
    \centering
    \includegraphics[width=\linewidth]{12series.png}

    \vspace{0.2cm}
    \scriptsize
    \textit{Exemplo de séries reais utilizadas no estudo (12 sinais simultâneos).}
  \end{column}

  \begin{column}[t]{0.48\textwidth}
    \textbf{Pré-processamento}

    \vspace{0.2cm}
    \begin{itemize}
      \item Estados operacionais:
        \begin{itemize}
          \item Estados originais: 6 (\(0\) a \(5\));
          \item Estados finais: 3 (normal (0), falha (1), anômalo (2)).
        \end{itemize}

      \item Normalização robusta;
      \item Amostragem a cada 5 minutos;
      \item Janelas temporais:
        \begin{itemize}
          \item \(L = 40\) amostras (\(\approx 3\)h20min);
          \item Deslocamento de 40 amostras (janelas não sobrepostas).
        \end{itemize}
    \end{itemize}
  \end{column}

\end{columns}
\end{frame}


% =======================
% Seção: Formulação Teórica
% =======================
\section{Formulação do Problema}

\begin{frame}{Definição do problema}
  \small
  \begin{itemize}
    \item Série temporal multivariada:
      \(\mathbf{x}_t \in \mathbb{R}^{d}\), \(t = 1,\dots,T\), com \(d = 11\) sensores.
    \item Estados discretos:
      \(s_t \in \{0,1,2\}\) (após fusão dos 6 estados iniciais).
    \item Definimos o \textbf{tempo até a próxima mudança de estado}:
      \[
        \tau_t = \min\{\Delta > 0 \;:\; s_{t+\Delta} \neq s_t\}.
      \]
    \item Para cada janela de histórico
      \(\mathbf{X} = (x_{t-L+1}, \dots, x_t)\),
      condicionada ao estado atual \(s_t\),
      queremos modelar a distribuição preditiva:
      \[
        p(\tau \mid \mathbf{X}, s_t).
      \]
    \item Na prática, o modelo produz, para cada estado \(s \in \{0,1,2\}\),
      parâmetros \(\mu_s(\mathbf{X})\) e \(\sigma_s^2(\mathbf{X})\) que
      definem as \textbf{PDFs do tempo até mudança de estado}.
  \end{itemize}

  \vspace{0.2cm}
  \noindent
  Assim, tratamos o problema como um \textbf{forecast probabilístico de tempo até evento}.
\end{frame}


\begin{frame}{Função de custo}
\small
\begin{itemize}
  \item Função de custo multi-tarefa:
\[
\mathcal{L} =
\mathcal{L}_\text{vae} \quad
\mathcal{L_{\text{difusão}}} =
\lambda_m \mathcal{L}_{\text{miss}} 
+ \lambda_v \mathcal{L}_\text{vae}
\]


\small
\[
\mathcal{L}_{\text{miss}} = \mathrm{BCE}(m,\hat{m})
\quad\text{com}\quad
\hat{m} = \sigma(f_{\text{miss}}(h))
\]
\(\mathcal{L}_{\text{miss}}\): aprendizado explícito do padrão de dados ausentes.

  \[
  \mathcal{L}_{\text{vae-}}
  =
  \mathbb{E}_{q(z_\tau|\mathbf{X})}
  \big[ - \log p(\tau \mid z_\tau) \big]
  +
  \beta\,\mathrm{KL}\big(q(z_\tau) \,\|\, \mathcal{N}(0,I)\big)
  \]

  \(\mathcal{L}_{\text{vae}}\): \textbf{modelagem probabilística do tempo até falha}, penalizando mudanças próximas e calibrando incerteza.
   
  \item Peso maior atribuído a instantes próximos à mudança de estado:
  \[
  \log p(\tau \mid z_\tau)
  \;\propto\;
  w_t \cdot \|\hat{\tau} - \tau\|^2,
  \quad
  w_t \gg 1
  \]

  \item Permite estimar \textbf{incerteza}, \textbf{intervalos de confiança} e
  \textbf{cobertura probabilística}.
\end{itemize}
\end{frame}




\begin{frame}{Arquitetura base e arquiteturas avaliadas}

  \vspace{0.6cm}
   \pipelinebase{
    \textbf{LSTM / GRU / BiLSTM / BiGRU}
  }
  \vspace{0.35cm}
 % ----------- TEXTO -----------
\scriptsize
\begin{itemize}
  \item \textbf{Arquitetura base}: Encoder temporal recorrente acoplado a um VAE latente.
   \item A entrada $\mathbf{X}=(\mathbf{x},\mathbf{c})_{1:T}$
        é codificada e processada pelo \textbf{RNN encoder},
        que parametriza a distribuição latente
        $(\mu_z,\log\sigma_z^2)$.

  \item A partir da amostragem $z \sim q(z\mid\mathbf{X})$,
        o \textbf{decoder} estima a distribuição sobre a
        mudança futura,
        produzindo $\hat{c}$ e sua incerteza associada
        $\log\sigma_c^2$.

  \item \textbf{Arquiteturas avaliadas}:
  \begin{itemize}
    \item \textbf{Deep MLP}: baseline feedforward (sem recorrência);
    \item \textbf{LSTM}: LSTM unidirecional e \textbf{BiLSTM};
    \item \textbf{GRU}: GRU unidirecional e \textbf{BiGRU}.
    \item \textbf{VAE}: Variational Autoencoder.
  \end{itemize}
\end{itemize}

\end{frame}
% =======================
% Seção: Arquiteturas
% =======================
\section{Arquiteturas e Variações}


\begin{frame}{Variações de treinamento em relação à arquitetura base}
\small

\textbf{Arquitetura base fixa:} Encoder temporal \textbf{BiLSTM} + VAE latente.

\vspace{0.3cm}

\textbf{1. Variações de otimização}
\begin{itemize}
  \item \textbf{BiLSTM + Warmup \& Scheduler:}  
        Aumenta gradualmente a taxa de aprendizado no início e depois aplica
        um \emph{scheduler} de decaimento, reduzindo instabilidades nas
        primeiras épocas e melhorando a convergência inicial.
  \item \textbf{BiLSTM + RAdam:}  
        Substitui o Adam por \textbf{RAdam}, que corrige a variância adaptativa
        nos primeiros passos, tornando o treinamento mais estável.
\end{itemize}

\vspace{0.3cm}

\textbf{2. Variações de regularização}
\begin{itemize}
  \item \textbf{BiLSTM + Variational Dropout:}  
        Aplica \emph{variational dropout} ($p=0{,}2$) compartilhado no tempo,
        reduzindo overfitting sem quebrar dependências temporais.
  \item \textbf{BiLSTM + Difusão (missingness):}  
        Acrescenta um objetivo probabilístico extra
        $\mathcal{L} = \lambda_m \mathcal{L}_{miss} + \lambda_v \mathcal{L}_{vae}$,
        forçando o modelo a ser robusto a \emph{missing data} e a calibrar melhor
        incerteza.
  \item \textbf{BiLSTM + LayerNorm:}  
        Aplica \textbf{LayerNorm} às ativações da BiLSTM, estabilizando as
        distribuições internas com efeito regularizante indireto.
 \item \textbf{BiLSTM + AdamW (L2/Weight Decay):}  
        Usa \textbf{AdamW} com \emph{weight decay} desacoplado
        ($\lambda = 10^{-4}$), acrescentando regularização L2 explícita
        nos pesos e melhorando generalização.
      \end{itemize}

\end{frame}


% =======================
% Seção: Configuração Experimental
% =======================
\section{Configuração Experimental}

\begin{frame}{Configuração Experimental}
  \begin{itemize}
    \item Divisão treino/teste: 80\% / 20\%.
    \item Dimensões do espaço latente foram validadas com a divisão 60\% / 20\% / 20\%.
    \item Treinamento por 500 épocas, batch size 256.
    \item Paciênica de 50 épocas para early stopping.
    \item Função de perda multi-tarefa conforme descrito.
    \item Otimização e Regularização tratadas como diferentes modelos.
    \item Otimizador: Adam (exceto variações).
    \item Taxa de aprendizado inicial: \(3 \times 10^{-4}\).
    \item Early stopping baseado no NELBO do teste.
    \item Avaliação final no conjunto de teste.
  \end{itemize}
\end{frame}
% =======================
% Seção: Resultados
% =======================
\section{Resultados}


\begin{frame}{Figuras de Mérito utilizadas}
  \begin{itemize}
   \item \textbf{NELBO} (Negative Evidence Lower Bound):
\begin{itemize}
  \item Loss probabilística minimizada no treinamento;
  \item Maximiza implicitamente a verossimilhança (ELBO);
  \item Balanceia reconstrução e regularização latente (KL).
\item
\(
\begin{aligned}
NELBO
&=
\mathbb{E}_{q(z\mid x)}
\!\left[
- \log p(x \mid z)
\right]
+
\mathrm{KL}\!\left(
q(z\mid x)\,\|\,p(z)
\right)
\end{aligned}
\)


\end{itemize}

\item \textbf{NLL} (Negative Log-Likelihood):
  \begin{itemize}
    \item Generaliza o MSE ao modelar explicitamente a variância
          da distribuição predita;
    \item Penaliza erros grandes \emph{e} variâncias mal calibradas
          (super ou subestimação de incerteza).
\item 
\(
\begin{aligned}
\text{NLL}
&=
- \log p(x \mid \mu, \sigma^2)
&=
\frac{(x-\mu)^2}{2\sigma^2}
+ \frac{1}{2}\log\sigma^2
+ \text{cte}
\end{aligned}
\)



  \end{itemize}
    \end{itemize}
\end{frame}


  \begin{frame}{Figuras de Mérito utilizadas}
     \begin{itemize}
    \item \textbf{MSE} (Mean Squared Error):
      \begin{itemize}
         \item Erro médio de reconstrução das séries;
    \item Mede fidelidade gerativa.
      \end{itemize}

  \item \textbf{Cobertura 90\%} (\texttt{cov\_90}):
  \begin{itemize}
    \item Mede a fração de amostras reais que caem dentro do
          intervalo preditivo teórico $[5\%,\,95\%]$;
    \item Avalia se o desvio padrão estimado produz uma
          \textbf{calibração probabilística consistente}
          com a cobertura nominal de 90\%.
  \end{itemize}

\item \textbf{Largura do intervalo 90\%} (\texttt{width\_90}):
  \begin{itemize}
    \item Corresponde à largura teórica do intervalo $[5\%,\,95\%]$
          derivado da distribuição predita;
    \item Quantifica a \textbf{sharpness} do modelo:
          intervalos menores indicam maior confiança,
          desde que a cobertura permaneça bem calibrada.
  \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Resultados quantitativos}
  \begin{table}[h]
    \centering
    \small
\begin{tabular}{lccccc}
\toprule
\textbf{Modelo} & \textbf{NELBO $\downarrow$} & \textbf{MSE (micro) $\downarrow$} 
& \textbf{NLL $\downarrow$} & \textbf{Cov90 $\rightarrow$ 0.90} & \textbf{Width90 $\downarrow$} \\
\midrule
\textbf{BiLSTM RAdam} & \textbf{0.023} & 0.002 $\pm$ 0.000 & \textbf{0.023} & 0.946 & 0.270 \\
BiLSTM Warmup/Sched. & 0.033 & 0.002 $\pm$ 0.000 & 0.033 & 0.946 & 0.270 \\
LSTM RAdam  & 0.036 & 0.001 $\pm$ 0.000 & 0.036 & 0.866 & 0.270 \\
LSTM & 0.064 & 0.002 $\pm$ 0.000 & 0.063 & 0.937 & 0.270 \\
BiLSTM & 0.150 & 0.002 $\pm$ 0.000 & 0.150 & 0.920 & 0.270 \\
BiGRU & 0.203 & 0.001 $\pm$ 0.000 & 0.202 & 0.875 & 0.270 \\
BiLSTM AdamW & 0.230 & 0.001 $\pm$ 0.001 & 0.229 & 0.866 & 0.270 \\
BiLSTM Difusão & 0.329 & 0.002 $\pm$ 0.001 & 0.328 & 0.839 & 0.270 \\
BiLSTM LayerNorm & 0.383 & 0.054 $\pm$ 0.001 & 0.332 & 0.839 & 0.270 \\
GRU & 0.464 & 0.003 $\pm$ 0.001 & 0.463 & 0.812 & 0.270 \\
BiLSTM VarDrop 0.2 & 0.992 & 0.002 $\pm$ 0.001 & 0.991 & 0.705 & 0.270 \\
DEEP & 8016.760 & 0.819 $\pm$ 0.011 & 33.711 & 0.250 & 0.270 \\
\bottomrule
\end{tabular}

    \caption{Comparação de desempenho entre modelos e variações. }
  \end{table}
\end{frame}

\begin{frame}{Curva de treinamento — Modelo \textbf{BiLSTM RAdam}}
\begin{itemize}
      \item Curva de NELBO bastante oscilatória, refletindo o
              desbalanceamento entre grupos e a raridade de mudanças de estado;
        \item Após as primeiras dezenas de épocas, o modelo passa a operar
              em um patamar de NELBO baixo, com mínimos recorrentes;
        \item A melhor época (\textbf{epoch 54}) atinge o menor NELBO entre as
              épocas avaliadas, sendo o melhor compromisso entre ajuste e
              complexidade para a tarefa de mudança de estado.   
\end{itemize}

  \centering
  \begin{figure}
  \includegraphics[width=0.70\linewidth]{nelbo.png}
  \caption{Evolução do NELBO durante o treinamento do modelo
  BiLSTM RAdam.}
  \end{figure}
\end{frame}


\begin{frame}{PDFs do tempo até mudança de estado}

\vspace{0.2cm}

\centering
\includegraphics[width=0.85\linewidth]{pdfs.png}

\scriptsize
\textit{Distribuições preditivas do tempo até mudança de estado
condicionadas ao histórico observado, ilustrando a modelagem
probabilística e a assimilação de incerteza pelo modelo.}

\footnotesize
\begin{itemize}
  \item PDFs do tempo até a mudança, condicionadas ao histórico $\mathbf{X}_{1:t}$.
  \item Curvas por estado futuro, com médias distintas e incerteza associada.
\end{itemize}

\end{frame}

% =======================
% Seção: Discussão e Conclusões
% =======================
\section{Discussão e Conclusões}

\begin{frame}{Discussão dos resultados — Mudança de estado}
  \begin{itemize}
    \item \textbf{Qualidade probabilística}:
      \begin{itemize}
        \item O \textbf{BiLSTM RAdam} apresenta o menor NELBO,
              indicando melhor ajuste probabilístico global;
        \item Bons valores de NLL e MSE micro confirmam alta precisão
              na predição de mudanças de estado raras.
      \end{itemize}

    \item \textbf{Calibração das incertezas}:
      \begin{itemize}
        \item Cobertura próxima de 0.90 nos melhores modelos
              indica boa calibração probabilística;
        \item Width90 constante sugere intervalos estáveis e comparáveis
              entre arquiteturas.
      \end{itemize}

    \item \textbf{Impacto arquitetural}:
      \begin{itemize}
        \item Modelos bidirecionais superam LSTM/GRU unidirecionais;
      \item Em cenários fortemente desbalanceados, o principal desafio é a
      \textbf{otimização sob gradientes raros}, e não o overfitting;
\item Otimizadores adaptativos (RAdam, warmup) estabilizam o treinamento
      e facilitam a captura de eventos de mudança raros, enquanto
      regularização excessiva tende a diluir esses sinais.

      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Conclusões — Mudança de estado}
  \begin{itemize}
    \item A predição de mudanças de estado é um problema
          fortemente desbalanceado e sensível à calibração probabilística.

    \item Arquiteturas recorrentes probabilísticas são essenciais:
      \begin{itemize}
        \item Capturam dependências temporais longas;
        \item Modelam explicitamente incertezas associadas ao evento de transição.
      \end{itemize}

    \item O \textbf{BiLSTM RAdam} é o modelo mais adequado:
      \begin{itemize}
        \item Menor NELBO entre os modelos avaliados;
        \item Boa calibração (Cov90 $\approx$ 0.95) e baixo erro micro.
      \end{itemize}

    \item Esses resultados indicam potencial para:
      \begin{itemize}
        \item Detecção antecipada de mudanças de regime;
        \item Suporte a sistemas de monitoramento e alerta industrial.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Referências}

\begin{itemize}
  \item Kingma, D. P.; Welling, M. \textit{Auto-Encoding Variational Bayes}. ICLR, 2014.
  \item Hochreiter, S.; Schmidhuber, J. \textit{Long Short-Term Memory}. Neural Computation, 1997.
  \item Cho et al. \textit{Learning Phrase Representations using RNN Encoder–Decoder}. EMNLP, 2014.
  \item Ho, J.; Jain, A.; Abbeel, P. \textit{Denoising Diffusion Probabilistic Models}. NeurIPS, 2020.
  \item Che et al. \textit{Recurrent Neural Networks for Multivariate Time Series with Missing Values}. Sci Rep, 2018.
\end{itemize}

\end{frame}


\end{document}
