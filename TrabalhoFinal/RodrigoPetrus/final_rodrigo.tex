\documentclass[10pt,aspectratio=169]{beamer}

% =======================
% Pacotes básicos
% =======================
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{positioning}
% -------- Pipeline base: Encoder + RNN + VAE (mesmo tamanho da variação) --------
\newcommand{\pipelinebase}[1]{%
\centering
\scriptsize
\begin{tikzpicture}[
  x=1cm, y=1cm,
  node distance=1.6cm and 1.2cm,
  every node/.style={transform shape},
  scale=0.85
]

\tikzstyle{box}=[%
  draw, thick, rounded corners=3pt,
  minimum width=2.2cm, minimum height=0.75cm,
  align=center, fill=gray!10
];
\tikzstyle{enc}=[box, fill=blue!12];
\tikzstyle{dec}=[box, fill=orange!18];
\tikzstyle{extra}=[box, fill=green!12];
\tikzstyle{circ}=[draw, thick, circle, minimum size=0.6cm];
\tikzstyle{arrow}=[->, thick];

% ---- arquitetura base (IGUAL PARA TODOS) ----
\node[box] (x) {
  Entrada\\
  $\mathbf{X}=(\mathbf{x},\mathbf{m})_{1:T}$
};

\node[enc, right=of x] (emb) {
  Encoder\\
  (MLP / Embedding)
};

\node[enc, right=of emb] (rnn) {
  RNN Encoder\\
  $\mu_z,\ \log\sigma_z^2$
};

\node[circ, right=of rnn] (z) {$z$};

\node[dec, right=of z] (dec) {
  Decoder\\
  $\hat{\mathbf{X}}_{\text{recon}},\ \log\sigma_x^2$
};


% ---- variação (sempre extra) ----
\node[extra, below=1.1cm of rnn, text width=4.2cm] (var) {#1};
\draw[arrow, dashed] (var) -- (rnn);

% ---- setas principais ----
\draw[arrow] (x)   -- (emb);
\draw[arrow] (emb) -- (rnn);
\draw[arrow] (rnn) -- (z);
\draw[arrow] (z)   -- (dec);

\end{tikzpicture}%
}

\title[Modo gerador]{Desenvolver um modelo gerador de séries temporais multivariadas
          para um compressor industrial offshore baseado em RNN e VAEs.}
\author{Rodrigo Petrus Domingues}
\institute{CPE727 -- Deep Learning}
\date{\today}

\begin{document}

% =======================
% Capa
% =======================
\begin{frame}
  \titlepage
\end{frame}

% =======================
% Sumário
% =======================
\begin{frame}{Sumário}
  \tableofcontents
\end{frame}

% =======================
% Seção: Motivação
% =======================
\section{Motivação e Objetivos}

\begin{frame}{Motivação e Objetivos do trabalho}
\noindent \textbf{Motivação:}
    \begin{itemize}
      \item Compressores industriais são ativos críticos (segurança, disponibilidade, custo).
      \item As séries temporais multivariadas de sensores refletem o estado operacional.
      \item Modelos generativos podem simular cenários operacionais variados.
      \item Gêmeos digitais e simulações realistas auxiliam na manutenção preditiva
    \end{itemize}

\noindent \textbf{Objetivos do trabalho:}

  \begin{itemize}
    \item Desenvolver \textbf{modelos geradores} de séries temporais multivariadas
          para um compressor industrial offshore.
    \item Aprender, de forma não supervisionada, a distribuição dos sinais de processo
          em janelas temporais.
    \item Gerar novas trajetórias plausíveis para:
      \begin{itemize}
        \item simulação de cenários,
        \item testes de algoritmos de monitoramento,
        \item análise de variabilidade operacional.
      \end{itemize}
     \end{itemize}
   \noindent \textbf{Comparar arquiteturas:}
        \begin{itemize}
          \item Modelos DEEP, LSTM, GRU, BiLSTM e BiGRU com GRU fuser, todos com otimização Adam;
          \item Variações de treinamento (warmup, scheduler, RAdam, AdamW, variational dropout, difusão e layernorm).
        \end{itemize}


\end{frame}

% =======================
% Seção: Base de dados e pré-processamento
% =======================
\section{Base de dados e pré-processamento}


\begin{frame}{Base de dados e pré-processamento}

\begin{columns}[t]
  
  \begin{column}[t]{0.48\textwidth}
    \textbf{Base de dados}

    \vspace{0.2cm}
    Cognite — sinais reais de sensores de um compressor industrial offshore.

    \vspace{0.3cm}
    \centering
    \includegraphics[width=\linewidth]{12series.png}

    \vspace{0.2cm}
    \scriptsize
    \textit{Exemplo de séries reais utilizadas no estudo (12 sinais simultâneos).}
  \end{column}

  \begin{column}[t]{0.48\textwidth}
    \textbf{Pré-processamento}

    \vspace{0.2cm}
    \begin{itemize}
      \item Estados operacionais:
        \begin{itemize}
          \item Estados originais: 6 (\(0\) a \(5\));
          \item Estados finais: 3 (normal, alerta, falha).
        \end{itemize}

      \item Normalização robusta (RobustScaler) por sensor;
      \item Série temporal com amostras a cada 5 minutos.
      \item Segmentação em janelas:
        \begin{itemize}
          \item \texttt{window\_size = 10}, \texttt{window\_step = 10}
          \item janelas não sobrepostas (modo gerador local).
        \end{itemize}
    \end{itemize}
  \end{column}

\end{columns}
\end{frame}


% =======================
% Seção: Formulação Teórica
% =======================
\section{Formulação do Problema}

\begin{frame}{Definição do problema (modo gerador)}

  \begin{itemize}
    \item Série multivariada: \(\mathbf{x}_t \in \mathbb{R}^{d}\), \(t = 1,\dots,T\), com \(d = 11\) sensores.
    \item Construímos janelas temporais:
      \[
        \mathbf{X} = (x_{t-L+1}, \dots, x_t) \in \mathbb{R}^{L \times d}
      \]
      com \(L = 10\) passos de tempo.
    \item Objetivo: aprender um \textbf{modelo gerador} para as janelas \(\mathbf{X}\):
      \[
        p_{\theta}(\mathbf{X}) \approx p_{\text{dados}}(\mathbf{X})
      \]
    \item Modelo latente (VAE + difusão):
      \begin{itemize}
        \item Encoder mapeia \(\mathbf{X}\) para um código latente \(z\);
        \item Decoder reconstrói \(\hat{\mathbf{X}}\) a partir de \(z\);
        \item O espaço latente é regularizado para permitir \textbf{amostrar novos} \(z\) e gerar janelas sintéticas.
      \end{itemize}
    \item Uso principal:
      \begin{itemize}
        \item Geração de séries realistas para simulação de cenários
              e testes de algoritmos de monitoramento.
      \end{itemize}
  \end{itemize}

  \vspace{0.3cm}
  \noindent O modelo aprende a \textbf{recriar e amostrar} janelas de sensores compatíveis
  com o comportamento real do compressor.
\end{frame}


\begin{frame}{Função de custo (modo gerador)}
\small

\begin{itemize}
  \item O modelo admite uma função de custo multi-termo:
\[
\mathcal{L} =
\mathcal{L}_\text{vae} \quad
\mathcal{L_{\text{difusão}}} =
\lambda_m \mathcal{L}_{\text{miss}} 
+ \lambda_v \mathcal{L}_\text{vae}
\]
\end{itemize}

\vspace{0.2cm}

\begin{itemize}
  \item O treinamento é \textbf{não supervisionado}, visando aprender:
    \[
      p_\theta(\mathbf{X}) \approx p_{\text{dados}}(\mathbf{X})
    \]
\end{itemize}

\vspace{0.2cm}

\small

\[
\mathcal{L}_{\text{miss}} = \mathrm{BCE}(m,\hat{m})
\quad\text{com}\quad
\hat{m} = \sigma(f_{\text{miss}}(h))
\]


\[
\mathcal{L}_{\text{vae}}
=
\underbrace{
\mathbb{E}_{q(z|\mathbf{X})}
\big[
-\log p(\mathbf{X}\mid z)
\big]
}_{\text{erro de reconstrução}}
+
\beta\,
\underbrace{
\mathrm{KL}\big(q(z\mid\mathbf{X}) \,\|\, \mathcal{N}(0,I)\big)
}_{\text{regularização do espaço latente}}
\]

\vspace{0.2cm}

\begin{itemize}
   \item BCE entre a máscara verdadeira de observação e a máscara prevista,
forçando o estado latente a codificar o padrão temporal de missing.
  \item O termo de reconstrução garante fidelidade aos sinais reais.
  \item O termo KL força um espaço latente contínuo e amostrável.
  \item Isso permite \textbf{gerar novas janelas sintéticas} de sensores
        com propriedades estatísticas similares às observadas.
\end{itemize}

\end{frame}


\begin{frame}{Arquitetura base e arquiteturas avaliadas}

  \vspace{0.6cm}
   \pipelinebase{
    \textbf{LSTM / GRU / BiLSTM / BiGRU}
  }
  \vspace{0.35cm}
 % ----------- TEXTO -----------
\small
\begin{itemize}
  \item \textbf{Arquitetura base}: Encoder temporal recorrente acoplado a um VAE latente.
    \item A entrada $\mathbf{X}$ é codificada e passada ao \textbf{RNN encoder},
        que parametriza $(\mu_z,\log\sigma_z^2)$.
 \item Amostramos $z$ e o \textbf{decoder} reconstrói $\hat{\mathbf{X}}$
      e estima a log-variância $\log\sigma_x^2$ para cada tempo/variável.


  \item \textbf{Arquiteturas avaliadas}:
  \begin{itemize}
    \item \textbf{Deep MLP}: baseline feedforward (sem recorrência);
    \item \textbf{LSTM}: LSTM unidirecional e \textbf{BiLSTM};
    \item \textbf{GRU}: GRU unidirecional e \textbf{BiGRU}.
    \item \textbf{VAE}: Variational Autoencoder.
  \end{itemize}
\end{itemize}

\end{frame}

% =======================
% Seção: Arquiteturas
% =======================
\section{Arquiteturas e Variações}


\begin{frame}{Variações de treinamento em relação à arquitetura base}
\small

\textbf{Arquitetura base fixa:} Encoder temporal \textbf{BiLSTM} + VAE latente  
(mesmo pipeline do slide anterior).

\vspace{0.3cm}

\textbf{1. Variações de otimização}
\begin{itemize}
  \item \textbf{BiLSTM + Warmup \& Scheduler:}  
        Aumenta gradualmente a taxa de aprendizado no início e depois aplica
        um \emph{scheduler} de decaimento, reduzindo instabilidades nas
        primeiras épocas e melhorando a convergência inicial.
  \item \textbf{BiLSTM + RAdam:}  
        Substitui o Adam por \textbf{RAdam}, que corrige a variância adaptativa
        nos primeiros passos, tornando o treinamento mais estável.
\end{itemize}

\vspace{0.3cm}

\textbf{2. Variações de regularização}
\begin{itemize}
  \item \textbf{BiLSTM + Variational Dropout:}  
        Aplica \emph{variational dropout} ($p=0{,}2$) compartilhado no tempo,
        reduzindo overfitting sem quebrar dependências temporais.
  \item \textbf{BiLSTM/GRU + Difusão (missingness):}  
        Acrescenta um objetivo probabilístico extra
        $\mathcal{L} = \lambda_m \mathcal{L}_{miss} + \lambda_v \mathcal{L}_{vae}$,
        forçando o modelo a ser robusto a \emph{missing data} e a calibrar melhor
        incerteza.
  \item \textbf{BiLSTM + LayerNorm:}  
        Aplica \textbf{LayerNorm} às ativações da BiLSTM, estabilizando as
        distribuições internas com efeito regularizante indireto.
 \item \textbf{BiLSTM + AdamW (L2/Weight Decay):}  
        Usa \textbf{AdamW} com \emph{weight decay} desacoplado
        ($\lambda = 10^{-4}$), acrescentando regularização L2 explícita
        nos pesos e melhorando generalização.
      \end{itemize}

\end{frame}

% =======================
% Seção: Configuração Experimental
% =======================
\section{Configuração Experimental}

\begin{frame}{Configuração Experimental}
  \begin{itemize}
    \item Divisão treino/teste: 80\% / 20\%.
    \item Dimensões do espaço latente foram validadas com a divisão 60\% / 20\% / 20\%.
    \item Treinamento por 500 épocas, batch size 256.
    \item Paciênica de 50 épocas para early stopping.
    \item Função de perda multi-tarefa conforme descrito.
    \item Otimização e Regularização tratadas como diferentes modelos.
    \item Otimizador: Adam (exceto variações).
    \item Taxa de aprendizado inicial: \(3 \times 10^{-4}\).
    \item Early stopping baseado no NELBO do teste.
    \item Avaliação final no conjunto de teste.
  \end{itemize}
\end{frame}







% =======================
% Seção: Resultados
% =======================
\section{Resultados}

\begin{frame}{Figuras de Mérito utilizadas}
  \begin{itemize}
   \item \textbf{NELBO} (Negative Evidence Lower Bound):
\begin{itemize}
  \item Loss probabilística minimizada no treinamento;
  \item Maximiza implicitamente a verossimilhança (ELBO);
  \item Balanceia reconstrução e regularização latente (KL).
\item
\(
\begin{aligned}
NELBO
&=
\mathbb{E}_{q(z\mid x)}
\!\left[
- \log p(x \mid z)
\right]
+
\mathrm{KL}\!\left(
q(z\mid x)\,\|\,p(z)
\right)
\end{aligned}
\)


\end{itemize}

\item \textbf{NLL} (Negative Log-Likelihood):
  \begin{itemize}
    \item Generaliza o MSE ao modelar explicitamente a variância
          da distribuição predita;
    \item Penaliza erros grandes \emph{e} variâncias mal calibradas
          (super ou subestimação de incerteza).
\item 
\(
\begin{aligned}
\text{NLL}
&=
- \log p(x \mid \mu, \sigma^2)
&=
\frac{(x-\mu)^2}{2\sigma^2}
+ \frac{1}{2}\log\sigma^2
+ \text{cte}
\end{aligned}
\)



  \end{itemize}
    \end{itemize}
\end{frame}


  \begin{frame}{Figuras de Mérito utilizadas}
     \begin{itemize}
    \item \textbf{MSE} (Mean Squared Error):
      \begin{itemize}
         \item Erro médio de reconstrução das séries;
    \item Mede fidelidade gerativa.
      \end{itemize}

  \item \textbf{Cobertura 90\%} (\texttt{cov\_90}):
  \begin{itemize}
    \item Mede a fração de amostras reais que caem dentro do
          intervalo preditivo teórico $[5\%,\,95\%]$;
    \item Avalia se o desvio padrão estimado produz uma
          \textbf{calibração probabilística consistente}
          com a cobertura nominal de 90\%.
  \end{itemize}

\item \textbf{Largura do intervalo 90\%} (\texttt{width\_90}):
  \begin{itemize}
    \item Corresponde à largura teórica do intervalo $[5\%,\,95\%]$
          derivado da distribuição predita;
    \item Quantifica a \textbf{sharpness} do modelo:
          intervalos menores indicam maior confiança,
          desde que a cobertura permaneça bem calibrada.
  \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Resultados quantitativos}
  \begin{table}[h]
    \centering
    \small
\begin{tabular}{lccccc}
\toprule
\textbf{Modelo} 
& \textbf{NELBO $\downarrow$} 
& \textbf{NLL $\downarrow$} 
& \textbf{MSE $\downarrow$}
& \textbf{Cov90 $\rightarrow$ 0.90} 
& \textbf{Width90 $\downarrow$} \\
\midrule

\textbf{GRU Difusão}
& \textbf{0.510}
& \textbf{0.456}
& \textbf{0.237 $\pm$ 0.064} 
& \textbf{0.897}
& \textbf{2.213} \\

BiLSTM Difusão
& 0.518
& 0.462
& 0.243 $\pm$ 0.067
& 0.902 
& 2.200 \\

GRU 
& 0.558 
& 0.498 
& 0.300 $\pm$ 0.108 
& 0.905 
& 2.252 \\

LSTM 
& 0.588 
& 0.513 
& 0.332 $\pm$ 0.129 
& 0.900 
& 2.251 \\

BiGRU 
& 0.589 
& 0.499 
& 0.335 $\pm$ 0.136 
& 0.909 
& 2.267 \\

BiLSTM VarDrop 0.2 
& 0.595 
& 0.506 
& 0.317 $\pm$ 0.119 
& 0.898 
& 2.240 \\

BiLSTM RAdam 
& 0.597 
& 0.511 
& 0.341 $\pm$ 0.139 
& 0.898 
& 2.234 \\

BiLSTM Warmup/Sched. 
& 0.598 
& 0.504 
& 0.354 $\pm$ 0.146 
& 0.903 
& 2.251 \\

BiLSTM AdamW 
& 0.601 
& 0.506 
& 0.341 $\pm$ 0.141 
& 0.900 
& 2.224 \\

BiLSTM 
& 0.609 
& 0.527 
& 0.337 $\pm$ 0.133 
& 0.901 
& 2.269 \\

BiLSTM LayerNorm 
& 0.617 
& 0.514 
& 0.342 $\pm$ 0.140 
& 0.907 
& 2.281 \\

DEEP 
& 1.217 
& 1.164 
& 0.693 $\pm$ 0.209 
& 0.916 
& 2.785 \\
\bottomrule
\end{tabular}


  \caption{
Comparação probabilística entre modelos.
O critério principal de seleção é a minimização do NELBO.
A métrica Cov90 avalia calibração probabilística, cujo valor ideal é próximo de 0.90.
A métrica Width90 mede a precisão dos intervalos de previsão e é comparada apenas
entre modelos com cobertura adequadamente calibrada.
}
  \end{table}

\end{frame}



\begin{frame}{Curva de treinamento — Modelo campeão}


 Evolução do NELBO ao longo das épocas para o modelo
        \textbf{GRU Difusão}.
 \begin{itemize}
      \item Fase inicial instável, com forte variação do NELBO nas
            primeiras épocas;
      \item Convergência progressiva para um patamar de NELBO mais estável
            após dezenas de épocas;
      \item A melhor época (260) é definida por \textit{early stopping},
            privilegiando estabilidade e capacidade de generalização,
            e não o mínimo pontual da curva.
    \end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=0.65\linewidth]{nelbo.png}
  \caption{Evolução do NELBO durante o treinamento do modelo
  BiLSTM Difusão.}
\end{figure}

\end{frame}

\begin{frame}{Geração de séries — Avaliação qualitativa}

\begin{columns}[c]

% -------- Figura --------
\begin{column}{0.62\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{distribuicao.png}
\end{column}

% -------- Texto --------
\begin{column}{0.38\textwidth}
\scriptsize
\textbf{Comparação qualitativa}

\vspace{0.5em}

Séries temporais reais versus séries geradas pelo modelo variacional,
comparadas a ajustes ponto-a-ponto baseados em MSE.

\vspace{0.5em}

A abordagem probabilística captura melhor:
\begin{itemize}
  \item a distribuição marginal dos sinais;
  \item a variabilidade temporal;
  \item a incerteza intrínseca dos dados.
\end{itemize}
\end{column}

\end{columns}

\end{frame}


% =======================
% Seção: Discussão e Conclusões
% =======================
\section{Discussão e Conclusões}
\begin{frame}{Discussão dos resultados (modo gerador)}
  \begin{itemize}
    \item \textbf{Qualidade gerativa}:
      \begin{itemize}
        \item Modelos variacionais recorrentes superam o baseline feedforward;
        \item \textbf{GRU  Difusão} apresenta o menor NELBO;
         \end{itemize}

    \item \textbf{Calibração vs. sharpness}:
      \begin{itemize}
        \item Difusão gera intervalos mais estreitos (menor \textit{width\_90});
        \item Cobertura próxima ao alvo de 90\%;
        \item Trade-off equilibrado entre precisão e incerteza.
      \end{itemize}

    \item \textbf{Impacto arquitetural}:
      \begin{itemize}
          \item A combinação \textbf{GRU + difusão} produz o melhor compromisso
              entre NELBO, NLL e MSE, com boa calibração (Cov90) e intervalos
              mais enxutos (Width90);
       \end{itemize}

    \item \textbf{Treinamento e regularização}:
      \begin{itemize}
        \item Difusão teve melhor resultado como principal regularizador probabilístico;
        \item Dropout trouxe ganhos secundários;
        \item LayerNorm piorou o resultado, por destruir escalas absolutas necessárias ao VAE;
        \item Otimizadores afetam marginalmente o desempenho.
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Conclusões (modo gerador)}
  \begin{itemize}
    \item O uso de \textbf{RNNs com difusão} se mostrou superior como modelo gerador
          probabilístico para séries temporais industriais.

    \item Modelos recorrentes superam o baseline feedforward:
      \begin{itemize}
           \item Fusão temporal melhora coerência de longo prazo.
      \end{itemize}

    \item \textbf{Difusão é decisiva}:
      \begin{itemize}
        \item \textbf{GRU com Difusão} é o modelo campeão, mas muito próximo do \textbf{BiLSTM com Difusão}.;
        \item Menor NELBO, NLL e MSE, com calibração adequada.
      \end{itemize}

    \item O modelo permite:
      \begin{itemize}
        \item Geração de séries realistas e calibradas;
        \item Simulação de cenários raros ou críticos.
      \end{itemize}

    \item Aplicações futuras:
      \begin{itemize}
        \item Gêmeos digitais industriais;
        \item Data augmentation orientada por incerteza.
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Referências}

\begin{itemize}
  \item Kingma, D. P.; Welling, M. \textit{Auto-Encoding Variational Bayes}. ICLR, 2014.
  \item Hochreiter, S.; Schmidhuber, J. \textit{Long Short-Term Memory}. Neural Computation, 1997.
  \item Cho et al. \textit{Learning Phrase Representations using RNN Encoder–Decoder}. EMNLP, 2014.
  \item Ho, J.; Jain, A.; Abbeel, P. \textit{Denoising Diffusion Probabilistic Models}. NeurIPS, 2020.
  \item Che et al. \textit{Recurrent Neural Networks for Multivariate Time Series with Missing Values}. Sci Rep, 2018.
\end{itemize}

\end{frame}





\end{document}
