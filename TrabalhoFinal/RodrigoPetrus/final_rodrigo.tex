\documentclass[10pt,aspectratio=169]{beamer}

% =======================
% Pacotes básicos
% =======================
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{positioning}


\title[Modo gerador]{Desenvolver um modelo gerador de séries temporais multivariadas
          para um compressor industrial offshore baseado em RNN e VAEs.}
\author{Rodrigo Petrus Domingues}
\institute{CPE727 -- Deep Learning}
\date{\today}

\begin{document}

% =======================
% Capa
% =======================
\begin{frame}
  \titlepage
\end{frame}

% =======================
% Sumário
% =======================
\begin{frame}{Sumário}
  \tableofcontents
\end{frame}

% =======================
% Seção: Motivação
% =======================
\section{Motivação e Objetivos}

\begin{frame}{Motivação e Objetivos do trabalho}
\noindent \textbf{Motivação:}
    \begin{itemize}
      \item Compressores industriais são ativos críticos (segurança, disponibilidade, custo).
      \item As séries temporais multivariadas de sensores refletem o estado operacional.
      \item Modelos generativos podem simular cenários operacionais variados.
      \item Gêmeos digitais e simulações realistas auxiliam na manutenção preditiva
    \end{itemize}

\noindent \textbf{Objetivos do trabalho:}

  \begin{itemize}
    \item Desenvolver \textbf{modelos geradores} de séries temporais multivariadas
          para um compressor industrial offshore.
    \item Aprender, de forma não supervisionada, a distribuição dos sinais de processo
          em janelas temporais.
    \item Gerar novas trajetórias plausíveis para:
      \begin{itemize}
        \item simulação de cenários,
        \item testes de algoritmos de monitoramento,
        \item análise de variabilidade operacional.
      \end{itemize}
     \end{itemize}
   \noindent \textbf{Comparar arquiteturas:}
        \begin{itemize}
          \item Modelos DEEP, LSTM, GRU, BiLSTM e BiGRU com GRU fuser, todos com otimização Adam;
          \item Variações de treinamento (warmup, scheduler, RAdam, AdamW, variational dropout, difusão e layernorm).
        \end{itemize}


\end{frame}

% =======================
% Seção: Base de dados e pré-processamento
% =======================
\section{Base de dados e pré-processamento}


\begin{frame}{Base de dados e pré-processamento}

\begin{columns}[t]
  
  \begin{column}[t]{0.48\textwidth}
    \textbf{Base de dados}

    \vspace{0.2cm}
    Cognite — sinais reais de sensores de um compressor industrial offshore.

    \vspace{0.3cm}
    \centering
    \includegraphics[width=\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/RodrigoPetrus/12series.png}

    \vspace{0.2cm}
    \scriptsize
    \textit{Exemplo de séries reais utilizadas no estudo (12 sinais simultâneos).}
  \end{column}

  \begin{column}[t]{0.48\textwidth}
    \textbf{Pré-processamento}

    \vspace{0.2cm}
    \begin{itemize}
      \item Estados operacionais:
        \begin{itemize}
          \item Estados originais: 6 (\(0\) a \(5\));
          \item Estados finais: 3 (normal, alerta, falha).
        \end{itemize}

      \item Normalização robusta (RobustScaler) por sensor;
      \item Série temporal com amostras a cada 5 minutos.
      \item Segmentação em janelas:
        \begin{itemize}
          \item \texttt{window\_size = 10}, \texttt{window\_step = 10}
          \item janelas não sobrepostas (modo gerador local).
        \end{itemize}
    \end{itemize}
  \end{column}

\end{columns}
\end{frame}


% =======================
% Seção: Formulação Teórica
% =======================
\section{Formulação do Problema}

\begin{frame}{Definição do problema (modo gerador)}

  \begin{itemize}
    \item Série multivariada: \(\mathbf{x}_t \in \mathbb{R}^{d}\), \(t = 1,\dots,T\), com \(d = 11\) sensores.
    \item Construímos janelas temporais:
      \[
        \mathbf{X} = (x_{t-L+1}, \dots, x_t) \in \mathbb{R}^{L \times d}
      \]
      com \(L = 10\) passos de tempo.
    \item Objetivo: aprender um \textbf{modelo gerador} para as janelas \(\mathbf{X}\):
      \[
        p_{\theta}(\mathbf{X}) \approx p_{\text{dados}}(\mathbf{X})
      \]
    \item Modelo latente (VAE + difusão):
      \begin{itemize}
        \item Encoder mapeia \(\mathbf{X}\) para um código latente \(z\);
        \item Decoder reconstrói \(\hat{\mathbf{X}}\) a partir de \(z\);
        \item O espaço latente é regularizado para permitir \textbf{amostrar novos} \(z\) e gerar janelas sintéticas.
      \end{itemize}
    \item Uso principal:
      \begin{itemize}
        \item Geração de séries realistas para simulação de cenários
              e testes de algoritmos de monitoramento.
      \end{itemize}
  \end{itemize}

  \vspace{0.3cm}
  \noindent O modelo aprende a \textbf{recriar e amostrar} janelas de sensores compatíveis
  com o comportamento real do compressor.
\end{frame}


\begin{frame}{Função de custo (modo gerador)}
\small

\begin{itemize}
  \item O modelo admite uma função de custo multi-termo:
\[
\mathcal{L} =
\lambda_m \mathcal{L}_{\text{miss}}
+ \lambda_v \mathcal{L}_\text{vae}
\]
\end{itemize}

\vspace{0.2cm}

\begin{itemize}
  \item O treinamento é \textbf{não supervisionado}, visando aprender:
    \[
      p_\theta(\mathbf{X}) \approx p_{\text{dados}}(\mathbf{X})
    \]
\end{itemize}

\vspace{0.2cm}

\small

\[
\mathcal{L}_{\text{miss}} = \mathrm{BCE}(m,\hat{m})
\quad\text{com}\quad
\hat{m} = \sigma(f_{\text{miss}}(h))
\]


\[
\mathcal{L}_{\text{vae}}
=
\underbrace{
\mathbb{E}_{q(z|\mathbf{X})}
\big[
-\log p(\mathbf{X}\mid z)
\big]
}_{\text{erro de reconstrução}}
+
\beta\,
\underbrace{
\mathrm{KL}\big(q(z\mid\mathbf{X}) \,\|\, \mathcal{N}(0,I)\big)
}_{\text{regularização do espaço latente}}
\]

\vspace{0.2cm}

\begin{itemize}
   \item BCE entre a máscara verdadeira de observação e a máscara prevista,
forçando o estado latente a codificar o padrão temporal de missing.
  \item O termo de reconstrução garante fidelidade aos sinais reais.
  \item O termo KL força um espaço latente contínuo e amostrável.
  \item Isso permite \textbf{gerar novas janelas sintéticas} de sensores
        com propriedades estatísticas similares às observadas.
\end{itemize}

\end{frame}





\begin{frame}{Arquiteturas avaliadas}
  \begin{itemize}
    \item \textbf{TSDF\_DEEP}:
      \begin{itemize}
        \item Modelo feedforward sobre janelas (baseline não-recorrente).
      \end{itemize}
    \item \textbf{TSDF\_LSTM}:
      \begin{itemize}
        \item RNN LSTM unidirecional;
        \item Versão \textbf{BiLSTM}.
      \end{itemize}
    \item \textbf{TSDF\_GRU}:
      \begin{itemize}
        \item RNN GRU unidirecional;
        \item Versão \textbf{BiGRU}.
      \end{itemize}
  \end{itemize}
  
\end{frame}


\begin{frame}{Arquitetura baseline: TSDF\_DEEP (feedforward)}
  \begin{itemize}
    \item Entrada: janela temporal \(\mathbf{X} = (x_1,\dots,x_L)\),
          com \(x_t \in \mathbb{R}^{11}\) e máscara \(m_t \in \{0,1\}^{11}\).
    \item Codificação inicial por MLP (por timestep):
      \[
        e_t = \phi\left( W_e [x_t \,\Vert\, m_t] + b_e \right),
        \quad t = 1,\dots,L
      \]
      onde \(\phi(\cdot)\) é uma ativação não linear (ReLU/GELU).
    \item Incorporação explícita do tempo:
      \[
        e'_t = \mathrm{MLP}\big([e_t \,\Vert\, t_t]\big)
      \]
      com \(t_t\) o timestamp normalizado do instante \(t\).
    \item Representação latente da janela:
      \begin{itemize}
        \item Obtida independentemente por timestep (sem estados recorrentes);
        \item Integra informação temporal apenas via timestamps explícitos.
      \end{itemize}
    \item Cabeças de saída (modo gerador):
      \begin{align*}
  \hat{x}_t &= f_{\theta}(e'_t) \quad \text{(reconstrução / geração)} 
\end{align*}
    \item Treinamento:
      \begin{itemize}
        \item Otimizador Adam;
        \item Mesmas funções de perda usadas nos modelos recorrentes.
      \end{itemize}
    
  \end{itemize}
\end{frame}



\begin{frame}{Arquitetura 1: LSTM}
  \begin{itemize}
    \item Entrada: janela temporal \(\mathbf{X} = (x_1,\dots,x_L)\), \(x_t \in \mathbb{R}^{11}\).
    \item Codificação temporal com LSTM unidirecional:
      \[
        (h_t, c_t) = \mathrm{LSTM}(x_t, h_{t-1}, c_{t-1}), \quad t = 1,\dots,L
      \]
   \item Representação latente da janela:
\[
  z = h_L \in \mathbb{R}^{H}
\]
\item $z$ representa um código latente contínuo da dinâmica temporal.
    \item Decoder:
\[
  \hat{\mathbf{X}} = p_\theta(\mathbf{X}\mid z)
\]
    \item Otimização:
      \begin{itemize}
        \item Otimizador Adam, minimizando perda conjunta (classificação de estado + regressão de tempo).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Arquitetura 2: GRU}
\begin{itemize}
  \item Mesma entrada:
  \[
    \mathbf{X} = (x_1, \dots, x_L), \quad x_t \in \mathbb{R}^{11}
  \]

  \item Codificação temporal recorrente:
  \[
    \{h_t\}_{t=1}^L = \mathrm{GRU}(\mathbf{X})
  \]

  \item Representação latente da janela:
  \[
    z = h_L \in \mathbb{R}^{H}
  \]

  \item $z$ representa um resumo contínuo da dinâmica temporal da janela.

  \item Reconstrução:
  \[
    \hat{\mathbf{X}} = p_\theta(\mathbf{X} \mid z)
  \]

  \item Treinamento:
  \begin{itemize}
    \item Otimização por Adam, com os mesmos hiperparâmetros do baseline Deep.
  \end{itemize}
\end{itemize}
\end{frame}



\begin{frame}{Arquitetura 3: BiGRU + GRU fuser}
  \begin{itemize}
    \item Codificação da janela com BiGRU:
      \[
        h_t^{\rightarrow}, h_t^{\leftarrow} = \mathrm{BiGRU}(x_t, h_{t-1}^{\rightarrow}, h_{t+1}^{\leftarrow})
      \]
      \[
        z_k = [h_L^{\rightarrow} \,\Vert\, h_1^{\leftarrow}] \in \mathbb{R}^{2H}
      \]
      onde \(z_k\) é o embedding da \(k\)-ésima janela.
    \item Fusão temporal entre janelas com GRU fuser:
      \[
        m_k = \mathrm{GRU}_{\text{fuser}}(z_k, m_{k-1}), \quad k = 1,\dots,K
      \]
      \item Fuser captura dependências de longo alcance entre janelas,
      produzindo um latente $\mathbf{m}_K$ adequado à geração.
    \item Cabeças de saída a partir de \(m_K\):
     \[
\hat{\mathbf{X}} \sim p_\theta(\mathbf{X}\mid m_K)
\]
    \item Otimização:
      \begin{itemize}
        \item Adam sobre todos os parâmetros (BiGRU + GRU fuser + cabeças).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Arquitetura 4: BiLSTM + GRU fuser}
  \begin{itemize}
    \item Codificação da janela com BiLSTM:
      \[
        (h_t^{\rightarrow}, c_t^{\rightarrow}), (h_t^{\leftarrow}, c_t^{\leftarrow})
          = \mathrm{BiLSTM}(x_t,\dots)
      \]
      \[
        z_k = [h_L^{\rightarrow} \,\Vert\, h_1^{\leftarrow}] \in \mathbb{R}^{2H}
      \]
    \item Fusão temporal entre janelas com GRU fuser:
      \[
        m_k = \mathrm{GRU}_{\text{fuser}}(z_k, m_{k-1}), \quad k = 1,\dots,K
      \]
        \item Fuser captura dependências de longo alcance entre janelas,
      produzindo um latente $\mathbf{m}_K$ adequado à geração.
    \item Cabeças de saída:
      \[
\hat{\mathbf{X}} \sim p_\theta(\mathbf{X}\mid m_K)
\]
    \item Otimização:
      \begin{itemize}
        \item Treino conjunto com Adam, mesma função de perda multi-tarefa.
      \end{itemize}
  \end{itemize}
\end{frame}


% =======================
% Seção: Arquiteturas
% =======================
\section{Arquiteturas e Variações}

\begin{frame}{Pipeline principal: Encoder Temporal + VAE Latente}
\centering
\scriptsize

\begin{tikzpicture}[x=1cm, y=1cm]

\tikzstyle{box}=[
  draw, thick, rounded corners=3pt,
  minimum width=2.0cm, minimum height=0.6cm,
  align=center, fill=gray!10
]
\tikzstyle{latent}=[box, fill=blue!10]
\tikzstyle{vae}=[box, fill=orange!15]
\tikzstyle{head}=[box, fill=green!12]
\tikzstyle{circ}=[draw, thick, circle, minimum size=0.6cm]
\tikzstyle{arrow}=[->, thick]

% ---- Pipeline principal ----
\node[box]    (x)     at (0,0)  {Entradas\\$(\mathbf{x},\mathbf{m})_{1:T}$};
\node[box]    (rnn)   at (2.5,0) {Encoder Temporal\\RNN ou BiRNN};
\node[latent] (h)     at (5.0,0) {Estado Temporal\\$h$};

\node[vae]    (muvar) at (7.7,0) {VAE Encoder\\$\mu_z,\log\sigma_z^2$};
\node[circ]   (z)     at (10.0,0) {$z$};

\node[vae]   (dec)   at (12.2,0) {Decoder\\$\mu_x,\log\sigma_x^2$};

% ---- Setas ----
\draw[arrow] (x) -- (rnn);
\draw[arrow] (rnn) -- (h);
\draw[arrow] (h) -- (muvar);
\draw[arrow] (muvar) -- (z);
\draw[arrow] (z) -- (dec);

\end{tikzpicture}
\vspace{0.5cm}

  \begin{itemize}
   \item Arquitetura base é composta de um encoder temporal uni ou bidirecional seguido de um espaço latente 
variacional, responsável por capturar incerteza e permitir geração probabilística das séries.

  \end{itemize}

\end{frame}

\begin{frame}{Variações de treinamento - Otimização}
    

  \begin{itemize}
    \item \textbf{BiLSTM + Warmup \& Scheduler}
      \begin{itemize}
        \item Aumenta gradualmente a taxa de aprendizado no início do treino, 
              evitando instabilidades e melhorando a convergência inicial.

      \end{itemize}

    \item \textbf{BiLSTM + RAdam}
      \begin{itemize}
        \item Reduz a variância adaptativa do Adam nos primeiros passos, 
              tornando o treinamento mais estável.
             \end{itemize}
 

     \end{itemize}
\end{frame}


\begin{frame}{Variações de treinamento — Regularização}
 \begin{itemize}

    \item \textbf{BiLSTM + Variational Dropout}
      \begin{itemize}
        \item Regularização explícita no tempo,
              reduzindo overfitting sem quebrar dependências temporais.
        \item Variational\_dropout utilizado: 0.2
      \end{itemize}

    \item \textbf{BiLSTM + Difusão (missingness)}
\begin{itemize}
  \item Regularização implícita via objetivos probabilísticos,
        forçando robustez a dados ausentes.
  \item
  \(
  \begin{aligned}
  \mathcal{L} &=
  \lambda_m\, \mathcal{L}_{\text{miss}}
  + \lambda_v\, \mathcal{L}_{\text{vae}}
  \end{aligned}
  \)
\end{itemize}


       \item \textbf{BiLSTM + LayerNorm}
      \begin{itemize}
        \item Estabiliza as ativações internas,
              com efeito regularizante indireto.
      
      \end{itemize}

    \item \textbf{BiLSTM + AdamW (L2/Weight Decay)}
      \begin{itemize}
        \item Regularização explícita dos pesos,
              melhorando generalização.
        \item  L2/weight\_decay utilizado: 1e-4
      \end{itemize}

  \end{itemize}
\end{frame}




% =======================
% Seção: Configuração Experimental
% =======================
\section{Configuração Experimental}

\begin{frame}{Configuração Experimental}
  \begin{itemize}
    \item Divisão treino/teste: 80\% / 20\%.
    \item A quantidade de neurônios já foi validada anteriormente em outra disciplina
    \item Treinamento por 500 épocas, batch size 256.
    \item Paciênica de 50 épocas para early stopping.
    \item Função de perda multi-tarefa conforme descrito.
    \item Otimização e Regularização tratadas como diferentes modelos.
    \item Otimizador: Adam (exceto variações).
    \item Taxa de aprendizado inicial: \(3 \times 10^{-4}\).
    \item Early stopping baseado no NELBO do teste.
    \item Avaliação final no conjunto de teste.
  \end{itemize}
\end{frame}






\begin{frame}{Arquitetura vencedora — BiLSTM + Difusão Latente (Geração)}
\centering
\scriptsize

\begin{tikzpicture}[x=1cm, y=1cm]

\tikzstyle{box}=[
  draw, thick, rounded corners=3pt,
  minimum width=2.0cm, minimum height=0.6cm,
  align=center, fill=gray!10
]
\tikzstyle{enc}=[box, fill=blue!10]
\tikzstyle{diff}=[box, fill=green!12]
\tikzstyle{vae}=[box, fill=orange!15]
\tikzstyle{circ}=[draw, thick, circle, minimum size=0.55cm]
\tikzstyle{arrow}=[->, thick]

% ---- pipeline principal ----
\node[box]  (x)    at (0,0)   {Entradas\\$(\mathbf{x},\mathbf{m})_{1:T}$};
\node[box]  (rnn)  at (2.4,0) {Encoder Temporal\\BiLSTM};
\node[enc]  (h)    at (4.8,0) {Estado Temporal\\$h$};

\node[diff] (diffb)at (7.2,0) {Difusão Latente\\(ruído $\leftrightarrow z$)};
\node[vae]  (muvar)at (9.7,0) {VAE Encoder\\$\mu_z,\log\sigma_z^2$};
\node[circ] (z)    at (11.6,0) {$z$};

\node[vae]  (dec)  at (13.6,0) {Decoder\\$\mu_x,\log\sigma_x^2$};

% ---- setas ----
\draw[arrow] (x)    -- (rnn);
\draw[arrow] (rnn)  -- (h);
\draw[arrow] (h)    -- (diffb);
\draw[arrow] (diffb)-- (muvar);
\draw[arrow] (muvar)-- (z);
\draw[arrow] (z)    -- (dec);

\end{tikzpicture}
\vspace{0.5cm}

  \begin{itemize}
   \item No modo gerador, a difusão modela a incerteza no espaço latente, aumentando robustez, diversidade e estabilidade estatística das séries reconstruídas.
  \end{itemize}
\end{frame}





% =======================
% Seção: Resultados
% =======================
\section{Resultados}

\begin{frame}{Figuras de Mérito utilizadas}
  \begin{itemize}
   \item \textbf{NELBO} (Negative Evidence Lower Bound):
\begin{itemize}
  \item Loss probabilística minimizada no treinamento;
  \item Maximiza implicitamente a verossimilhança (ELBO);
  \item Balanceia reconstrução e regularização latente (KL).
\item
\(
\begin{aligned}
NELBO
&=
\mathbb{E}_{q(z\mid x)}
\!\left[
- \log p(x \mid z)
\right]
+
\mathrm{KL}\!\left(
q(z\mid x)\,\|\,p(z)
\right)
\end{aligned}
\)


\end{itemize}

\item \textbf{NLL} (Negative Log-Likelihood):
  \begin{itemize}
    \item Generaliza o MSE ao modelar explicitamente a variância
          da distribuição predita;
    \item Penaliza erros grandes \emph{e} variâncias mal calibradas
          (super ou subestimação de incerteza).
\item 
\(
\begin{aligned}
\text{NLL}
&=
- \log p(x \mid \mu, \sigma^2)
&=
\frac{(x-\mu)^2}{2\sigma^2}
+ \frac{1}{2}\log\sigma^2
+ \text{cte}
\end{aligned}
\)



  \end{itemize}
    \end{itemize}
\end{frame}


  \begin{frame}{Figuras de Mérito utilizadas}
     \begin{itemize}
    \item \textbf{MSE} (Mean Squared Error):
      \begin{itemize}
         \item Erro médio de reconstrução das séries;
    \item Mede fidelidade gerativa.
      \end{itemize}

  \item \textbf{Cobertura 90\%} (\texttt{cov\_90}):
  \begin{itemize}
    \item Mede a fração de amostras reais que caem dentro do
          intervalo preditivo teórico $[5\%,\,95\%]$;
    \item Avalia se o desvio padrão estimado produz uma
          \textbf{calibração probabilística consistente}
          com a cobertura nominal de 90\%.
  \end{itemize}

\item \textbf{Largura do intervalo 90\%} (\texttt{width\_90}):
  \begin{itemize}
    \item Corresponde à largura teórica do intervalo $[5\%,\,95\%]$
          derivado da distribuição predita;
    \item Quantifica a \textbf{sharpness} do modelo:
          intervalos menores indicam maior confiança,
          desde que a cobertura permaneça bem calibrada.
  \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Resultados quantitativos}
  \begin{table}[h]
    \centering
    \small
\begin{tabular}{lccccc}
\toprule
\textbf{Modelo} 
& \textbf{NELBO $\downarrow$} 
& \textbf{NLL $\downarrow$} 
& \textbf{MSE $\downarrow$}
& \textbf{Cov90 $\rightarrow$ 0.90} 
& \textbf{Width90 $\downarrow$} \\
\midrule
\textbf{BiLSTM Difusão} 
& \textbf{0.518} 
& \textbf{0.462} 
& \textbf{0.243 $\pm$ 0.067} 
& \textbf{0.902} 
& \textbf{2.200} \\

GRU 
& 0.558 
& 0.498 
& 0.300 $\pm$ 0.108 
& 0.905 
& 2.252 \\

LSTM 
& 0.588 
& 0.513 
& 0.332 $\pm$ 0.129 
& 0.900 
& 2.251 \\

BiGRU 
& 0.589 
& 0.499 
& 0.335 $\pm$ 0.136 
& 0.909 
& 2.267 \\

BiLSTM VarDrop 0.2 
& 0.595 
& 0.506 
& 0.317 $\pm$ 0.119 
& 0.898 
& 2.240 \\

BiLSTM RAdam 
& 0.597 
& 0.511 
& 0.341 $\pm$ 0.139 
& 0.898 
& 2.234 \\

BiLSTM Warmup/Sched. 
& 0.598 
& 0.504 
& 0.354 $\pm$ 0.146 
& 0.903 
& 2.251 \\

BiLSTM AdamW 
& 0.601 
& 0.506 
& 0.341 $\pm$ 0.141 
& 0.900 
& 2.224 \\

BiLSTM 
& 0.609 
& 0.527 
& 0.337 $\pm$ 0.133 
& 0.901 
& 2.269 \\

BiLSTM LayerNorm 
& 0.617 
& 0.514 
& 0.342 $\pm$ 0.140 
& 0.907 
& 2.281 \\

DEEP 
& 1.217 
& 1.164 
& 0.693 $\pm$ 0.209 
& 0.916 
& 2.785 \\
\bottomrule
\end{tabular}


  \caption{
Comparação probabilística entre modelos.
O critério principal de seleção é a minimização do NELBO.
A métrica Cov90 avalia calibração probabilística, cujo valor ideal é próximo de 0.90.
A métrica Width90 mede a precisão dos intervalos de previsão e é comparada apenas
entre modelos com cobertura adequadamente calibrada.
}
  \end{table}

\end{frame}



\begin{frame}{Curva de treinamento — Modelo campeão}


 Evolução do NELBO ao longo das épocas para o modelo
        \textbf{BiLSTM Difusão}.
 \begin{itemize}
      \item Fase inicial instável, com forte variação do NELBO nas
            primeiras épocas;
      \item Convergência progressiva para um patamar de NELBO mais estável
            após dezenas de épocas;
      \item A melhor época (133) é definida por \textit{early stopping},
            privilegiando estabilidade e capacidade de generalização,
            e não o mínimo pontual da curva.
    \end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=0.65\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/RodrigoPetrus/nelbo.png}
  \caption{Evolução do NELBO durante o treinamento do modelo
  BiLSTM Difusão.}
\end{figure}

\end{frame}

\begin{frame}{Geração de séries — Avaliação qualitativa}

\begin{columns}[c]

% -------- Figura --------
\begin{column}{0.62\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/RodrigoPetrus/distribuicao.png}
\end{column}

% -------- Texto --------
\begin{column}{0.38\textwidth}
\scriptsize
\textbf{Comparação qualitativa}

\vspace{0.5em}

Séries temporais reais versus séries geradas pelo modelo variacional,
comparadas a ajustes ponto-a-ponto baseados em MSE.

\vspace{0.5em}

A abordagem probabilística captura melhor:
\begin{itemize}
  \item a distribuição marginal dos sinais;
  \item a variabilidade temporal;
  \item a incerteza intrínseca dos dados.
\end{itemize}
\end{column}

\end{columns}

\end{frame}


% =======================
% Seção: Discussão e Conclusões
% =======================
\section{Discussão e Conclusões}
\begin{frame}{Discussão dos resultados (modo gerador)}
  \begin{itemize}
    \item \textbf{Qualidade gerativa}:
      \begin{itemize}
        \item Modelos variacionais recorrentes superam o baseline feedforward;
        \item \textbf{BiLSTM  Difusão} apresenta o menor NELBO;
        \item Redução consistente de NLL e MSE frente a LSTM/GRU unidirecionais.
      \end{itemize}

    \item \textbf{Calibração vs. sharpness}:
      \begin{itemize}
        \item Difusão gera intervalos mais estreitos (menor \textit{width\_90});
        \item Cobertura próxima ao alvo de 90\%;
        \item Trade-off equilibrado entre precisão e incerteza.
      \end{itemize}

    \item \textbf{Impacto arquitetural}:
      \begin{itemize}
        \item Bidirecionalidade melhora as representações latentes;
        \item \textit{GRU fuser} aumenta coerência entre janelas;
        \item Maior profundidade temporal favorece geração estável.
      \end{itemize}

    \item \textbf{Treinamento e regularização}:
      \begin{itemize}
        \item Difusão atua como principal regularizador probabilístico;
        \item Dropout e LayerNorm trazem ganhos secundários;
        \item Otimizadores afetam marginalmente o desempenho.
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Conclusões (modo gerador)}
  \begin{itemize}
    \item O framework \textbf{TSDF} é eficaz como modelo gerador
          probabilístico para séries temporais industriais.

    \item Modelos recorrentes superam o baseline feedforward:
      \begin{itemize}
        \item Arquiteturas bidirecionais aprendem latentes mais ricos;
        \item Fusão temporal melhora coerência de longo prazo.
      \end{itemize}

    \item \textbf{Difusão é decisiva}:
      \begin{itemize}
        \item \textbf{BiLSTM Difusão} é o modelo campeão;
        \item Menor NELBO, NLL e MSE, com calibração adequada.
      \end{itemize}

    \item O modelo permite:
      \begin{itemize}
        \item Geração de séries realistas e calibradas;
        \item Simulação de cenários raros ou críticos.
      \end{itemize}

    \item Aplicações futuras:
      \begin{itemize}
        \item Gêmeos digitais industriais;
        \item Data augmentation orientada por incerteza.
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Referências}

\begin{itemize}
  \item Kingma, D. P.; Welling, M. \textit{Auto-Encoding Variational Bayes}. ICLR, 2014.
  \item Hochreiter, S.; Schmidhuber, J. \textit{Long Short-Term Memory}. Neural Computation, 1997.
  \item Cho et al. \textit{Learning Phrase Representations using RNN Encoder–Decoder}. EMNLP, 2014.
  \item Ho, J.; Jain, A.; Abbeel, P. \textit{Denoising Diffusion Probabilistic Models}. NeurIPS, 2020.
  \item Che et al. \textit{Recurrent Neural Networks for Multivariate Time Series with Missing Values}. Sci Rep, 2018.
\end{itemize}

\end{frame}





\end{document}
