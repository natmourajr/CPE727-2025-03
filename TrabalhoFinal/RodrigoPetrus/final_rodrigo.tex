\documentclass[10pt,aspectratio=169]{beamer}

% =======================
% Pacotes básicos
% =======================
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[portuguese]{babel}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{positioning}
% -------- Pipeline base: Encoder + RNN + VAE (mesmo tamanho da variação) --------
\newcommand{\pipelinebase}[1]{%
\centering
\scriptsize
\begin{tikzpicture}[
  x=1cm, y=1cm,
  node distance=1.6cm and 1.2cm,
  every node/.style={transform shape},
  scale=0.85
]

\tikzstyle{box}=[%
  draw, thick, rounded corners=3pt,
  minimum width=2.2cm, minimum height=0.75cm,
  align=center, fill=gray!10
];
\tikzstyle{enc}=[box, fill=blue!12];
\tikzstyle{dec}=[box, fill=orange!18];
\tikzstyle{extra}=[box, fill=green!12];
\tikzstyle{circ}=[draw, thick, circle, minimum size=0.6cm];
\tikzstyle{arrow}=[->, thick];

% ---- arquitetura base (IGUAL PARA TODOS) ----
\node[box] (x) {
  Entrada\\
  $\mathbf{X}=(\mathbf{x},\mathbf{m})_{1:T}$
};

\node[enc, right=of x] (emb) {
  Encoder\\
  (MLP / Embedding)
};

\node[enc, right=of emb] (rnn) {
  RNN Encoder\\
  $\mu_z,\ \log\sigma_z^2$
};

\node[circ, right=of rnn] (z) {$z$};

\node[dec, right=of z] (dec) {
  Decoder\\
  $\mu_x,\ \log\sigma_x^2$
};

% ---- variação (sempre extra) ----
\node[extra, below=1.1cm of rnn, text width=4.2cm] (var) {#1};
\draw[arrow, dashed] (var) -- (rnn);

% ---- setas principais ----
\draw[arrow] (x)   -- (emb);
\draw[arrow] (emb) -- (rnn);
\draw[arrow] (rnn) -- (z);
\draw[arrow] (z)   -- (dec);

\end{tikzpicture}%
}

% -------- Pipeline com variação --------
\newcommand{\BiLSTMPipelineVar}[1]{%
\centering
\scriptsize
\begin{tikzpicture}[
  x=1cm, y=1cm,
  node distance=1.6cm and 1.2cm,
  every node/.style={transform shape},
  scale=0.85
]

\tikzstyle{box}=[%
  draw, thick, rounded corners=3pt,
  minimum width=2.2cm, minimum height=0.75cm,
  align=center, fill=gray!10
];
\tikzstyle{enc}=[box, fill=blue!12];
\tikzstyle{dec}=[box, fill=orange!18];
\tikzstyle{extra}=[box, fill=green!12];
\tikzstyle{circ}=[draw, thick, circle, minimum size=0.6cm];
\tikzstyle{arrow}=[->, thick];

% ---- arquitetura base (IGUAL PARA TODOS) ----
\node[box] (x) {
  Entrada\\
  $\mathbf{X}=(\mathbf{x},\mathbf{m})_{1:T}$
};

\node[enc, right=of x] (emb) {
  Encoder\\
  (MLP / Embedding)
};

\node[enc, right=of emb] (rnn) {
  RNN Encoder\\
  \textbf{BiLSTM}\\
  $\mu_z,\ \log\sigma_z^2$
};

\node[circ, right=of rnn] (z) {$z$};

\node[dec, right=of z] (dec) {
  Decoder\\
  $\mu_x,\ \log\sigma_x^2$
};

% ---- variação (sempre extra) ----
\node[extra, below=1.1cm of rnn, text width=4.2cm] (var) {#1};
\draw[arrow, dashed] (var) -- (rnn);

% ---- setas principais ----
\draw[arrow] (x)   -- (emb);
\draw[arrow] (emb) -- (rnn);
\draw[arrow] (rnn) -- (z);
\draw[arrow] (z)   -- (dec);

\end{tikzpicture}%
}



\title[Modo gerador]{Desenvolver um modelo gerador de séries temporais multivariadas
          para um compressor industrial offshore baseado em RNN e VAEs.}
\author{Rodrigo Petrus Domingues}
\institute{CPE727 -- Deep Learning}
\date{\today}

\begin{document}

% =======================
% Capa
% =======================
\begin{frame}
  \titlepage
\end{frame}

% =======================
% Sumário
% =======================
\begin{frame}{Sumário}
  \tableofcontents
\end{frame}

% =======================
% Seção: Motivação
% =======================
\section{Motivação e Objetivos}

\begin{frame}{Motivação e Objetivos do trabalho}
\noindent \textbf{Motivação:}
    \begin{itemize}
      \item Compressores industriais são ativos críticos (segurança, disponibilidade, custo).
      \item As séries temporais multivariadas de sensores refletem o estado operacional.
      \item Modelos generativos podem simular cenários operacionais variados.
      \item Gêmeos digitais e simulações realistas auxiliam na manutenção preditiva
    \end{itemize}

\noindent \textbf{Objetivos do trabalho:}

  \begin{itemize}
    \item Desenvolver \textbf{modelos geradores} de séries temporais multivariadas
          para um compressor industrial offshore.
    \item Aprender, de forma não supervisionada, a distribuição dos sinais de processo
          em janelas temporais.
    \item Gerar novas trajetórias plausíveis para:
      \begin{itemize}
        \item simulação de cenários,
        \item testes de algoritmos de monitoramento,
        \item análise de variabilidade operacional.
      \end{itemize}
     \end{itemize}
   \noindent \textbf{Comparar arquiteturas:}
        \begin{itemize}
          \item Modelos DEEP, LSTM, GRU, BiLSTM e BiGRU com GRU fuser, todos com otimização Adam;
          \item Variações de treinamento (warmup, scheduler, RAdam, AdamW, variational dropout, difusão e layernorm).
        \end{itemize}


\end{frame}

% =======================
% Seção: Base de dados e pré-processamento
% =======================
\section{Base de dados e pré-processamento}


\begin{frame}{Base de dados e pré-processamento}

\begin{columns}[t]
  
  \begin{column}[t]{0.48\textwidth}
    \textbf{Base de dados}

    \vspace{0.2cm}
    Cognite — sinais reais de sensores de um compressor industrial offshore.

    \vspace{0.3cm}
    \centering
    \includegraphics[width=\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/RodrigoPetrus/12series.png}

    \vspace{0.2cm}
    \scriptsize
    \textit{Exemplo de séries reais utilizadas no estudo (12 sinais simultâneos).}
  \end{column}

  \begin{column}[t]{0.48\textwidth}
    \textbf{Pré-processamento}

    \vspace{0.2cm}
    \begin{itemize}
      \item Estados operacionais:
        \begin{itemize}
          \item Estados originais: 6 (\(0\) a \(5\));
          \item Estados finais: 3 (normal, alerta, falha).
        \end{itemize}

      \item Normalização robusta (RobustScaler) por sensor;
      \item Série temporal com amostras a cada 5 minutos.
      \item Segmentação em janelas:
        \begin{itemize}
          \item \texttt{window\_size = 10}, \texttt{window\_step = 10}
          \item janelas não sobrepostas (modo gerador local).
        \end{itemize}
    \end{itemize}
  \end{column}

\end{columns}
\end{frame}


% =======================
% Seção: Formulação Teórica
% =======================
\section{Formulação do Problema}

\begin{frame}{Definição do problema (modo gerador)}

  \begin{itemize}
    \item Série multivariada: \(\mathbf{x}_t \in \mathbb{R}^{d}\), \(t = 1,\dots,T\), com \(d = 11\) sensores.
    \item Construímos janelas temporais:
      \[
        \mathbf{X} = (x_{t-L+1}, \dots, x_t) \in \mathbb{R}^{L \times d}
      \]
      com \(L = 10\) passos de tempo.
    \item Objetivo: aprender um \textbf{modelo gerador} para as janelas \(\mathbf{X}\):
      \[
        p_{\theta}(\mathbf{X}) \approx p_{\text{dados}}(\mathbf{X})
      \]
    \item Modelo latente (VAE + difusão):
      \begin{itemize}
        \item Encoder mapeia \(\mathbf{X}\) para um código latente \(z\);
        \item Decoder reconstrói \(\hat{\mathbf{X}}\) a partir de \(z\);
        \item O espaço latente é regularizado para permitir \textbf{amostrar novos} \(z\) e gerar janelas sintéticas.
      \end{itemize}
    \item Uso principal:
      \begin{itemize}
        \item Geração de séries realistas para simulação de cenários
              e testes de algoritmos de monitoramento.
      \end{itemize}
  \end{itemize}

  \vspace{0.3cm}
  \noindent O modelo aprende a \textbf{recriar e amostrar} janelas de sensores compatíveis
  com o comportamento real do compressor.
\end{frame}


\begin{frame}{Função de custo (modo gerador)}
\small

\begin{itemize}
  \item O modelo admite uma função de custo multi-termo:
\[
\mathcal{L} =
\lambda_m \mathcal{L}_{\text{miss}}
+ \lambda_v \mathcal{L}_\text{vae}
\]
\end{itemize}

\vspace{0.2cm}

\begin{itemize}
  \item O treinamento é \textbf{não supervisionado}, visando aprender:
    \[
      p_\theta(\mathbf{X}) \approx p_{\text{dados}}(\mathbf{X})
    \]
\end{itemize}

\vspace{0.2cm}

\small

\[
\mathcal{L}_{\text{miss}} = \mathrm{BCE}(m,\hat{m})
\quad\text{com}\quad
\hat{m} = \sigma(f_{\text{miss}}(h))
\]


\[
\mathcal{L}_{\text{vae}}
=
\underbrace{
\mathbb{E}_{q(z|\mathbf{X})}
\big[
-\log p(\mathbf{X}\mid z)
\big]
}_{\text{erro de reconstrução}}
+
\beta\,
\underbrace{
\mathrm{KL}\big(q(z\mid\mathbf{X}) \,\|\, \mathcal{N}(0,I)\big)
}_{\text{regularização do espaço latente}}
\]

\vspace{0.2cm}

\begin{itemize}
   \item BCE entre a máscara verdadeira de observação e a máscara prevista,
forçando o estado latente a codificar o padrão temporal de missing.
  \item O termo de reconstrução garante fidelidade aos sinais reais.
  \item O termo KL força um espaço latente contínuo e amostrável.
  \item Isso permite \textbf{gerar novas janelas sintéticas} de sensores
        com propriedades estatísticas similares às observadas.
\end{itemize}

\end{frame}


\begin{frame}{Arquitetura base e arquiteturas avaliadas}

  \vspace{0.6cm}
   \pipelinebase{
    \textbf{Deep / LSTM / GRU / BiLSTM / BiGRU}
  }
  \vspace{0.35cm}
 % ----------- TEXTO -----------
\small
\begin{itemize}
  \item \textbf{Arquitetura base}: Encoder temporal recorrente acoplado a um VAE latente.
    \item A entrada $\mathbf{X}$ é codificada e passada ao \textbf{RNN encoder},
        que parametriza $(\mu_z,\log\sigma_z^2)$.
  \item Amostramos $z$ e o \textbf{decoder} reconstrói $\hat{\mathbf{X}}$
        com média e variância $(\mu_x,\log\sigma_x^2)$.

  \item \textbf{Arquiteturas avaliadas}:
  \begin{itemize}
    \item \textbf{TSDF\_DEEP}: baseline feedforward (sem recorrência);
    \item \textbf{TSDF\_LSTM}: LSTM unidirecional e \textbf{BiLSTM};
    \item \textbf{TSDF\_GRU}: GRU unidirecional e \textbf{BiGRU}.
  \end{itemize}
\end{itemize}

\end{frame}

% =======================
% Seção: Arquiteturas
% =======================
\section{Arquiteturas e Variações}


\begin{frame}{Variações de treinamento em relação à arquitetura base}
\small

\textbf{Arquitetura base fixa:} Encoder temporal \textbf{BiLSTM} + VAE latente  
(mesmo pipeline do slide anterior).

\vspace{0.3cm}

\textbf{1. Variações de otimização}
\begin{itemize}
  \item \textbf{BiLSTM + Warmup \& Scheduler}  
        Aumenta gradualmente a taxa de aprendizado no início e depois aplica
        um \emph{scheduler} de decaimento, reduzindo instabilidades nas
        primeiras épocas e melhorando a convergência inicial.
  \item \textbf{BiLSTM + RAdam}  
        Substitui o Adam por \textbf{RAdam}, que corrige a variância adaptativa
        nos primeiros passos, tornando o treinamento mais estável.
\end{itemize}

\vspace{0.3cm}

\textbf{2. Variações de regularização}
\begin{itemize}
  \item \textbf{BiLSTM + Variational Dropout}  
        Aplica \emph{variational dropout} ($p=0{,}2$) compartilhado no tempo,
        reduzindo overfitting sem quebrar dependências temporais.
  \item \textbf{BiLSTM + Difusão (missingness)}  
        Acrescenta um objetivo probabilístico extra
        $\mathcal{L} = \lambda_m \mathcal{L}_{miss} + \lambda_v \mathcal{L}_{vae}$,
        forçando o modelo a ser robusto a \emph{missing data} e a calibrar melhor
        incerteza.
  \item \textbf{BiLSTM + LayerNorm}  
        Aplica \textbf{LayerNorm} às ativações da BiLSTM, estabilizando as
        distribuições internas com efeito regularizante indireto.
 \item \textbf{BiLSTM + AdamW (l2/Weight Decay)}  
        Usa \textbf{AdamW} com \emph{weight decay} desacoplado
        ($\lambda = 10^{-4}$), acrescentando regularização L2 explícita
        nos pesos e melhorando generalização.
      \end{itemize}

\end{frame}


\begin{frame}{Variações de treinamento - Otimização}
   \centering \textbf{BiLSTM + Warmup \& Scheduler}
  
  \vspace{0.6cm}
  \BiLSTMPipelineVar{%
    Adam com \textbf{warmup} da taxa de aprendizado\\
    seguido de \textbf{scheduler} de decaimento
  }

  \vspace{0.4cm}
  \begin{itemize}
    \item Aumenta gradualmente a taxa de aprendizado no início do treino;
    \item Evita instabilidades e melhora a convergência inicial.

  \end{itemize}
\end{frame}


\begin{frame}{Variações de treinamento - Otimização}
   \centering \textbf{BiLSTM + RAdam}
  
  \vspace{0.6cm}
 \BiLSTMPipelineVar{
  Otimizador \textbf{RAdam}\\
  (retificação adaptativa da variância)
}


  \vspace{0.4cm}
  \begin{itemize}
    \item Reduz a variância adaptativa do Adam nos primeiros passos;
    \item Torna o treinamento mais estável.
  \end{itemize}
\end{frame}





\begin{frame}{Variações de treinamento - Regularização}
   \centering \textbf{BiLSTM + Variational Dropout}
  
  \vspace{0.6cm}
\BiLSTMPipelineVar{
  \textbf{Variational Dropout} na BiLSTM\\
  (mask fixo no tempo, $p=0{,}2$)
}

  \vspace{0.4cm}
  \begin{itemize}
   \item Regularização explícita no tempo,
              reduzindo overfitting sem quebrar dependências temporais.
        \item Variational\_dropout utilizado: 0.2
  \end{itemize}
\end{frame}


\begin{frame}{Variações de treinamento - Regularização}
   \centering \textbf{BiLSTM + Difusão (missingness)}
  
  \vspace{0.6cm}
\BiLSTMPipelineVar{
  Objetivo extra de \textbf{difusão / missingness}\\
  $\mathcal{L}=\lambda_m\mathcal{L}_{miss}
  +\lambda_v\mathcal{L}_{vae}$
}
  \vspace{0.4cm}
\begin{itemize}
  \item Regularização implícita via objetivos probabilísticos,
        forçando robustez a dados ausentes.
  \item
  \(
  \begin{aligned}
  \mathcal{L} &=
  \lambda_m\, \mathcal{L}_{\text{miss}}
  + \lambda_v\, \mathcal{L}_{\text{vae}}
  \end{aligned}
  \)
\end{itemize}
\end{frame}



\begin{frame}{Variações de treinamento - Regularização}
   \centering \textbf{BiLSTM + LayerNorm}
  
  \vspace{0.6cm}
\BiLSTMPipelineVar{
  \textbf{LayerNorm} aplicada\\
  às saídas da BiLSTM
}

  \vspace{0.4cm}
\begin{itemize}
      \item Estabiliza as ativações internas,
              com efeito regularizante indireto.
\end{itemize}
\end{frame}

\begin{frame}{Variações de treinamento - Regularização}
     \centering \textbf{BiLSTM + AdamW (L2/Weight Decay)}

       \vspace{0.6cm}
  \BiLSTMPipelineVar{%
    Otimizador \textbf{AdamW}\\
    (Adam com \textit{weight decay} desacoplado)
  }

  \vspace{0.4cm}
  \begin{itemize}
    \item Usa AdamW em vez de Adam clássico;
    \item \textit{Weight decay} $= 1\times 10^{-4}$ para regularizar os pesos
          e melhorar generalização.
  \end{itemize}
\end{frame}




% =======================
% Seção: Configuração Experimental
% =======================
\section{Configuração Experimental}

\begin{frame}{Configuração Experimental}
  \begin{itemize}
    \item Divisão treino/teste: 80\% / 20\%.
    \item A quantidade de neurônios já foi validada anteriormente em outra disciplina
    \item Treinamento por 500 épocas, batch size 256.
    \item Paciênica de 50 épocas para early stopping.
    \item Função de perda multi-tarefa conforme descrito.
    \item Otimização e Regularização tratadas como diferentes modelos.
    \item Otimizador: Adam (exceto variações).
    \item Taxa de aprendizado inicial: \(3 \times 10^{-4}\).
    \item Early stopping baseado no NELBO do teste.
    \item Avaliação final no conjunto de teste.
  \end{itemize}
\end{frame}







% =======================
% Seção: Resultados
% =======================
\section{Resultados}

\begin{frame}{Figuras de Mérito utilizadas}
  \begin{itemize}
   \item \textbf{NELBO} (Negative Evidence Lower Bound):
\begin{itemize}
  \item Loss probabilística minimizada no treinamento;
  \item Maximiza implicitamente a verossimilhança (ELBO);
  \item Balanceia reconstrução e regularização latente (KL).
\item
\(
\begin{aligned}
NELBO
&=
\mathbb{E}_{q(z\mid x)}
\!\left[
- \log p(x \mid z)
\right]
+
\mathrm{KL}\!\left(
q(z\mid x)\,\|\,p(z)
\right)
\end{aligned}
\)


\end{itemize}

\item \textbf{NLL} (Negative Log-Likelihood):
  \begin{itemize}
    \item Generaliza o MSE ao modelar explicitamente a variância
          da distribuição predita;
    \item Penaliza erros grandes \emph{e} variâncias mal calibradas
          (super ou subestimação de incerteza).
\item 
\(
\begin{aligned}
\text{NLL}
&=
- \log p(x \mid \mu, \sigma^2)
&=
\frac{(x-\mu)^2}{2\sigma^2}
+ \frac{1}{2}\log\sigma^2
+ \text{cte}
\end{aligned}
\)



  \end{itemize}
    \end{itemize}
\end{frame}


  \begin{frame}{Figuras de Mérito utilizadas}
     \begin{itemize}
    \item \textbf{MSE} (Mean Squared Error):
      \begin{itemize}
         \item Erro médio de reconstrução das séries;
    \item Mede fidelidade gerativa.
      \end{itemize}

  \item \textbf{Cobertura 90\%} (\texttt{cov\_90}):
  \begin{itemize}
    \item Mede a fração de amostras reais que caem dentro do
          intervalo preditivo teórico $[5\%,\,95\%]$;
    \item Avalia se o desvio padrão estimado produz uma
          \textbf{calibração probabilística consistente}
          com a cobertura nominal de 90\%.
  \end{itemize}

\item \textbf{Largura do intervalo 90\%} (\texttt{width\_90}):
  \begin{itemize}
    \item Corresponde à largura teórica do intervalo $[5\%,\,95\%]$
          derivado da distribuição predita;
    \item Quantifica a \textbf{sharpness} do modelo:
          intervalos menores indicam maior confiança,
          desde que a cobertura permaneça bem calibrada.
  \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Resultados quantitativos}
  \begin{table}[h]
    \centering
    \small
\begin{tabular}{lccccc}
\toprule
\textbf{Modelo} 
& \textbf{NELBO $\downarrow$} 
& \textbf{NLL $\downarrow$} 
& \textbf{MSE $\downarrow$}
& \textbf{Cov90 $\rightarrow$ 0.90} 
& \textbf{Width90 $\downarrow$} \\
\midrule
\textbf{BiLSTM Difusão} 
& \textbf{0.518} 
& \textbf{0.462} 
& \textbf{0.243 $\pm$ 0.067} 
& \textbf{0.902} 
& \textbf{2.200} \\

GRU 
& 0.558 
& 0.498 
& 0.300 $\pm$ 0.108 
& 0.905 
& 2.252 \\

LSTM 
& 0.588 
& 0.513 
& 0.332 $\pm$ 0.129 
& 0.900 
& 2.251 \\

BiGRU 
& 0.589 
& 0.499 
& 0.335 $\pm$ 0.136 
& 0.909 
& 2.267 \\

BiLSTM VarDrop 0.2 
& 0.595 
& 0.506 
& 0.317 $\pm$ 0.119 
& 0.898 
& 2.240 \\

BiLSTM RAdam 
& 0.597 
& 0.511 
& 0.341 $\pm$ 0.139 
& 0.898 
& 2.234 \\

BiLSTM Warmup/Sched. 
& 0.598 
& 0.504 
& 0.354 $\pm$ 0.146 
& 0.903 
& 2.251 \\

BiLSTM AdamW 
& 0.601 
& 0.506 
& 0.341 $\pm$ 0.141 
& 0.900 
& 2.224 \\

BiLSTM 
& 0.609 
& 0.527 
& 0.337 $\pm$ 0.133 
& 0.901 
& 2.269 \\

BiLSTM LayerNorm 
& 0.617 
& 0.514 
& 0.342 $\pm$ 0.140 
& 0.907 
& 2.281 \\

DEEP 
& 1.217 
& 1.164 
& 0.693 $\pm$ 0.209 
& 0.916 
& 2.785 \\
\bottomrule
\end{tabular}


  \caption{
Comparação probabilística entre modelos.
O critério principal de seleção é a minimização do NELBO.
A métrica Cov90 avalia calibração probabilística, cujo valor ideal é próximo de 0.90.
A métrica Width90 mede a precisão dos intervalos de previsão e é comparada apenas
entre modelos com cobertura adequadamente calibrada.
}
  \end{table}

\end{frame}



\begin{frame}{Curva de treinamento — Modelo campeão}


 Evolução do NELBO ao longo das épocas para o modelo
        \textbf{BiLSTM Difusão}.
 \begin{itemize}
      \item Fase inicial instável, com forte variação do NELBO nas
            primeiras épocas;
      \item Convergência progressiva para um patamar de NELBO mais estável
            após dezenas de épocas;
      \item A melhor época (133) é definida por \textit{early stopping},
            privilegiando estabilidade e capacidade de generalização,
            e não o mínimo pontual da curva.
    \end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=0.65\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/RodrigoPetrus/nelbo.png}
  \caption{Evolução do NELBO durante o treinamento do modelo
  BiLSTM Difusão.}
\end{figure}

\end{frame}

\begin{frame}{Geração de séries — Avaliação qualitativa}

\begin{columns}[c]

% -------- Figura --------
\begin{column}{0.62\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{/home/ferna/CPE727-2025-03/TrabalhoFinal/RodrigoPetrus/distribuicao.png}
\end{column}

% -------- Texto --------
\begin{column}{0.38\textwidth}
\scriptsize
\textbf{Comparação qualitativa}

\vspace{0.5em}

Séries temporais reais versus séries geradas pelo modelo variacional,
comparadas a ajustes ponto-a-ponto baseados em MSE.

\vspace{0.5em}

A abordagem probabilística captura melhor:
\begin{itemize}
  \item a distribuição marginal dos sinais;
  \item a variabilidade temporal;
  \item a incerteza intrínseca dos dados.
\end{itemize}
\end{column}

\end{columns}

\end{frame}


% =======================
% Seção: Discussão e Conclusões
% =======================
\section{Discussão e Conclusões}
\begin{frame}{Discussão dos resultados (modo gerador)}
  \begin{itemize}
    \item \textbf{Qualidade gerativa}:
      \begin{itemize}
        \item Modelos variacionais recorrentes superam o baseline feedforward;
        \item \textbf{BiLSTM  Difusão} apresenta o menor NELBO;
        \item Redução consistente de NLL e MSE frente a LSTM/GRU unidirecionais.
      \end{itemize}

    \item \textbf{Calibração vs. sharpness}:
      \begin{itemize}
        \item Difusão gera intervalos mais estreitos (menor \textit{width\_90});
        \item Cobertura próxima ao alvo de 90\%;
        \item Trade-off equilibrado entre precisão e incerteza.
      \end{itemize}

    \item \textbf{Impacto arquitetural}:
      \begin{itemize}
        \item Bidirecionalidade melhora as representações latentes;
        \item \textit{GRU fuser} aumenta coerência entre janelas;
        \item Maior profundidade temporal favorece geração estável.
      \end{itemize}

    \item \textbf{Treinamento e regularização}:
      \begin{itemize}
        \item Difusão atua como principal regularizador probabilístico;
        \item Dropout e LayerNorm trazem ganhos secundários;
        \item Otimizadores afetam marginalmente o desempenho.
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Conclusões (modo gerador)}
  \begin{itemize}
    \item O framework \textbf{TSDF} é eficaz como modelo gerador
          probabilístico para séries temporais industriais.

    \item Modelos recorrentes superam o baseline feedforward:
      \begin{itemize}
        \item Arquiteturas bidirecionais aprendem latentes mais ricos;
        \item Fusão temporal melhora coerência de longo prazo.
      \end{itemize}

    \item \textbf{Difusão é decisiva}:
      \begin{itemize}
        \item \textbf{BiLSTM Difusão} é o modelo campeão;
        \item Menor NELBO, NLL e MSE, com calibração adequada.
      \end{itemize}

    \item O modelo permite:
      \begin{itemize}
        \item Geração de séries realistas e calibradas;
        \item Simulação de cenários raros ou críticos.
      \end{itemize}

    \item Aplicações futuras:
      \begin{itemize}
        \item Gêmeos digitais industriais;
        \item Data augmentation orientada por incerteza.
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Referências}

\begin{itemize}
  \item Kingma, D. P.; Welling, M. \textit{Auto-Encoding Variational Bayes}. ICLR, 2014.
  \item Hochreiter, S.; Schmidhuber, J. \textit{Long Short-Term Memory}. Neural Computation, 1997.
  \item Cho et al. \textit{Learning Phrase Representations using RNN Encoder–Decoder}. EMNLP, 2014.
  \item Ho, J.; Jain, A.; Abbeel, P. \textit{Denoising Diffusion Probabilistic Models}. NeurIPS, 2020.
  \item Che et al. \textit{Recurrent Neural Networks for Multivariate Time Series with Missing Values}. Sci Rep, 2018.
\end{itemize}

\end{frame}





\end{document}
