{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sIBm6cMc2Pq",
        "outputId": "861b3a4a-d7ed-48a0-e7ee-88e8da541d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fetching Data for ^GSPC ---\n",
            "\n",
            "================================================================================\n",
            "                       CONSTRUÇÃO DA TABELA DE RESULTADOS                       \n",
            "================================================================================\n",
            "\n",
            ">>> Processando Modelo: CNN\n",
            "   [Tuning] Buscando melhor janela entre [10, 20, 30, 40, 50]...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-748198812.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(TICKER, start=DATES[\"train_start\"], end=DATES[\"test_end\"], interval='1d', progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [Seleção] Melhor janela encontrada para CNN: 10 (Val MAPE: 1.73%)\n",
            "   [Stats] Executando 5 rodadas no dataset de TESTE...\n",
            "\n",
            ">>> Processando Modelo: LSTM\n",
            "   [Tuning] Buscando melhor janela entre [10, 20, 30, 40, 50]...\n",
            "   [Seleção] Melhor janela encontrada para LSTM: 10 (Val MAPE: 2.34%)\n",
            "   [Stats] Executando 5 rodadas no dataset de TESTE...\n",
            "\n",
            ">>> Processando Modelo: Transformer\n",
            "   [Tuning] Buscando melhor janela entre [10, 20, 30, 40, 50]...\n",
            "   [Seleção] Melhor janela encontrada para Transformer: 10 (Val MAPE: 3.01%)\n",
            "   [Stats] Executando 5 rodadas no dataset de TESTE...\n",
            "\n",
            "\n",
            "\n",
            "============================================================\n",
            "TABELA FINAL DE RESULTADOS (MAPE)\n",
            "============================================================\n",
            "+-------------+---------------------+---------------------+\n",
            "| Modelos     | Tamanho da Janela   | Figuras de Mérito   |\n",
            "+=============+=====================+=====================+\n",
            "| Baseline    | -                   | 2.65%               |\n",
            "+-------------+---------------------+---------------------+\n",
            "| CNN         | 10                  | 1.06% ± 0.17%       |\n",
            "+-------------+---------------------+---------------------+\n",
            "| LSTM        | 10                  | 4.41% ± 0.65%       |\n",
            "+-------------+---------------------+---------------------+\n",
            "| Transformer | 10                  | 2.42% ± 0.25%       |\n",
            "+-------------+---------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 20\n",
        "PREDICTION_HORIZON = 1\n",
        "TICKER = \"^GSPC\"\n",
        "WINDOW_SIZES_TO_TEST = [10, 20, 30, 40, 50]\n",
        "DROPOUT_RATE = 0.2\n",
        "NUM_RUNS_FOR_STATS = 5 # Quantidade de rodadas para calcular o desvio padrão (±)\n",
        "\n",
        "DATES = {\n",
        "    \"train_start\": \"2015-01-01\", \"train_end\": \"2021-12-31\",\n",
        "    \"val_start\":   \"2022-01-01\", \"val_end\":   \"2022-12-31\",\n",
        "    \"test_start\":  \"2023-01-01\", \"test_end\":  \"2024-12-31\"\n",
        "}\n",
        "\n",
        "# --- Reproducibility ---\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# --- 1. Data Preparation ---\n",
        "def fetch_raw_data():\n",
        "    print(f\"--- Fetching Data for {TICKER} ---\")\n",
        "    df = yf.download(TICKER, start=DATES[\"train_start\"], end=DATES[\"test_end\"], interval='1d', progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = df.columns.get_level_values(0)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
        "    return df\n",
        "\n",
        "def get_scaled_data(df, start_date, end_date, scaler=None, fit_scaler=False):\n",
        "    slice_df = df.loc[start_date:end_date].copy()\n",
        "    if slice_df.empty:\n",
        "        raise ValueError(f\"No data for {start_date} to {end_date}\")\n",
        "\n",
        "    raw_values = slice_df.values\n",
        "    if fit_scaler:\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_values = scaler.fit_transform(raw_values)\n",
        "    else:\n",
        "        scaled_values = scaler.transform(raw_values)\n",
        "    return scaled_values, scaler, slice_df.index\n",
        "\n",
        "def inverse_transform_close(scaled_data, scaler):\n",
        "    dummy = np.zeros((len(scaled_data), scaler.n_features_in_))\n",
        "    dummy[:, 3] = scaled_data.flatten()\n",
        "    inversed = scaler.inverse_transform(dummy)\n",
        "    return inversed[:, 3]\n",
        "\n",
        "def create_windowed_data(data, window_size, horizon):\n",
        "    X, y = [], []\n",
        "    target_col_idx = 3 # Close price\n",
        "    if len(data) <= window_size + horizon:\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    end_index = len(data) - window_size - horizon + 1\n",
        "    for i in range(end_index):\n",
        "        window = data[i : i + window_size]\n",
        "        target = data[i + window_size + horizon - 1, target_col_idx]\n",
        "        X.append(window)\n",
        "        y.append(target)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --- 2. Model Definitions ---\n",
        "class TimeSeriesCNN(nn.Module):\n",
        "    def __init__(self, num_features, window_size):\n",
        "        super(TimeSeriesCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(num_features, 64, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.drop1 = nn.Dropout(0)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.drop2 = nn.Dropout(0)\n",
        "        final_length = window_size // 4\n",
        "        self.flatten_dim = 128 * final_length\n",
        "        self.fc1 = nn.Linear(self.flatten_dim, 50)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.drop1(self.pool1(self.relu1(self.conv1(x))))\n",
        "        x = self.drop2(self.pool2(self.relu2(self.conv2(x))))\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class TimeSeriesLSTM(nn.Module):\n",
        "    def __init__(self, num_features, hidden_size1=64, hidden_size2=32):\n",
        "        super(TimeSeriesLSTM, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size=num_features, hidden_size=hidden_size1, batch_first=True)\n",
        "        self.dense_inter = nn.Linear(hidden_size1, hidden_size1)\n",
        "        self.relu_inter = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        self.lstm2 = nn.LSTM(input_size=hidden_size1, hidden_size=hidden_size2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm1(x)\n",
        "        out = self.relu_inter(self.dense_inter(out))\n",
        "        out = self.dropout(out)\n",
        "        out, _ = self.lstm2(out)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, num_features, window_size, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.input_linear = nn.Linear(num_features, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                   dim_feedforward=128,\n",
        "                                                   dropout=DROPOUT_RATE,\n",
        "                                                   batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(d_model * window_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_linear(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# --- 3. Helper Functions ---\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    # mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-9))) * 100\n",
        "    # Evitar divisão por zero ou valores muito pequenos de forma mais robusta\n",
        "    y_true_safe = np.where(y_true == 0, 1e-9, y_true)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100\n",
        "    return mape\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in loader:\n",
        "        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_model(model, data_scaled, window_size, scaler):\n",
        "    X, y = create_windowed_data(data_scaled, window_size, PREDICTION_HORIZON)\n",
        "    if len(X) == 0: return float('inf')\n",
        "\n",
        "    loader = DataLoader(StockDataset(X, y), batch_size=BATCH_SIZE, shuffle=False)\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in loader:\n",
        "            batch_X = batch_X.to(DEVICE)\n",
        "            outputs = model(batch_X)\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(batch_y.numpy())\n",
        "\n",
        "    real_preds = inverse_transform_close(np.array(preds), scaler)\n",
        "    real_actuals = inverse_transform_close(np.array(actuals), scaler)\n",
        "    mape = calculate_metrics(real_actuals, real_preds)\n",
        "    return mape\n",
        "\n",
        "# --- 4. Main Pipeline ---\n",
        "if __name__ == \"__main__\":\n",
        "    full_df = fetch_raw_data()\n",
        "\n",
        "    # Scale Data\n",
        "    train_scaled, main_scaler, _ = get_scaled_data(full_df, DATES[\"train_start\"], DATES[\"train_end\"], fit_scaler=True)\n",
        "    val_scaled, _, _             = get_scaled_data(full_df, DATES[\"val_start\"], DATES[\"val_end\"], scaler=main_scaler)\n",
        "    test_scaled, _, _            = get_scaled_data(full_df, DATES[\"test_start\"], DATES[\"test_end\"], scaler=main_scaler)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"{'CONSTRUÇÃO DA TABELA DE RESULTADOS':^80}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Lista para armazenar os resultados finais\n",
        "    final_results = []\n",
        "\n",
        "    # 1. Adicionar Baseline (Hardcoded conforme pedido)\n",
        "    final_results.append({\n",
        "        \"Modelos\": \"Baseline\",\n",
        "        \"Tamanho da Janela\": \"-\",\n",
        "        \"Figuras de Mérito\": \"2.65%\"\n",
        "    })\n",
        "\n",
        "    model_names = [\"CNN\", \"LSTM\", \"Transformer\"]\n",
        "\n",
        "    for model_type in model_names:\n",
        "        print(f\"\\n>>> Processando Modelo: {model_type}\")\n",
        "\n",
        "        # --- PASSO 1: Encontrar o melhor tamanho de janela (Tuning) ---\n",
        "        best_window = None\n",
        "        best_val_mape = float('inf')\n",
        "\n",
        "        print(f\"   [Tuning] Buscando melhor janela entre {WINDOW_SIZES_TO_TEST}...\")\n",
        "\n",
        "        for w_size in WINDOW_SIZES_TO_TEST:\n",
        "            set_seed(42) # Seed fixa para comparação justa na validação\n",
        "\n",
        "            # Preparar dados\n",
        "            X_train, y_train = create_windowed_data(train_scaled, w_size, PREDICTION_HORIZON)\n",
        "            if len(X_train) == 0: continue\n",
        "            train_loader = DataLoader(StockDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "            num_features = X_train.shape[2]\n",
        "\n",
        "            # Instanciar Modelo\n",
        "            if model_type == \"CNN\": model = TimeSeriesCNN(num_features, w_size).to(DEVICE)\n",
        "            elif model_type == \"LSTM\": model = TimeSeriesLSTM(num_features).to(DEVICE)\n",
        "            elif model_type == \"Transformer\": model = TimeSeriesTransformer(num_features, w_size).to(DEVICE)\n",
        "\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "            # Treinar (rápido para seleção)\n",
        "            for epoch in range(EPOCHS):\n",
        "                train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "\n",
        "            # Validar\n",
        "            val_mape = evaluate_model(model, val_scaled, w_size, main_scaler)\n",
        "\n",
        "            if val_mape < best_val_mape:\n",
        "                best_val_mape = val_mape\n",
        "                best_window = w_size\n",
        "\n",
        "        print(f\"   [Seleção] Melhor janela encontrada para {model_type}: {best_window} (Val MAPE: {best_val_mape:.2f}%)\")\n",
        "\n",
        "        # --- PASSO 2: Calcular Estatísticas (Média ± Std) no Teste ---\n",
        "        test_mapes = []\n",
        "        print(f\"   [Stats] Executando {NUM_RUNS_FOR_STATS} rodadas no dataset de TESTE...\")\n",
        "\n",
        "        for run in range(NUM_RUNS_FOR_STATS):\n",
        "            current_seed = 0 + run\n",
        "            set_seed(current_seed) # Variar seed\n",
        "\n",
        "            # Recriar dataset com a janela vencedora\n",
        "            X_train, y_train = create_windowed_data(train_scaled, best_window, PREDICTION_HORIZON)\n",
        "            train_loader = DataLoader(StockDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "            num_features = X_train.shape[2]\n",
        "\n",
        "            if model_type == \"CNN\": model = TimeSeriesCNN(num_features, best_window).to(DEVICE)\n",
        "            elif model_type == \"LSTM\": model = TimeSeriesLSTM(num_features).to(DEVICE)\n",
        "            elif model_type == \"Transformer\": model = TimeSeriesTransformer(num_features, best_window).to(DEVICE)\n",
        "\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "            for epoch in range(EPOCHS):\n",
        "                train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "\n",
        "            # Avaliar no Teste\n",
        "            t_mape = evaluate_model(model, test_scaled, best_window, main_scaler)\n",
        "            test_mapes.append(t_mape)\n",
        "\n",
        "        # Calcular Média e Desvio Padrão\n",
        "        avg_mape = np.mean(test_mapes)\n",
        "        std_mape = np.std(test_mapes)\n",
        "\n",
        "        # Formatar String\n",
        "        result_str = f\"{avg_mape:.2f}% ± {std_mape:.2f}%\"\n",
        "\n",
        "        # Adicionar à tabela\n",
        "        final_results.append({\n",
        "            \"Modelos\": model_type,\n",
        "            \"Tamanho da Janela\": best_window,\n",
        "            \"Figuras de Mérito\": result_str\n",
        "        })\n",
        "\n",
        "    # --- 5. Exibição da Tabela Final ---\n",
        "    df_results = pd.DataFrame(final_results)\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"TABELA FINAL DE RESULTADOS (MAPE)\")\n",
        "    print(\"=\"*60)\n",
        "    # Ajustar opções do pandas para exibir bonito no console\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', 1000)\n",
        "    pd.set_option('display.colheader_justify', 'center')\n",
        "\n",
        "    print(df_results.to_markdown(index=False, tablefmt=\"grid\"))"
      ]
    }
  ]
}