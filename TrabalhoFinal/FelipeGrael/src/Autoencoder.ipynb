{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f51ba54d-6eb3-4063-8939-74f05e3c0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from deep_sentences.autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a96ebe-d096-4cbf-a9d0-3ce7ec7eadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"mteb/stsbenchmark-sts\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c266d0-34bb-4c32-b8f0-84740eab39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = datasets.concatenate_datasets([ds['train'], ds['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bdc23cc-6427-4551-ab60-02a412b3a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.DataFrame(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87bd6962-6a41-4a50-9596-ca8bbf62c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = pd.concat([dev_df['sentence1'], dev_df['sentence2']], ignore_index=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7d5d2d-0597-4fd3-95c2-9d8656315c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=2000,\n",
    "    norm='l2',\n",
    ")\n",
    "tfidf = vectorizer.fit_transform(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "csiw46pft9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (13227, 2000)\n",
      "TF-IDF matrix density: 0.22%\n",
      "Number of non-zero elements: 57105\n",
      "\n",
      "TF-IDF statistics:\n",
      "  Min: 0.000000\n",
      "  Max: 1.000000\n",
      "  Mean: 0.000986\n",
      "  Std: 0.022212\n",
      "\n",
      "Row L2 norms:\n",
      "  Mean: 0.988660\n",
      "  Std: 0.105886\n",
      "  Min: 0.000000\n",
      "  Max: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Check TF-IDF matrix statistics\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix density: {tfidf.nnz / (tfidf.shape[0] * tfidf.shape[1]) * 100:.2f}%\")\n",
    "print(f\"Number of non-zero elements: {tfidf.nnz}\")\n",
    "\n",
    "# Convert to dense for inspection\n",
    "tfidf_dense = tfidf.toarray()\n",
    "print(f\"\\nTF-IDF statistics:\")\n",
    "print(f\"  Min: {tfidf_dense.min():.6f}\")\n",
    "print(f\"  Max: {tfidf_dense.max():.6f}\")\n",
    "print(f\"  Mean: {tfidf_dense.mean():.6f}\")\n",
    "print(f\"  Std: {tfidf_dense.std():.6f}\")\n",
    "\n",
    "# Check row norms (should be 1.0 due to L2 normalization)\n",
    "row_norms = np.linalg.norm(tfidf_dense, axis=1)\n",
    "print(f\"\\nRow L2 norms:\")\n",
    "print(f\"  Mean: {row_norms.mean():.6f}\")\n",
    "print(f\"  Std: {row_norms.std():.6f}\")\n",
    "print(f\"  Min: {row_norms.min():.6f}\")\n",
    "print(f\"  Max: {row_norms.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab0281-2d78-481c-832a-a80b53f54030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "y2qll62n0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1jj6n0n5h9p",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfDataset(Dataset):\n",
    "    \"\"\"Dataset for TF-IDF features.\"\"\"\n",
    "    \n",
    "    def __init__(self, tfidf_matrix):\n",
    "        # Convert sparse matrix to dense tensor\n",
    "        self.features = torch.FloatTensor(tfidf_matrix.toarray())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return feature and dummy target (autoencoder doesn't need labels)\n",
    "        return self.features[idx], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0uc9ucpgnya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10581, Validation size: 2646\n"
     ]
    }
   ],
   "source": [
    "# Create dataset from TF-IDF features\n",
    "dataset = TfidfDataset(tfidf)\n",
    "\n",
    "# Split into train (80%) and validation (20%)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ky0lccb2o2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0  # Use 0 for CPU training to avoid multiprocessing overhead\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "w3si6jvr8p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([64, 2000])\n",
      "Sample batch dtype: torch.float32\n",
      "Sample batch min: 0.000000\n",
      "Sample batch max: 1.000000\n",
      "Sample batch mean: 0.000970\n",
      "Sample batch std: 0.022340\n",
      "Percentage of zeros: 99.80%\n",
      "\n",
      "First sample (first 20 features): tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data to diagnose potential issues\n",
    "sample_batch, _ = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample batch dtype: {sample_batch.dtype}\")\n",
    "print(f\"Sample batch min: {sample_batch.min():.6f}\")\n",
    "print(f\"Sample batch max: {sample_batch.max():.6f}\")\n",
    "print(f\"Sample batch mean: {sample_batch.mean():.6f}\")\n",
    "print(f\"Sample batch std: {sample_batch.std():.6f}\")\n",
    "print(f\"Percentage of zeros: {(sample_batch == 0).float().mean() * 100:.2f}%\")\n",
    "print(f\"\\nFirst sample (first 20 features): {sample_batch[0, :20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9n8w9etfevn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "Encoder: Sequential(\n",
      "  (0): Linear(in_features=2000, out_features=300, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=300, out_features=100, bias=True)\n",
      ")\n",
      "\n",
      "Decoder: Sequential(\n",
      "  (0): Linear(in_features=100, out_features=300, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=300, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Autoencoder model\n",
    "model = Autoencoder(\n",
    "    dimensions=[2000, 300, 100],  # Input -> Hidden -> Latent\n",
    "    activation=\"relu\",\n",
    "    latent_activation=\"linear\",\n",
    "    l1_alpha=0.001,  # L1 regularization on latent space\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"Encoder: {model.encoder}\")\n",
    "print(f\"\\nDecoder: {model.decoder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "x5bkde3yq9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch shape: torch.Size([64, 2000])\n",
      "Test output shape: torch.Size([64, 2000])\n",
      "Test output min: -0.120033, max: 0.123690\n",
      "Test output mean: 0.000968, std: 0.041581\n",
      "\n",
      "Initial losses (before training):\n",
      "  Total loss: 0.002234\n",
      "  Reconstruction loss: 0.002208\n",
      "  L1 loss: 0.000027\n"
     ]
    }
   ],
   "source": [
    "# Test model forward pass and loss calculation before training\n",
    "test_batch, _ = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_batch)\n",
    "    test_loss, test_recon, test_l1 = model.compute_loss(test_batch, test_output)\n",
    "    \n",
    "print(f\"Test batch shape: {test_batch.shape}\")\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "print(f\"Test output min: {test_output.min():.6f}, max: {test_output.max():.6f}\")\n",
    "print(f\"Test output mean: {test_output.mean():.6f}, std: {test_output.std():.6f}\")\n",
    "print(f\"\\nInitial losses (before training):\")\n",
    "print(f\"  Total loss: {test_loss:.6f}\")\n",
    "print(f\"  Reconstruction loss: {test_recon:.6f}\")\n",
    "print(f\"  L1 loss: {test_l1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "jn6ur13ipf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='../checkpoints/autoencoder',\n",
    "    filename='autoencoder-{epoch:02d}-{val_loss:.4f}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "zqujhdgbhb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized. Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/setup.py:175: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    accelerator='cpu',  # Force CPU usage\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "\n",
    "print(f\"Trainer initialized. Using device: {trainer.strategy.root_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ue0udr4mtp",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/.venv/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:881: Checkpoint directory /home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/checkpoints/autoencoder exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ encoder    │ Sequential │  630 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ latent_act │ Identity   │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ decoder    │ Sequential │  632 K │ train │     0 │\n",
       "└───┴────────────┴────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ encoder    │ Sequential │  630 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ latent_act │ Identity   │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ decoder    │ Sequential │  632 K │ train │     0 │\n",
       "└───┴────────────┴────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.3 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.3 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 5                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 9                                                                                           \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.3 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.3 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 5                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 9                                                                                           \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8af45a1825a4a4b98898f5963215c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/.venv/lib/python3.13/site-packages/l\n",
       "ightning/pytorch/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which \n",
       "may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the \n",
       "`DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/.venv/lib/python3.13/site-packages/l\n",
       "ightning/pytorch/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which \n",
       "may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the \n",
       "`DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/.venv/lib/python3.13/site-packages/l\n",
       "ightning/pytorch/trainer/connectors/data_connector.py:434: The 'train_dataloader' does not have many workers which \n",
       "may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the \n",
       "`DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/.venv/lib/python3.13/site-packages/l\n",
       "ightning/pytorch/trainer/connectors/data_connector.py:434: The 'train_dataloader' does not have many workers which \n",
       "may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the \n",
       "`DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sogpcvmg16o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/felipefg/Dropbox/DSc/src/CPE727/CPE727-2025-03/TrabalhoFinal/FelipeGrael/checkpoints/autoencoder/autoencoder-epoch=99-val_loss=0.0004.ckpt\n",
      "\n",
      "Best model metrics:\n",
      "Validation loss: 0.000354\n"
     ]
    }
   ],
   "source": [
    "# Load the best model checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "# Load the best model\n",
    "best_model = Autoencoder.load_from_checkpoint(best_model_path)\n",
    "best_model.eval()\n",
    "\n",
    "print(f\"\\nBest model metrics:\")\n",
    "print(f\"Validation loss: {checkpoint_callback.best_model_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "yg77ppq5psl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([64, 2000])\n",
      "Latent representation shape: torch.Size([64, 100])\n",
      "Mean reconstruction error: 0.000366\n",
      "Std reconstruction error: 0.000128\n"
     ]
    }
   ],
   "source": [
    "# Example: Encode some sentences to 300-dimensional latent space\n",
    "with torch.no_grad():\n",
    "    # Get a batch of data\n",
    "    sample_batch, _ = next(iter(val_loader))\n",
    "    \n",
    "    # Encode to latent space\n",
    "    latent_representations = best_model.encode(sample_batch)\n",
    "    \n",
    "    # Reconstruct\n",
    "    reconstructions = best_model(sample_batch)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    recon_error = torch.mean((sample_batch - reconstructions) ** 2, dim=1)\n",
    "    \n",
    "    print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "    print(f\"Latent representation shape: {latent_representations.shape}\")\n",
    "    print(f\"Mean reconstruction error: {recon_error.mean():.6f}\")\n",
    "    print(f\"Std reconstruction error: {recon_error.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5eddc14-2216-47ad-8f6f-3bd0c2c6a0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_representations.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "949459db-7218-4fdb-87bd-9f9d59bd0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_similarity(row):\n",
    "    batch = vectorizer.transform([row['sentence1'], row['sentence2']])\n",
    "    batch = torch.tensor(batch.toarray()).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent_representation = best_model.encode(batch)\n",
    "\n",
    "    latent_representation = latent_representation.numpy()\n",
    "    return 5 * cosine_similarity(latent_representation[0,:].reshape(1, -1), latent_representation[1,:].reshape(1,-1))[0][0]\n",
    "    #return np.linalg.norm(latent_representation[0,:] - latent_representation[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "abfca909-6b05-4212-9d11-4e7fd9e31543",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df['ae_sim'] = dev_df.apply(autoencoder_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fe52c73f-e570-4dab-930d-2d040d615aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=np.float64(0.49031845585933187), pvalue=np.float64(0.0))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(dev_df['score'], dev_df['ae_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818ee99-5d9c-4532-9da5-325b41bb5f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
