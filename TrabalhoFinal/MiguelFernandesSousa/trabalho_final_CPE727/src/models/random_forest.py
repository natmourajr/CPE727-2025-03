"""
Classificador Random Forest

Implementa√ß√£o de um classificador discriminativo baseado em ensemble de
√°rvores de decis√£o (CART - Classification and Regression Trees).

Modelo:
    Ensemble de T √°rvores de decis√£o treinadas em subsets aleat√≥rios dos dados
    com features aleat√≥rias em cada split.

    Predi√ß√£o: Vota√ß√£o majorit√°ria (classifica√ß√£o) ou m√©dia (regress√£o)
    ≈∑ = mode({h‚ÇÅ(x), h‚ÇÇ(x), ..., h_T(x)})

Vantagens:
    - N√£o requer normaliza√ß√£o de features
    - Robusto a outliers e features irrelevantes
    - Pode capturar rela√ß√µes n√£o-lineares complexas
    - Fornece import√¢ncia de features
    - Reduz overfitting atrav√©s de bagging e feature randomization
"""
from typing import Optional

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import BaseEstimator, ClassifierMixin


class RandomForest(BaseEstimator, ClassifierMixin):
    """
    Wrapper para Random Forest Classifier do scikit-learn

    Mant√©m compatibilidade com a interface do projeto enquanto usa
    implementa√ß√£o otimizada do sklearn.

    Random Forest combina bagging (bootstrap aggregating) com feature
    randomization para criar um ensemble robusto de √°rvores de decis√£o.

    Par√¢metros:
        n_estimators: N√∫mero de √°rvores no ensemble
        max_depth: Profundidade m√°xima das √°rvores (None = sem limite)
        min_samples_split: M√≠nimo de amostras para split interno
        min_samples_leaf: M√≠nimo de amostras em n√≥ folha
        max_features: N√∫mero de features a considerar em cada split
        bootstrap: Se deve usar bootstrap sampling
        max_samples: Fra√ß√£o de amostras para cada √°rvore
        class_weight: Pesos das classes (None ou 'balanced')
        random_state: Seed para reprodutibilidade
        n_jobs: N√∫mero de jobs paralelos (-1 = todos os cores)
    """

    def __init__(
        self,
        n_estimators: int = 100,
        max_depth: Optional[int] = None,
        min_samples_split: int = 2,
        min_samples_leaf: int = 1,
        max_features: str = "sqrt",
        bootstrap: bool = True,
        max_samples: Optional[float] = None,
        class_weight: Optional[str] = None,
        random_state: int = 42,
        n_jobs: int = -1,
        verbose: int = 0,
    ):
        """
        Inicializa o classificador Random Forest

        Args:
            n_estimators: Number of trees (default: 100)
                         More trees ‚Üí better performance but slower
            max_depth: Maximum tree depth (default: None = unlimited)
                      Lower values ‚Üí less overfitting but may underfit
            min_samples_split: Minimum samples to split (default: 2)
            min_samples_leaf: Minimum samples in leaf (default: 1)
            max_features: Features to consider per split (default: 'sqrt')
                         Options: 'sqrt', 'log2', int, float
            bootstrap: Use bootstrap sampling (default: True)
            max_samples: Fraction of samples per tree (default: None = 1.0)
            class_weight: Class weights (default: None)
                         'balanced' for imbalanced datasets
            random_state: Random seed (default: 42)
            n_jobs: Parallel jobs (default: -1 = all cores)
            verbose: Verbosity level (default: 0)
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.max_samples = max_samples
        self.class_weight = class_weight
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.model = None

    def fit(self, X: np.ndarray, y: np.ndarray):
        """
        Treina o Random Forest

        Para cada √°rvore t=1..T:
            1. Amostra bootstrap D_t de D (n amostras com reposi√ß√£o)
            2. Treina √°rvore h_t em D_t com feature randomization:
               - Em cada split, considera apenas m features aleat√≥rias
               - Escolhe melhor split usando crit√©rio de impureza (Gini)
            3. √Årvore cresce at√© max_depth ou min_samples_leaf

        Args:
            X: Features (n_samples, n_features)
            y: Labels (n_samples,)

        Returns:
            self
        """
        self.model = RandomForestClassifier(
            n_estimators=self.n_estimators,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            min_samples_leaf=self.min_samples_leaf,
            max_features=self.max_features,
            bootstrap=self.bootstrap,
            max_samples=self.max_samples,
            class_weight=self.class_weight,
            random_state=self.random_state,
            n_jobs=self.n_jobs,
            verbose=self.verbose,
        )
        self.model.fit(X, y)

        # Armazenar informa√ß√µes √∫teis
        self.classes_ = self.model.classes_
        self.n_features_in_ = X.shape[1]
        self.feature_importances_ = self.model.feature_importances_

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Prediz classes para novos dados

        Para cada amostra x:
            1. Coleta predi√ß√µes de todas as T √°rvores: {h‚ÇÅ(x), ..., h_T(x)}
            2. Retorna classe mais votada (mode):
               ≈∑ = argmax_c Œ£‚Çú ùüô[h_t(x) = c]

        Args:
            X: Features (n_samples, n_features)

        Returns:
            Predi√ß√µes (n_samples,)
        """
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")

        return self.model.predict(X)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Prediz probabilidades de classe

        Para cada amostra x e classe c:
            P(y=c|x) = (1/T) Œ£‚Çú ùüô[h_t(x) = c]
            Propor√ß√£o de √°rvores que votaram na classe c

        Args:
            X: Features (n_samples, n_features)

        Returns:
            Probabilidades (n_samples, n_classes)
        """
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")

        return self.model.predict_proba(X)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        Calcula acur√°cia do modelo

        Args:
            X: Features (n_samples, n_features)
            y: True labels (n_samples,)

        Returns:
            Acur√°cia (0 a 1)
        """
        if self.model is None:
            raise ValueError("Model must be fitted before scoring")

        return self.model.score(X, y)

    def get_feature_importances(self) -> np.ndarray:
        """
        Retorna import√¢ncias das features

        Import√¢ncia calculada como redu√ß√£o m√©dia de impureza (Gini)
        normalizada atrav√©s de todas as √°rvores.

        Returns:
            Feature importances (n_features,)
        """
        if self.model is None:
            raise ValueError("Model must be fitted first")

        return self.feature_importances_

    def get_params(self, deep: bool = True) -> dict:
        """
        Retorna par√¢metros do modelo

        Args:
            deep: Se True, retorna par√¢metros de sub-objetos

        Returns:
            Dicion√°rio com par√¢metros
        """
        return {
            "n_estimators": self.n_estimators,
            "max_depth": self.max_depth,
            "min_samples_split": self.min_samples_split,
            "min_samples_leaf": self.min_samples_leaf,
            "max_features": self.max_features,
            "bootstrap": self.bootstrap,
            "max_samples": self.max_samples,
            "class_weight": self.class_weight,
            "random_state": self.random_state,
            "n_jobs": self.n_jobs,
            "verbose": self.verbose,
        }

    def set_params(self, **params):
        """
        Define par√¢metros do modelo

        Args:
            **params: Par√¢metros a serem definidos

        Returns:
            self
        """
        for key, value in params.items():
            setattr(self, key, value)
        return self


if __name__ == "__main__":
    # Teste do modelo
    print("Testando Random Forest...\n")

    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report

    # Criar dataset de teste
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_classes=10,
        n_informative=15,
        n_redundant=5,
        random_state=42,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Treinar modelo
    print("Treinando modelo com 100 √°rvores...")
    model = RandomForest(n_estimators=100, max_depth=10, random_state=42)
    model.fit(X_train, y_train)

    # Avaliar
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    print(f"\n‚úì Acur√°cia: {acc:.4f}")
    print(f"‚úì Classes encontradas: {model.classes_}")
    print(f"‚úì N√∫mero de features: {model.n_features_in_}")
    print(f"‚úì N√∫mero de √°rvores: {model.n_estimators}")

    # Testar probabilidades
    probs = model.predict_proba(X_test[:5])
    print(f"\n‚úì Probabilidades (primeiras 5 amostras):")
    print(probs)

    # Testar import√¢ncia de features
    importances = model.get_feature_importances()
    print(f"\n‚úì Top 5 features mais importantes:")
    top_indices = np.argsort(importances)[::-1][:5]
    for i, idx in enumerate(top_indices, 1):
        print(f"   {i}. Feature {idx}: {importances[idx]:.4f}")

    print("\n‚úì Random Forest implementado com sucesso!")
