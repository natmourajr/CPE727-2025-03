\documentclass[12pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{setspace}

% Configurações
\onehalfspacing
\setcounter{secnumdepth}{2}

% Título e autores
\title{\textbf{Trabalho Final}\\CPE727 -- Aprendizado Profundo}
\author{Miguel Fernandes de Sousa\\CRID: 125074229\\PEE/COPPE/UFRJ}
\date{Dezembro de 2025}

\begin{document}

\maketitle

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth,height=3cm]{/Users/miguel/Developer/msc/disciplinas/trabalho_final_CPE775/samples/fashionmnis_converted.jpg}
    \caption{Amostras do dataset Fashion MNIST: 50 imagens de diferentes categorias de roupas em escala de cinza (28x28 pixels).}
    \label{fig:fashion_mnist_samples}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Amostras do dataset AG\_NEWS com título e descrição de notícias em diferentes categorias.}
    \label{tab:ag_news_samples}
    \tiny
    \begin{tabular}{|l|p{2.5cm}|p{4cm}|}
        \hline
        \textbf{Classe} & \textbf{Título} & \textbf{Descrição} \\
        \hline
        World & Venezuelans Vote Early in Referendum on Chavez Rule (Reuters) & Reuters - Venezuelans turned out early and in large numbers on Sunday to vote in a historic referendum that will either remove left-wing President Hugo Chavez from office or give him a new mandate to govern for the next two years. \\
        \hline
        Sports & Phelps, Thorpe Advance in 200 Freestyle (AP) & AP - Michael Phelps took care of qualifying for the Olympic 200-meter freestyle semifinals Sunday, and then found out he had been added to the American team for the evening's 400 freestyle relay final. Phelps' rivals Ian Thorpe and Pieter van den Hoogenband and teammate Klete Keller were faster than the teenager in the 200 free preliminaries. \\
        \hline
        Business & Wall St. Bears Claw Back Into the Black (Reuters) & Reuters - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again. \\
        \hline
        Sci/Tech & Group to Propose New High-Speed Wireless Format (Reuters) & Reuters - A group of technology companies including Texas Instruments Inc., STMicroelectronics N.V. and others said they plan to propose a new high-speed wireless format for short-range communications that would operate in the 5 GHz frequency band. \\
        \hline
    \end{tabular}
\end{table}

\section{Identificação}
\begin{itemize}
    \item \textbf{Nome:} Miguel Fernandes de Sousa
    \item \textbf{Período:} 2025/3
    \item \textbf{CRID:} 125074229
\end{itemize}


\section{Resumo}

Este trabalho avalia o desempenho de modelos de aprendizado profundo em problemas de classificação multiclasse, utilizando dois datasets distintos: Fashion MNIST (imagens, 10 classes, 784 features) e AG\_NEWS (texto, 4 classes, 10.000 features TF-IDF para o baseline e sequências tokenizadas de comprimento fixo 200 com vocabulário de 10.000 palavras para a LSTM, com cerca de 40 tokens efetivos por texto). Foram implementados e avaliados modelos de baseline: Naive Bayes (Gaussiano, Bernoulli, Multinomial), Gaussian Mixture Models (GMM), Regressão Logística (Softmax e One-vs-Rest), e Random Forest. Para os modelos de aprendizado profundo, foram utilizados CNN para o Fashion MNIST e LSTM para o AG\_NEWS.

Os experimentos incluíram otimização de hiperparâmetros com validação cruzada por grid search em dois estágios: primeiro em subconjunto pequeno de dados e depois em subconjunto maior de dados, para CNN e LSTM, com rastreamento de métricas, parâmetros e artefatos via MLflow. 

\section{Introdução}
O objetivo do trabalho é fazer um comparativo entre modelos generativos e discriminativos de baseline com os modelos de aprendizado profundo em problemas de classificação multiclasse, utilizando dois datasets distintos: Fashion MNIST (imagens) e AG\_NEWS (texto). A escolha por esses dois datasets se dá pela modalidade distinta dos dados, permitindo avaliar o desempenho de modelos variados.


\section{Bibliografia}
Ng \& Jordan (2002), ao compararem modelos generativos e discriminativos, demonstram que modelos generativos, tal como naive bayes, convergem mais rápido e contam com acurácia melhor quando há poucos dados. Apesar disso, modelos discriminativos, tal como a regressão logística, tendem a superar os generativos à medida que a quantidade de dados aumenta. Em geral, conjuntos de dados com poucos dados tendem a se beneficiar do naive bayes.

Por sua vez, Zheng et al. (2023) revisitam o trabalho de Ng \& Jordan (2002), identificando que o fenômeno descrito ocorre não apenas como atributos "brutos" de modelos supervisionados, mas também ao utilizar atributos pré-treinados de redes neurais profundas para treinar classificadores lineares, inclusive em classificadores multiclasses.

Bouzidi et al. (2024) fizeram um levantamento comparando a literatura de CNNs e Vision Transformers (ViTs) para classificação em Fashion MNIST. Os resultados indicam que CNNs superaram ViTs em acurácia: 99.18\% vs. 95.25\%. Os autores justificam essa diferença em razão da baixa dimensionalidade do Fashion MNIST, o que cria padrões locais de textura, borda e outros atributos, o que é mais eficientemente identificado pelos filtros da arquitetura da CNN.  ViTs, por sua vez, foram projetados para datasets enormes, em que o contexto global é capturado pelo mecanismo de atenção, o que seria desnecessário ou um ``exagero de uso'' no caso do Fashion MNIST, pois demanda mais treinamento para convergir.

Finalmente, Ozdemir (2024) comparou CNN, BiLSTM e o transformer BERT para classificação de notícias no dataset AG News. Nesse caso, o modelo BERT alcançou 94.42\% de acurácia, superando a CNN (91.20\%) e a BiLSTM (91.10\%). Segundo o autor, isso ocorre  porque textos de notícias possuem natureza semântica e sequencial, ao contrário de imagens, com natureza espacial. Além disso, por mais que a LSTM tenha sua arquitetura projetada para dados sequenciais, ela não possui o mecanismo de atenção, capaz de capturar correspondências não-sequenciais do contexto geral. Lembrando que o modelo BERT foi pré-treinado em bilhões de palavras.

Como trabalho final da disciplina de Aprendizado de Máquina, este projeto contribui para a validação dos conceitos teóricos aprendidos na disciplina com datasets conhecidos e de fácil reprodução. O projeto apresentado apresenta um framework flexível e extensível, sendo prático de colocar novos modelos ou novos datasets. Há também o uso do MLFlow, viabilizando versionamento dos modelos e artefatos.


\section{Metodologia}

\subsection{Descrição dos Conjuntos de Dados}
Foram utilizados dois datasets: Fashion MNIST, para classificação de 10 categorias de roupas a partir de imagens 28x28 pixels, totalizando 784 features; e o dataset AG\_NEWS, para classificação de 4 categorias de notícias. No caso do baseline, as features de entrada são obtidas a partir de vetorização TF-IDF, limitado a 10.000 features; para a LSTM, os textos são tokenizados em nível de palavra com vocabulário de 10.000 termos e sequências padronizadas para 200 tokens (padding/truncamento). O Fashion MNIST conta com 70 mil imagens, enquanto o AG\_NEWS conta com 127.600 pares de título e descrição curta, geralmente uma frase, sobre o corpo da notícia (que não consta no dataset).

\subsubsection{Fashion MNIST (Imagens)}

Fashion MNIST 

\begin{itemize}
    \item \textbf{Total de amostras:} 70.000 imagens
    \begin{itemize}
        \item Treino original: 60.000 (dividido em 48.000 treino + 12.000 validação)
        \item Teste: 10.000
    \end{itemize}
    \item \textbf{Dimensionalidade:} 28x28 pixels = 784 features (após achatamento)
    \item \textbf{Tipo de dado:} Escala de cinza, normalizado para [0.0, 1.0], float32
    \item \textbf{Classes:} 10 categorias com cerca de 4.800 amostras por classe no conjunto de treinamento:
    \begin{enumerate}
        \item T-shirt
        \item Trouser
        \item Pullover
        \item Dress
        \item Coat
        \item Sandal
        \item Shirt
        \item Sneaker
        \item Bag
        \item Ankle boot
    \end{enumerate}
\end{itemize}

A escolha pelo Fashion MNIST se dá por ser um problema de classificação de imagens, com uma quantidade razoável de features de entrada e tamanho razoável para o dataset. As features de entrada sendo todas Float facilitam muito o pré-processamento.

\subsubsection{AG\_NEWS (Texto)}

AG's News Topic Classification Dataset
\begin{itemize}
    \item \textbf{Total de amostras:} 127.600 notícias
    \item \textbf{Representação dos dados:}
    \begin{itemize}
        \item \textbf{Modelos baseline:} Representação em forma de "bag-of-words" via TF-IDF, sem o uso de embeddings.
        \item \textbf{Modelos deep learning (LSTM):} Representação tokenizada com sequências de índices de palavras (word-level tokenization).
    \end{itemize}
    \item \textbf{Divisão dos dados:}
    \begin{itemize}
        \item Treino original: 120.000 (dividido em 96.000 treino + 24.000 validação)
        \item Teste: 7.600
    \end{itemize}
    \item \textbf{Dimensionalidade:}
    \begin{itemize}
        \item \textbf{TF-IDF (baseline):} Limitado a 10.000 features, descarta termos que apareçam em mais de 50\% dos documentos (artigos, preposições, termos que não trazem informação) e em menos de 5 documentos (termos que possam ser ruído ou erros de digitação), compondo os limites superior e inferior de frequência dos termos.
        \item \textbf{Tokenização (LSTM):} Vocabulário de 10.000 palavras mais frequentes, sequências de comprimento fixo de 200 tokens com padding.
    \end{itemize}
    \item \textbf{Tipo de dado:}
    \begin{itemize}
        \item \textbf{TF-IDF:} Texto transformado via TF-IDF, com ``term frequency'' em escala log e normalização l2, com módulo máximo igual a 1.
        \item \textbf{Tokenização:} Sequências de inteiros representando índices de palavras no vocabulário.
    \end{itemize}
    \item \textbf{Classes:} 4 categorias balanceadas (30.000 amostras por classe no treino):
    \begin{enumerate}
        \item World
        \item Sports
        \item Business
        \item Sci/Tech
    \end{enumerate}
\end{itemize}

O dataset AG\_NEWS complementa Fashion MNIST ao fornecer um dataset textual com características distintas, com maior dimensionalidade e esparsidade, mas com features menos dependentes entre si, se comparadas a tarefas de classificação de imagem.

\subsubsection{Detalhamento da Representação TF-IDF}

A representação TF-IDF (Term Frequency-Inverse Document Frequency) transforma cada documento em um vetor de 10.000 dimensões, onde cada posição corresponde a uma palavra do vocabulário. Por exemplo, considere o documento ``Apple launches new iPhone'':

\begin{verbatim}
Vocabulário (exemplo simplificado):
  "apple" → posição 0
  "launches" → posição 1
  "new" → posição 2
  "iphone" → posição 3
  ... (até 10.000 palavras)

Vetor TF-IDF resultante:
[0.234, 0.189, 0.156, 0.201, 0.0, 0.0, ..., 0.0]
  ↑      ↑      ↑      ↑      ↑    ↑           ↑
 apple launches new  iphone  outras palavras (zeros)
\end{verbatim}

Cada posição do vetor corresponde a uma palavra do vocabulário com posição fixa. Os valores são scores TF-IDF (números reais entre 0.0 e $\sim$1.0). Aproximadamente 99.8\% dos valores são zeros, caracterizando uma representação extremamente esparsa.

O valor TF-IDF para cada palavra é calculado como o produto de dois componentes:

\begin{equation}
\text{TF-IDF}(w, d) = \text{TF}(w, d) \times \text{IDF}(w)
\end{equation}

Onde o componente de frequência do termo (TF) é dado por:

\begin{equation}
\text{TF}(w, d) = \begin{cases}
  1 + \log(\text{count}(w, d)) & \text{se count}(w, d) > 0 \\
  0 & \text{caso contrário}
\end{cases}
\end{equation}

E o componente de frequência inversa de documento (IDF) é dado por:

\begin{equation}
\text{IDF}(w) = \log\left(\frac{N + 1}{df(w) + 1}\right) + 1
\end{equation}

Onde $\text{count}(w, d)$ é a frequência da palavra $w$ no documento $d$, $N$ é o número total de documentos (120.000 no treino do AG\_NEWS), e $df(w)$ é o número de documentos que contêm a palavra $w$ (document frequency).

Após a aplicação do TF-IDF, cada documento é normalizado usando norma L2 ($\|\mathbf{x}\|_2 = 1$), resultando em vetores unitários com valores individuais tipicamente entre 0 e 0.5. A matriz TF-IDF resultante apresenta esparsidade de aproximadamente 99.8\% (ou seja, 98\% dos valores são zeros), característica fundamental que influencia o desempenho de modelos generativos. A alta esparsidade resulta em menor correlação entre features comparada aos pixels de imagens, onde pixels vizinhos tendem a ter valores similares devido à continuidade espacial. Esta diferença estrutural explica parcialmente por que modelos generativos como Naive Bayes têm desempenho significativamente melhor em dados textuais (gap de 1.68 pp) do que em imagens (gap de 22.29 pp).

\subsubsection{Detalhamento da Representação Tokenizada para LSTM}

Para o modelo LSTM, foi utilizada uma representação tokenizada, uma vez que a TF-IDF não é compatível com a LSTM. Enquanto TF-IDF gera vetores densos de valores reais (float), a tokenização gera sequências de inteiros que representam índices de palavras no vocabulário, formato adequado para camadas de embedding neurais.

O processo de tokenização segue as seguintes etapas:

\textbf{1. Tokenização em nível de palavra:}

O texto é segmentado em tokens (palavras individuais) utilizando expressões regulares que extraem sequências alfanuméricas. Por exemplo, o documento ``Apple launches new iPhone'' é tokenizado em [``apple'', ``launches'', ``new'', ``iphone''], aplicando lowercase para normalização.

\textbf{2. Construção do vocabulário:}

É construído um vocabulário contendo as 10.000 palavras mais frequentes no conjunto de treinamento, além de dois tokens especiais: \texttt{<PAD>} (índice 0, para padding) e \texttt{<UNK>} (índice 1, para palavras desconhecidas). Cada palavra recebe um índice único de 0 a 9.999.

\textbf{3. Conversão texto → sequência:}

Cada documento é convertido em uma sequência de índices. Por exemplo, considerando um vocabulário simplificado:

\begin{verbatim}
Vocabulário:
  <PAD> → 0
  <UNK> → 1
  "apple" → 2
  "launches" → 3
  "new" → 4
  "iphone" → 5
  ...

Texto: "Apple launches new iPhone"
Sequência: [2, 3, 4, 5]
\end{verbatim}

\textbf{4. Padding e truncamento:}

Todas as sequências são ajustadas para comprimento fixo de 200 tokens. Sequências menores são preenchidas com zeros (padding) no final, enquanto sequências maiores são truncadas. Por exemplo:

\begin{verbatim}
Sequência original (4 tokens): [2, 3, 4, 5]
Após padding (200 tokens):     [2, 3, 4, 5, 0, 0, 0, ..., 0]
                                ↑           ↑
                              conteúdo   196 zeros
\end{verbatim}

A matriz tokenizada resultante tem dimensões (n\_amostras, 200), onde cada elemento é um inteiro entre 0 e 9.999. A análise estatística das sequências mostra que o comprimento médio dos textos é de aproximadamente 40 tokens, com mínimo de 12 e máximo de 183 tokens. Consequentemente, aproximadamente 80\% dos valores na matriz são zeros (padding), caracterizando também alta esparsidade, mas de natureza distinta da TF-IDF.

A principal diferença conceitual entre as duas representações é que TF-IDF cria um vetor onde cada posição representa uma palavra específica do vocabulário (bag-of-words), descartando informação de ordem. Já a tokenização preserva a ordem sequencial das palavras, permitindo que modelos recorrentes (LSTM) capturem dependências temporais e contexto sequencial do texto. Por exemplo, ``não gostei'' vs. ``gostei muito'' têm vetores TF-IDF similares (ambos contêm ``gostei''), mas sequências tokenizadas completamente diferentes que preservam o sentido oposto.

\\
\textbf{Tokenização (AG\_NEWS)}. A tokenização é em nível de palavra, com vocabulário de 10.000 termos mais frequentes e dois tokens especiais: \texttt{<PAD>} (0) e \texttt{<UNK>} (1). Cada texto é convertido em uma sequência de índices e padronizado para comprimento fixo de 200 tokens (padding/truncamento). Sequências menores recebem zeros no final, sequências maiores são truncadas. O comprimento médio dos textos é de \textasciitilde40 tokens (mín. 12, máx. 183), resultando em cerca de 80\% de padding. Esta representação preserva a ordem das palavras, permitindo que a LSTM capture dependências temporais e contexto sequencial.

\subsection{Análise Exploratória dos Dados}

\subsubsection{Fashion MNIST}
Na análise exploratória do Fashion MNIST, algumas visualizações tornam possível observar que regiões de features são muito correlatas com algumas classes.
Por exemplo, para a seção vertical do centro, há uma correlação alta com as classes T-shirt, shirt, top, pullover. Por sua vez, o retângulo horizontal centralizado está muito correlato às classes de calçados.

As visualizações que tornam possível visualizar essa relação mais claramente são tanto a imagem gerada a partir das médias de cada classe, assim como a visualização de variância por pixel de cada classe (Figura~\ref{fig:fashion_pixel_variance_clothing_footwear}).
Essa característica, por intuição, indica que essas regiões são as que mais explicam a maior concentração da variância da base de dados.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../../eda/outputs/fashion_mnist/pixel_variance_clothing_vs_footwear.png}
    \caption{Variância dos pixels: roupas vs calçados no Fashion MNIST.}
    \label{fig:fashion_pixel_variance_clothing_footwear}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../../eda/outputs/fashion_mnist/matrix.png}
    \caption{Matriz de separabilidade entre classes no Fashion MNIST.}
    \label{fig:fashion_separability}
\end{figure}


\subsubsection{AG\_NEWS}

O dataset AG\_NEWS apresenta 120.000 amostras de treinamento balanceadas, cerca de 30.000 por classe, distribuídas entre as categorias World, Sports, Business e Sci/Tech. Os textos são curtos, com média de aproximadamente 35-40 palavras por documento.

O vocabulário inicial do dataset é de 17.932 termos distintos para uma amostra de 5.000 documentos, com a remoção de stopwords resultando em uma redução mínima de apenas 1,5\%, indicando que o texto já é relativamente limpo. Após aplicação dos filtros de frequência (min\_df=5, max\_df=0.5) e limitação a 10.000 features mais relevantes, obteve-se um vocabulário final otimizado. A distribuição de frequências dos tokens seguiu a Lei de Zipf, com aproximadamente 2.631 termos necessários para cobrir 80\% do corpus e 32.444 termos para 99\% de cobertura. A matriz TF-IDF resultante apresentou 99,8\% de zeros, com valores diferentes de zero concentrados no intervalo de 0,1 a 0,3 (Figura~\ref{fig:agnews_tfidf_sparsity}). 

 Ao listar os termos com as maiores log-odds por classe, é possível identificar termos bem específicos de cada classe (Figura~\ref{fig:agnews_top_terms}): a classe World apresenta termos geopolíticos e de conflitos (``gaza'', ``darfur'', ``arafat'', ``sudan'', ``hostages''), Sports contém termos esportivos tipicamente norte-americanos (``nhl'', ``touchdowns'', ``pacers'', ``pga'', ``semifinals''), Business concentrou-se em termos financeiros e corporativos (``fullquote'', ``fannie'', ``kmart'', ``treasuries'', ``economists''), enquanto Sci/Tech apresenta vocubulário alusivo a computação, indústria aeroespacial, entre outros (``mars'', ``spyware'', ``mozilla'', ``sp2'', ``worm'', ``viruses''). Na representação tokenizada usada pela LSTM, cada notícia é truncada ou preenchida para 200 posições, com média de 35--40 tokens reais por documento (cerca de 80\% de padding), reforçando a similaridade de esparsidade, porém agora em um formato sequencial.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../../eda/outputs/ag_news/top_terms_heatmap.png}
    \caption{Mapa de calor dos termos mais frequentes no AG\_NEWS.}
    \label{fig:agnews_top_terms}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../../eda/outputs/ag_news/tfidf_sparsity.png}
    \caption{Esparsidade da matriz TF-IDF no AG\_NEWS.}
    \label{fig:agnews_tfidf_sparsity}
\end{figure}



\subsection{Pré-Processamento}

Para o pré-processamento, o conjunto Fashion MNIST utilizou normalização min-max: o valor mínimo de 0 foi mapeado em -1 e o valor máximo de 255 foi mapeado em +1. Essa escolha se deu 1) pela maior previsibilidade dos dados de entrada e 2) pela distribuição de intensidade não seguir uma distribuição normal, pelo contrário: a maioria dos pixels está justamente nos dois extremos da faixa de valores. A matriz de pixels foi convertida em um vetor de tamanho 784. \textbf{Exceção:} Para o modelo MultinomialNB (que requer valores não-negativos), a normalização foi ajustada para o intervalo [0, 1].

Por sua vez, o AG\_NEWS utilizou limitação dos termos mais frequentes e remoção de stop words (parcela reduzida do vocabulário). A quantidade máxima de features foi limitada em 10.000, ignorando termos que aparecem em mais da metade dos documentos e termos pouco frequentes (menos de 5 documentos). A vetorização foi feita com TF-IDF, utilizando frequência logarítmica e normalização L2.


\subsection{Modelos de Baseline}

Os modelos utilizados foram divididos em duas categorias: modelos generativos (Naive Bayes Gaussiano, Naive Bayes Bernoulli, Naive Bayes Multinomial, e Gaussian Mixture Models) e modelos discriminativos (Regressão Logística com Softmax, Regressão Logística One-vs-Rest, e Floresta Aleatória).

Para o modelo de mistura de gaussianas (GMM), foram avaliadas configurações com uma a quatro componentes por classe, permitindo melhor acomodação à distribuição dos dados. A matriz de covariância diagonal foi utilizada para ambos datasets devido à alta dimensionalidade (784 features no Fashion MNIST, 10.000 no AG\_NEWS). Foram utilizadas 5 inicializações aleatórias com máximo de 100 iterações para AG\_NEWS e 200 para Fashion MNIST.

No caso dos modelos de Naive Bayes, há modelos para diferentes distribuições na biblioteca scikit-learn. Foram avaliados o GaussianNB (assume distribuição normal, adequado para features contínuas), BernoulliNB (assume distribuição binomial, adequado para features binárias), e MultinomialNB (assume distribuição multinomial, adequado para features de contagem/frequência) para ambos os datasets.

Para a regressão logística, foi utilizada a regularização L2 (Ridge), avaliando diferentes valores para o parâmetro de penalização C (inverso da força de regularização). Foram implementadas duas variantes: a classificação um-contra-todos (One-vs-Rest) e a softmax (multinomial), ambas disponíveis na biblioteca scikit-learn.

\subsection{Modelos de Aprendizado Profundo}

Para os modelos de aprendizado profundo, foram implementadas duas arquiteturas distintas adequadas às características de cada dataset: uma Rede Neural Convolucional (CNN) para o Fashion MNIST e uma LSTM (Long Short-Term Memory) para o AG\_NEWS.

\subsubsection{CNN para Fashion MNIST}

Foi implementada uma CNN básica para classificação de imagens do Fashion MNIST. A arquitetura segue o paradigma clássico de redes convolucionais com camadas de convolução, pooling e camadas totalmente conectadas:

\textbf{Arquitetura:}
\begin{itemize}
    \item \textbf{Entrada:} Imagem 1x28x28 (escala de cinza)
    \item \textbf{Conv1:} 1 canal de entrada → 32 filtros 3x3, seguido de ReLU e MaxPool 2x2 → saída 32x13x13
    \item \textbf{Conv2:} 32 canais → 64 filtros 3x3, seguido de ReLU e MaxPool 2x2 → saída 64x5x5
    \item \textbf{Flatten:} 64x5x5 → 1600 features
    \item \textbf{FC1:} 1600 → 128 neurônios, seguido de ReLU e Dropout
    \item \textbf{FC2:} 128 → 10 classes (saída)
\end{itemize}

A escolha de filtros 3x3 permite capturar padrões locais de textura e bordas característicos de roupas. O MaxPooling reduz a dimensionalidade espacial enquanto preserva características relevantes. O Dropout é aplicado após a primeira camada totalmente conectada para regularização e prevenção de overfitting.

\subsubsection{LSTM para AG\_NEWS}

Para o dataset AG\_NEWS, foi implementada uma arquitetura LSTM que processa sequências de tokens, capturando dependências temporais e contexto sequencial do texto:

\textbf{Arquitetura:}
\begin{itemize}
    \item \textbf{Entrada:} Sequência de índices de tokens (batch\_size, 200)
    \item \textbf{Embedding:} vocab\_size (10.000) → embedding\_dim, com padding\_idx=0
    \item \textbf{LSTM:} embedding\_dim → hidden\_dim, com opção de bidirecionalidade
    \item \textbf{Hidden State:} Último estado oculto da LSTM (ou concatenação dos estados forward/backward se bidirecional)
    \item \textbf{Dropout:} Aplicado ao hidden state
    \item \textbf{FC:} hidden\_dim (ou 2×hidden\_dim se bidirecional) → 4 classes (saída)
\end{itemize}

A camada de embedding converte índices de palavras em vetores densos de dimensão fixa, aprendidos durante o treinamento. A LSTM processa a sequência token por token, mantendo um estado oculto que acumula informação contextual. A opção de LSTM bidirecional permite capturar contexto tanto da esquerda para direita quanto da direita para esquerda.

Ambos os modelos utilizam CrossEntropyLoss como função de perda e otimizador Adam para treinamento. A implementação inclui um wrapper PyTorchClassifier que fornece interface compatível com scikit-learn, facilitando integração com pipeline de experimentos.

\subsection{Otimização de Hiperparâmetros para Baseline}

Para a otimização de hiperparâmetros, foi utilizado o Grid Search com validação cruzada estratificada (Stratified K-Fold). Para o Fashion MNIST, foram utilizadas 5 divisões (5-fold CV), enquanto para o AG\_NEWS foram utilizadas 2 divisões primeiro para parte dos hiperparametros e 3 divisões em seguida para os demais (``2-3-fold CV''), para agilizar o processo, devido ao maior tamanho do dataset e problemas de estouro de memória ao treinar em paralelo, acarretando em uma validação cruzada sequencial que dura mais tempo. O critério de avaliação foi a acurácia (accuracy), com registro de F1 score, precisão e recall. A biblioteca MLFlow foi utilizada para registrar o histórico dos resultados dos modelos.

Os hiperparâmetros otimizados para cada modelo foram:

\begin{itemize}
    \item \textbf{Naive Bayes Gaussiano:} var\_smoothing $\in$ \{1e-09, 1e-08, 1e-07, 1e-06, 1e-05\}
    \item \textbf{GMM:} n\_components $\in$ \{1, 2, 3, 4\}, covariance\_type $\in$ \{`full', `diag'\}, max\_iter = 100, n\_init = 5
    \item \textbf{Regressão Logística (Softmax e OvR):} C $\in$ \{0.01, 0.1, 1.0, 10.0\}, solver = `lbfgs', max\_iter = 1000
    \item \textbf{Random Forest:} n\_estimators $\in$ \{100, 200\}, max\_depth $\in$ \{None, 10, 20\}, min\_samples\_split $\in$ \{2, 5\}, max\_features $\in$ \{`sqrt', `log2'\}
\end{itemize}

Para o dataset Fashion MNIST, os melhores hiperparâmetros encontrados foram:
\begin{itemize}
    \item \textbf{Naive Bayes:} var\_smoothing = 1e-09 (melhor F1 macro CV: 0.5065)
    \item \textbf{GMM:} n\_components = 1, covariance\_type = `diag' (GMM não convergiu adequadamente com covariância completa)
    \item \textbf{Logistic Softmax:} C = 1.0 (melhor accuracy CV: $\sim$84\%)
    \item \textbf{Logistic OvR:} C = 0.1 (melhor accuracy CV: $\sim$83\%)
    \item \textbf{Random Forest:} n\_estimators = 200, max\_depth = None, min\_samples\_split = 2, max\_features = `sqrt' (melhor accuracy CV: 88.05\%)
\end{itemize}

Para o dataset AG\_NEWS, os melhores hiperparâmetros encontrados foram:
\begin{itemize}
    \item \textbf{Naive Bayes Gaussiano:} var\_smoothing = 1e-05 (melhor accuracy CV: 80.31\%)
    \item \textbf{Naive Bayes Bernoulli:} alpha = 0.1, binarize = 0.0 (melhor accuracy CV: 89.17\%)
    \item \textbf{Naive Bayes Multinomial:} alpha = 0.01 (melhor accuracy CV: 87.92\%)
    \item \textbf{GMM:} n\_components = 1, covariance\_type = `diag', n\_init = 5, max\_iter = 100 (melhor accuracy CV: 80.97\%)
    \item \textbf{Logistic Softmax:} C = 1.0, solver = `lbfgs', max\_iter = 1000 (melhor accuracy CV: 89.90\%)
    \item \textbf{Logistic OvR:} C = 10.0, solver = `lbfgs', max\_iter = 1000 (melhor accuracy CV: 89.76\%)
    \item \textbf{Random Forest:} n\_estimators = 300, max\_depth = None, min\_samples\_split = 5, max\_features = `log2' (melhor accuracy CV: 87.70\%)
\end{itemize}

\subsection{Otimização de Hiperparâmetros para Aprendizado Profundo}

Para os modelos de aprendizado profundo, foi adotada uma estratégia de otimização de hiperparâmetros em dois estágios para reduzir o custo computacional e evitar problemas de memória. Esta abordagem é similar à estratégia de duas fases utilizada nos modelos baseline, mas adaptada para as características específicas de redes neurais.

\subsubsection{Estratégia de Dois Estágios}

\textbf{Estágio 1 (amostra pequena):} Exploração rápida do espaço de hiperparâmetros utilizando um subconjunto pequeno dos dados (500 amostras para Fashion MNIST, 1.000 para AG\_NEWS). O objetivo é identificar rapidamente quais hiperparâmetros são mais promissores. Utiliza poucas épocas (3) e validação cruzada com 3 folds para agilizar o processo.

\textbf{Estágio 2 (amostra maior):} Refinamento dos hiperparâmetros mais promissores identificados no Estágio 1, utilizando um subconjunto maior dos dados (5.000 amostras para Fashion MNIST, 10.000 para AG\_NEWS). Aumenta o número de épocas (10 para CNN, 5 para LSTM devido ao maior tamanho do dataset) e utiliza 2-3 folds para avaliação mais robusta.

Após identificar os melhores hiperparâmetros nos dois estágios, o modelo final é treinado no conjunto completo de dados com maior número de épocas (20) para maximizar o desempenho.

\subsubsection{CNN para Fashion MNIST}

Os hiperparâmetros otimizados via Grid Search foram:

\textbf{Estágio 1 (amostra pequena - 500 amostras):}
\begin{itemize}
    \item learning\_rate $\in$ \{0.001, 0.01\}
    \item dropout $\in$ \{0.3, 0.5\}
    \item batch\_size $\in$ \{32, 64\}
    \item epochs = 3
\end{itemize}

\textbf{Estágio 2 (amostra maior - 5.000 amostras):}
\begin{itemize}
    \item learning\_rate $\in$ \{0.0005, 0.001, 0.002\}
    \item dropout $\in$ \{0.3, 0.4, 0.5\}
    \item batch\_size $\in$ \{32, 64\}
    \item epochs = 10
    \item conv\_channels $\in$ \{(32, 64)\}
    \item kernel\_size $\in$ \{3\}, padding $\in$ \{\texttt{same}\}
    \item fc\_units $\in$ \{128\}, use\_batchnorm $\in$ \{True\}
\end{itemize}

\textbf{Melhores hiperparâmetros encontrados:}
\begin{itemize}
    \item \textbf{Estágio 1:} learning\_rate = 0.01, dropout = 0.3, batch\_size = 32, epochs = 3 (melhor CV accuracy: 67.60\% $\pm$ 0.40\%, 24 configurações testadas)
    \item \textbf{Estágio 2:} learning\_rate = 0.001, dropout = 0.4, batch\_size = 32, epochs = 10, conv\_channels = (32,64), kernel\_size = 3, padding = \texttt{same}, fc\_units = 128, use\_batchnorm = True (melhor CV accuracy: 87.28\%)
    \item \textbf{Treinamento final:} learning\_rate = 0.001, dropout = 0.4, batch\_size = 32, epochs = 20 (test accuracy: 92.29\%, tempo de treinamento: 375.2s)
\end{itemize}

\textbf{Glossário rápido (CNN):} learning\_rate controla o passo do otimizador; dropout regulariza as FC/conv; batch\_size define amostras por minibatch; conv\_channels = filtros por bloco conv; kernel\_size = tamanho do filtro; padding = \texttt{same} mantém resolução; use\_batchnorm ativa normalização por lote nas conv; fc\_units = neurônios na FC antes da saída; epochs = número de passagens completas.

\subsubsection{LSTM para AG\_NEWS}

Os hiperparâmetros otimizados via Grid Search foram:

\textbf{Estágio 1 (amostra pequena - 1.000 amostras):}
\begin{itemize}
    \item learning\_rate $\in$ \{0.001, 0.01\}
    \item embedding\_dim $\in$ \{50, 100\}
    \item hidden\_dim $\in$ \{64, 128\}
    \item dropout $\in$ \{0.3, 0.5\}
    \item bidirectional $\in$ \{False, True\}
    \item batch\_size $\in$ \{32, 64\}
    \item epochs = 3
\end{itemize}

\textbf{Estágio 2 (amostra maior - 10.000 amostras):}
\begin{itemize}
    \item learning\_rate $\in$ \{0.005, 0.01\}
    \item embedding\_dim = 100
    \item hidden\_dim $\in$ \{64, 128\}
    \item dropout $\in$ \{0.2, 0.3, 0.4\}
    \item bidirectional = True
    \item batch\_size = 32
    \item epochs = 5
\end{itemize}

\textbf{Melhores hiperparâmetros encontrados:}
\begin{itemize}
    \item \textbf{Estágio 1:} learning\_rate = 0.01, embedding\_dim = 100, hidden\_dim = 64, dropout = 0.3, bidirectional = True, batch\_size = 32, epochs = 3 (melhor CV accuracy: 54.50\% $\pm$ 3.32\%, 113 configurações testadas)
    \item \textbf{Estágio 2:} learning\_rate = 0.005, embedding\_dim = 100, hidden\_dim = 128, dropout = 0.4, bidirectional = True, batch\_size = 32, epochs = 5 (melhor CV accuracy: 83.99\% $\pm$ 0.15\%, 12 configurações testadas, tempo total: 59 minutos)
    \item \textbf{Treinamento final:} Executado no dataset completo (120.000 amostras) com learning\_rate = 0.005, embedding\_dim = 100, hidden\_dim = 128, dropout = 0.4, bidirectional = True, batch\_size = 32 e epochs = 20. Resultado no teste: \textbf{accuracy = 0.8917}, \textbf{precision macro = 0.8930}, \textbf{recall macro = 0.8917}, \textbf{F1 macro = 0.8919}; tempo de treino \textasciitilde88.5 minutos.
\end{itemize}

\textbf{Glossário rápido (LSTM):} learning\_rate controla o passo do otimizador; embedding\_dim = dimensão dos vetores de palavra; hidden\_dim = tamanho do estado oculto da LSTM; dropout = regularização entre camadas; bidirectional = usa LSTM nas duas direções; batch\_size = amostras por minibatch; epochs = épocas de treino; vocab\_size = tamanho do vocabulário; max\_seq\_length = 200 tokens (padding/truncamento).

A estratégia de dois estágios reduziu significativamente o tempo de experimentação comparado a executar Grid Search diretamente no conjunto completo de dados. Para o Fashion MNIST, o tempo estimado foi reduzido de várias horas para aproximadamente 30-40 minutos. Para o AG\_NEWS, o tempo total do Stage 2 foi de aproximadamente 59 minutos (12 configurações testadas), enquanto uma busca direta no dataset completo levaria estimadas 8-12 horas.

\subsection{Métricas de Avaliação}

Para a avaliação final, foram utilizadas métricas-padrão de classificação: acurácia, precisão macro, recall macro e F1-score macro, além de matriz de confusão e análise da incidência dos erros.

Durante a otimização de hiperparâmetros, as seguintes métricas de validação cruzada foram registradas:
\begin{itemize}
    \item \textbf{Accuracy (CV):} Média e desvio padrão da acurácia nos folds (5 para Fashion MNIST, 2-3 para AG\_NEWS)
    \item \textbf{Precision Macro (CV):} Média da precisão entre todas as classes
    \item \textbf{Recall Macro (CV):} Média do recall entre todas as classes
    \item \textbf{F1-Score Macro (CV):} Média harmônica entre precisão e recall
\end{itemize}

Na avaliação final no conjunto de teste, foram calculadas:
\begin{itemize}
    \item \textbf{Test Accuracy:} Proporção de predições corretas no conjunto de teste
    \item \textbf{Test Precision Macro:} Precisão média entre classes (útil para dados balanceados)
    \item \textbf{Test Recall Macro:} Recall médio entre classes
    \item \textbf{Test F1-Score Macro:} F1-score médio entre classes
    \item \textbf{Training Time:} Tempo total de treinamento em segundos
    \item \textbf{Tuning Time:} Tempo de busca de hiperparâmetros (quando aplicável)
\end{itemize}

\section{Experimentos e Resultados}

Os experimentos foram organizados na avaliação de modelos baseline para ambos os datasets. Todos os experimentos foram rastreados utilizando MLFlow.

Para otimização de hiperparâmetros, primeiro se faz a busca e ajuste de hiperparâmetros via Grid Search com validação cruzada, identificando os melhores parâmetros para cada modelo. Ao término dessa fase, se faz a avaliação final dos modelos retreinados com os melhores hiperparâmetros no conjunto completo de treino e testados no conjunto de teste.

\subsection{Fashion MNIST}


\subsubsection{Modelos Baseline}

Os modelos baseline foram avaliados utilizando todas as 784 features. A Tabela~\ref{tab:fashion_mnist_final} apresenta os resultados no conjunto de teste após otimização de hiperparâmetros.

\begin{table}[h]
    \centering
    \caption{Avaliação final dos modelos no Fashion MNIST}
    \label{tab:fashion_mnist_final}
    \small
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Modelo} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
        \hline
        Logistic OvR & 83.50\% & 83.74\% & 83.85\% & 83.78\% \\
        Logistic Softmax & 83.40\% & 83.85\% & 83.75\% & 83.76\% \\
        Naive Bayes Multinomial & 65.55\% & 65.42\% & 65.55\% & 62.76\% \\
        Naive Bayes Bernoulli & 64.82\% & 66.58\% & 64.82\% & 63.98\% \\
        Naive Bayes Gaussiano & 59.10\% & 62.03\% & 58.40\% & 55.74\% \\
        \hline
    \end{tabular}
\end{table}

Os modelos discriminativos (Regressão Logística) superaram os modelos generativos Naive Bayes. Os modelos Naive Bayes Multinomial (65.55\%) e Bernoulli (64.82\%) praticamente empataram, seguidos pelo Gaussiano (59.10\%). Mesmo o melhor Naive Bayes ficou 17.95 pontos percentuais abaixo da melhor Regressão Logística (83.50\%, Logistic OvR). Esse resultado confirma que, para classificação de imagens com alta correlação entre features, modelos discriminativos são mais adequados. A superioridade do Multinomial sobre o Gaussiano demonstra que a escolha da distribuição é crítica mesmo quando o modelo generativo é inadequado para o problema.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../../confusion_matrices_baseline/fashion_mnist_logistic_softmax.png}
    \caption{Matriz de confusão do modelo Logistic Softmax no Fashion MNIST (baseline).}
    \label{fig:fashion_confusion_logistic_softmax}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../../confusion_matrices_baseline/fashion_mnist_naive_bayes.png}
    \caption{Matriz de confusão do modelo Naive Bayes (Multinomial) no Fashion MNIST (baseline).}
    \label{fig:fashion_confusion_naive_bayes}
\end{figure}

\subsubsection{Aprendizado Profundo}

Para o Fashion MNIST, foi treinada uma Rede Neural Convolucional (CNN) utilizando PyTorch. A arquitetura implementada (run final) usa duas camadas convolucionais com batch normalization e padding \texttt{same} (32 e 64 filtros, kernel 3), seguida de max pooling e duas camadas totalmente conectadas (FC de 128 unidades). Hiperparâmetros finais: learning\_rate = 0.001, dropout = 0.4, batch\_size = 32, epochs = 20. O modelo alcançou \textbf{Accuracy = 0.9229}, \textbf{Precision macro = 0.9230}, \textbf{Recall macro = 0.9229} e \textbf{F1 macro = 0.9228}, superando todos os baselines (melhor baseline: Regressão Logística OvR com 83.50\%).

Para a CNN, geramos a matriz de confusão no conjunto de teste e salvamos em \texttt{results/plots/confusion\_matrix\_cnn\_final\_new.png}. O gráfico de convergência (loss/accuracy por época) está em \texttt{results/plots/training\_curves\_cnn\_final\_new.png}. A função de perda usada no treino é \textbf{CrossEntropyLoss} (log-loss) otimizada com Adam.

\begin{figure}[H]
    \centering
\includegraphics[width=0.8\textwidth]{../../results/plots/confusion_matrix_cnn_final_new.png}
    \caption{Matriz de confusão da CNN final no Fashion MNIST (teste).}
    \label{fig:fashion_confusion_cnn_final}
\end{figure}

\begin{figure}[H]
    \centering
\includegraphics[width=0.8\textwidth]{../../results/plots/training_curves_cnn_final_new.png}
    \caption{Curvas de loss e accuracy de treino da CNN final (20 épocas).}
    \label{fig:fashion_training_curves_cnn_final}
\end{figure}

Este resultado demonstra a superioridade do aprendizado profundo para classificação de imagens, onde a CNN consegue extrair automaticamente features hierárquicas relevantes dos dados brutos, enquanto os modelos baseline operam diretamente sobre os valores de pixels normalizados.

\subsection{AG\_NEWS}

\subsubsection{Modelos Baseline e Resultados de Validação Cruzada}

Para o dataset AG\_NEWS, foram avaliados sete modelos após otimização de hiperparâmetros utilizando Grid Search CV com validação cruzada estratificada. A Tabela~\ref{tab:agnews_cv_results} apresenta os resultados da validação cruzada, enquanto a Tabela~\ref{tab:agnews_final} apresenta os resultados finais no conjunto de teste.

\begin{table}[h]
    \centering
    \caption{Resultados de Validação Cruzada no AG\_NEWS (Phase 1: Hyperparameter Tuning)}
    \label{tab:agnews_cv_results}
    \small
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Modelo} & \textbf{CV Accuracy} & \textbf{CV F1-Score Macro} \\
        \hline
        Logistic Softmax & 89.90\% $\pm$ 0.10\% & 89.86\% \\
        Logistic OvR & 89.76\% $\pm$ 0.15\% & 89.71\% \\
        Naive Bayes Bernoulli & 89.17\% $\pm$ 0.00\% & 89.12\% \\
        Naive Bayes Multinomial & 87.92\% $\pm$ 0.52\% & 87.91\% \\
        Random Forest & 87.70\% $\pm$ 0.69\% & 87.64\% \\
        Naive Bayes Gaussiano & 80.31\% & 80.18\% \\
        GMM & 80.97\% & 80.73\% \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Avaliação final dos modelos no AG\_NEWS (Phase 2: Test Set Evaluation)}
    \label{tab:agnews_final}
    \small
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Modelo} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
        \hline
        Logistic OvR & 91.34\% & 91.32\% & 91.34\% & 91.32\% \\
        Logistic Softmax & 91.24\% & 91.22\% & 91.24\% & 91.22\% \\
        Random Forest & 90.96\% & 90.94\% & 90.96\% & 90.92\% \\
        Naive Bayes Multinomial & 89.66\% & 89.61\% & 89.66\% & 89.62\% \\
        Naive Bayes Bernoulli & 89.36\% & 89.31\% & 89.36\% & 89.31\% \\
        GMM & 86.89\% & 86.91\% & 86.89\% & 86.90\% \\
        Naive Bayes Gaussiano & 86.64\% & 86.68\% & 86.64\% & 86.64\% \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../../confusion_matrices_baseline/agnews_naive_bayes_multinomial.png}
    \caption{Matriz de confusão do Naive Bayes Multinomial no AG\_NEWS (baseline).}
    \label{fig:agnews_confusion_nb_multinomial}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../../confusion_matrices_baseline/agnews_naive_bayes_bernoulli.png}
    \caption{Matriz de confusão do Naive Bayes Bernoulli no AG\_NEWS (baseline).}
    \label{fig:agnews_confusion_nb_bernoulli}
\end{figure}

\textbf{Observações principais:}
Pela tabela, os modelos discriminativos obtiveram os melhores valores de acurácia. Porém, dessa vez, os modelos discriminativos lineares foram ligeiramente superiores, praticamente empatados, com a Random Forest, que é um modelo discriminativo não-linear. No caso da Fashion MNIST, a Random Forest foi 3.5\% superior à regressão logística. Um possível motivo para essa maior competitividade da regressão logística é a maior separabilidade linear no espaço do TF-IDF em razão da maior esparsidade.

Outro ponto importante foi a também maior competitividade dos modelos generativos Naive Bayes, que foram bem melhores do que no caso do Fashion MNIST. A GMM também deve desempenho razoável. Essa maior capacidade de generalização pode ser tanto pela maior quantidade de dados, quanto pela maior independência entre os parâmetros de entrada se comparados à vetores que representam imagens com pixels altamente correlacionados.



Para dados textuais (AG\_NEWS), os modelos discriminativos também superaram os generativos, porém com margens menores que no Fashion MNIST. A diferença entre o melhor discriminativo (Logistic OvR: 91.34\%) e o melhor generativo (Naive Bayes Multinomial: 89.66\%) é de apenas 1.68 pontos percentuais, comparado a 17.95 pontos percentuais no Fashion MNIST. Isso confirma que features TF-IDF, com menor correlação entre si e distribuição esparsa, são mais adequadas para modelagem generativa do que pixels de imagens com alta correlação espacial.

\subsubsection{Aprendizado Profundo}

Para o dataset AG\_NEWS, foi implementada uma arquitetura LSTM (Long Short-Term Memory) bidirecional utilizando PyTorch. A arquitetura consiste em camada de embedding, LSTM bidirecional, dropout e camada totalmente conectada para classificação (detalhada na Seção de Metodologia).

A otimização de hiperparâmetros seguiu a mesma estratégia de dois estágios utilizada para a CNN. No Estágio 1 (Tiny), foram testadas 113 configurações diferentes em um subconjunto de 1.000 amostras, identificando que LSTM bidirecional com embedding\_dim = 100 e learning\_rate = 0.01 eram promissores, alcançando 54.50\% de CV accuracy. No Estágio 2 (Small), 12 configurações foram refinadas em 10.000 amostras, identificando os melhores hiperparâmetros: learning\_rate = 0.005, embedding\_dim = 100, hidden\_dim = 128, dropout = 0.4, bidirectional = True, batch\_size = 32, com CV accuracy de 83.99\% $\pm$ 0.15\%.

\textbf{Treinamento final (dataset completo).} 
Utilizando os hiperparâmetros selecionados, o modelo foi treinado em 120.000 amostras por 20 épocas (batch\_size = 32). No conjunto de teste, obteve-se acurácia de 0.8917, precisão macro de 0.8930, recall macro de 0.8917 e F1 macro de 0.8919, com tempo total de treino de 88.5 minutos. A matriz de confusão (Figura~\ref{fig:agnews_confusion_lstm_final}) mostra que os erros se concentram entre \textit{Business} e \textit{Sci/Tech}. As curvas de aprendizado (Figura~\ref{fig:agnews_training_curves_lstm_final}) indicam convergência estável sem overfitting visível.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{../../results/plots/confusion_matrix_lstm_final.png}
    \caption{Matriz de confusão da LSTM final no AG\_NEWS (teste).}
    \label{fig:agnews_confusion_lstm_final}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{../../results/plots/training_curves_lstm_final.png}
    \caption{Curvas de loss e accuracy de treino da LSTM final (20 épocas).}
    \label{fig:agnews_training_curves_lstm_final}
\end{figure}

\subsubsection{Overflow de Memória na Validação Cruzada do AG NEWS}

Durante os experimentos com Random Forest no AG\_NEWS, foi identificado um problema de consumo de memória, nominalmente, segundo o sistema operacional, atingiu 96 GB de RAM, paralizando a máquina. Esse problema  foi causado por um paralelismo aninhado na validação cruzada.

O dataset AG\_NEWS passa por múltiplas transformações que expandem sua representação em memória:


O dataset original consiste em 96.000 amostras de texto, totalizando aproximadamente 18 megabytes. Após a vetorização utilizando o método TF-IDF, obtém-se uma matriz esparsa com tamanho de 110 megabytes. No entanto, como o algoritmo Random Forest não aceita matrizes esparsas com representação compacta, ocorre uma conversão automática para um array denso, resultando em uma matriz de 96.000 linhas por 10.000 colunas, cada elemento ocupando 8 bytes, o que corresponde a aproximadamente 7,15 gigabytes.

O problema ocorre quando dois níveis de paralelização são ativados simultaneamente:

\begin{verbatim}
GridSearchCV(n_jobs=-1)           # 16 workers paralelos
  -> RandomForest(n_jobs=-1)     # 16 workers por GridSearchCV worker
      = 16 x 16 = 256 processos paralelos
\end{verbatim}

Cada processo necessita de uma cópia do dataset de 7.15 GB, demandando 96 GB. 

A solução implementada consistiu em desabilitar o paralelismo do Random Forest durante o Grid Search, mantendo apenas o paralelismo do GridSearchCV. Com esta modificação, o consumo de memória foi reduzido de 96 GB para 12-20 GB (redução de 4-8×), tornando a execução mais viável, mas ainda assim muito pesada.

\subsubsection{Estratégia de Otimização em Duas Fases}

Para viabilizar a execução de Grid Search CV em tempo razoável e sem estourar a memória disponível, foi desenvolvida uma estratégia de otimização em duas fases. A primeira fase identifica quais hiperparâmetros são mais persistentes através de um teste rápido com subconjunto pequeno dos dados, de 5.000 amostras, 2 folds e 54 configurações da grid search, sem gerar overflow de memória.

Resultados da Fase 1 para Random Forest:

\begin{table}[h]
    \centering
    \caption{Top 5 configurações - Fase 1 (Quick Test)}
    \label{tab:rf_phase1}
    \small
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Rank} & \textbf{Accuracy} & \textbf{max\_features} & \textbf{max\_depth} & \textbf{n\_estimators} \\
        \hline
        1 & 87.70\% (±0.69\%) & log2 & None & 300 \\
        2 & 87.46\% (±0.42\%) & log2 & None & 200 \\
        3 & 87.36\% (±0.67\%) & log2 & None & 200 \\
        4 & 87.36\% (±0.58\%) & log2 & None & 300 \\
        5 & 86.70\% (±0.82\%) & log2 & None & 100 \\
        \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item \texttt{max\_features='log2'} aparece em todas as top 5 configurações
    \item \texttt{max\_depth=None} 
    \item \texttt{n\_estimators} entre 200-300 
    \item Configurações com \texttt{max\_features='sqrt'} ou \texttt{0.1} não aparecem no top 5
    \item Configurações com \texttt{max\_depth=20} ou \texttt{30} não aparecem no top 5
\end{itemize}

A partir desse primeiro levantamento, os parâmetros de max\_features e max\_depth já foram selecionados.


%2 folds ou 3 folds?
Já na segunda fase, aumenta-se o subconjunto de busca para 30.000 amostras para uma grid de 54 com 2 folds.


% altere a tabela abaixo. deixe igual está no slide. tem uma tabela mt parecida com essa.
\begin{table}[h]
    \centering
    \caption{Comparação de configurações de Grid Search}
    \label{tab:grid_search_tradeoffs}
    \small
    \begin{tabular}{|l|c|c|c|l|}
        \hline
        \textbf{Configuração} & \textbf{Samples} & \textbf{CV Folds} & \textbf{Tempo} & \textbf{Uso} \\
        \hline
         & 5k & 2 & 10 min & Exploração \\
        Balanceado & 30k & 2 & 40 min & Desenvolvimento \\
        Completo & 96k & 3 & 8-12h & Publicação \\
        \hline
    \end{tabular}
\end{table}

Esta estratégia reduziu o tempo de experimentação de 8 a 12 horas para aproximadamente 50 minutos.

\section{Avaliação dos Resultados}

\subsection{Comparação entre Modelos Generativos e Discriminativos}

Os resultados experimentais confirmaram as expectativas sobre o desempenho relativo de modelos generativos e discriminativos para classificação multiclasse:

\textbf{Fashion MNIST (Imagens):}
\begin{itemize}
    \item Modelos discriminativos (Logistic Regression) superaram consistentemente modelos generativos (Naive Bayes)
    \item Logistic Regression alcançou 83.50\% de acurácia (OvR), enquanto o melhor Naive Bayes (Multinomial) obteve apenas 65.55\%
    \item Entre os Naive Bayes, MultinomialNB e BernoulliNB apresentaram desempenho praticamente equivalente (65.55\% vs 64.82\%, diferença de apenas 0.73 pp), ambos superando o GaussianNB (59.10\%) em aproximadamente 6 pp, demonstrando que a escolha da distribuição é crítica
    \item A diferença de 17.95 pontos percentuais demonstra que os modelos generativos não são tão adequados para o Fashion MNIST, talvez pelo tamanho do dataset e pela alta correlação espacial dificultando-o de generalizar para a quantidade de dados disponível.
\end{itemize}

% sinto que tem bastante redundancia nos resultados tambem...
\textbf{AG\_NEWS (Texto):}
\begin{itemize}
    \item Modelos discriminativos foram melhores com vantagem sutil: Logistic OvR (91.34\%) vs. Naive Bayes Multinomial (89.66\%)
    \item A diferença de 1.68 pontos percentuais é significativamente menor que no Fashion MNIST.    \item Naive Bayes Multinomial (89.66\%) e Bernoulli (89.36\%) superaram Gaussiano (86.64\%).
    \item Random Forest superou os modelos Naive Bayes, demonstrando que modelos ensemble não-lineares capturam padrões complexos também em dados textuais
    \item GMM com 1 componente por classe e covariância diagonal obteve 86.89\%
    \item Em geral, todos os modelos generalizaram bem
  \end{itemize}

\subsection{Discussão}


A modalidade dos dados afeta o desempenho relativo entre modelos generativos e discriminativos. A diferença de desempenho é maior em imagens (17.95 pp comparando Logistic OvR vs. melhor NB) do que em texto (1.68 pp), indicando que a natureza das features (pixels correlacionados vs. TF-IDF esparso) influencia a adequação de cada abordagem.

No AG\_NEWS, a Regressão Logística (OvR: 91.34\%; Softmax: 91.24\%) superou a Random Forest (90.96\%), sugerindo que features TF-IDF permitem separação linear adequada.

A escolha da distribuição no Naive Bayes afeta os resultados. No Fashion MNIST, MultinomialNB (65.55\%) e BernoulliNB (64.82\%) apresentaram desempenho equivalente (diferença de 0.73 pp), ambos superando GaussianNB (59.10\%) em aproximadamente 6 pp. No AG\_NEWS, o padrão se repetiu: MultinomialNB (89.66\%) e BernoulliNB (89.36\%) ficaram praticamente empatados (diferença de 0.30 pp), ambos superando GaussianNB (86.64\%) em aproximadamente 3 pp. As distribuições Multinomial e Bernoulli, adequadas para dados discretos e esparsos, produzem resultados equivalentes, enquanto a distribuição Gaussiana apresenta desempenho inferior.

Para o GMM, a covariância diagonal foi necessária devido à alta dimensionalidade (784 features no Fashion MNIST, 10.000 no AG\_NEWS). A covariância completa não convergiu. O GMM com covariância diagonal obteve desempenho inferior aos modelos discriminativos.

A estratégia de utilizar subconjuntos menores para otimização de hiperparâmetros mostrou-se viável. Usar 30k amostras (31\%) com 2-fold CV reduziu o tempo de Grid Search de 8-12 horas para 40 minutos, mantendo qualidade de resultados para desenvolvimento iterativo.

\section{Conclusões}

Este trabalho atingiu seu objetivo principal de comparar modelos generativos e discriminativos em classificação multiclasse através de experimentos sistemáticos em dois datasets de modalidades distintas. 


Os experimentos demonstraram que modelos discriminativos superaram modelos generativos em ambos os datasets. Também demonstraram que a modalidade dos dados importa, sobretudo para o caso dos modelos generativos. A escolha da distribuição, no caso do Naive Bayes, apresenta diferença sutil de resultados, mas o suficiente para para fazer uma escolha adequada e não assumir que a distribuição gaussiana seja a mais adequada.

No AG\_NEWS, a regressão logística enquanto separador linear obteve o melhor resultado, indicando que as features desse dataset talvez não dependam tanto de modelos não lineares.

A questão do consumo de memória faz-se necessária dominar ao aplicar validação cruzada para grandes datasets, necessitando de certa criatividade e aceitando resultados com experimentos mais simples que possam generalizar bem para o conjunto todo.

\section{Bibliografia}

\begin{enumerate}
    \item Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.

    \item Theodoridis, S. (2015). \textit{Machine Learning: A Bayesian and Optimization Perspective}. Academic Press.

    \item Murphy, K. P. (2012). \textit{Machine Learning: A Probabilistic Perspective}. MIT Press.

    \item Xiao, H., Rasul, K., \& Vollgraf, R. (2017). \textit{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}. arXiv:1708.07747.

    \item Ng, A., \& Jordan, M. (2002). On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes. In \textit{Advances in neural information processing systems} (Vol. 14).

    \item Bouzidi, S., Hcini, G., Jdey, I., \& Drira, F. (2024). \textit{Convolutional neural networks and vision transformers for fashion mnist classification: A literature review}. arXiv:2406.03478.

    \item Zheng, Wu, Bao, Cao, Li, \& Zhu (2023). \textit{Revisiting Discriminative vs. Generative Classifiers: Theory and Implications}. arXiv:2302.02334.

    \item Ozdemir, S. (2024). \textit{News Classification with State-of-the-Art Deep Learning Methods}. In 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP) (pp. 1-5). IEEE.
\end{enumerate}

\end{document}
