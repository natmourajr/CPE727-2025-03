"""
Script para download e prepara√ß√£o do dataset Shenzhen Hospital X-ray Set
"""
import os
import requests
import zipfile
from pathlib import Path
from tqdm import tqdm
import shutil

# URLs do dataset
SHENZHEN_URL = "https://lhncbc.nlm.nih.gov/LHC-downloads/downloads.html#tuberculosis-image-data-sets"

# Fontes de download (em ordem de prioridade)
# Fontes de download (em ordem de prioridade)
DATASET_SOURCES = [
    {
        'name': 'Kaggle (Prioridade)',
        'type': 'kaggle',
        'id': 'raddar/tuberculosis-chest-xrays-shenzhen'
    },
    {
        'name': 'NIH OpenI (Original)',
        'url': 'https://openi.nlm.nih.gov/imgs/collections/ChinaSet_AllFiles.zip',
        'type': 'direct'
    }
]

def download_file(url, destination, resume=True):
    """
    Download de arquivo com barra de progresso e suporte a resumo
    
    Args:
        url: URL do arquivo
        destination: Path de destino
        resume: Se True, tenta continuar download parcial
    """
    destination = Path(destination)
    temp_file = destination.with_suffix(destination.suffix + '.part')
    
    # Verificar se h√° download parcial
    downloaded_size = 0
    if resume and temp_file.exists():
        downloaded_size = temp_file.stat().st_size
        print(f"üì¶ Download parcial encontrado: {downloaded_size / (1024*1024):.1f} MB")
        print("üîÑ Retomando download...")
    
    headers = {}
    if downloaded_size > 0:
        headers['Range'] = f'bytes={downloaded_size}-'
    
    try:
        response = requests.get(url, stream=True, timeout=30, headers=headers)
        
        # Verificar se servidor suporta range requests
        if downloaded_size > 0 and response.status_code not in [206, 200]:
            print("‚ö†Ô∏è  Servidor n√£o suporta resumo, baixando do in√≠cio...")
            downloaded_size = 0
            headers = {}
            response = requests.get(url, stream=True, timeout=30, headers=headers)
        
        response.raise_for_status()
        
        # Tamanho total do arquivo
        if 'content-length' in response.headers:
            total_size = int(response.headers.get('content-length'))
            if response.status_code == 206:  # Partial content
                total_size = total_size + downloaded_size
        else:
            total_size = downloaded_size
        
        # Modo de abertura do arquivo
        mode = 'ab' if downloaded_size > 0 else 'wb'
        
        with open(temp_file, mode) as file, tqdm(
            desc=destination.name,
            total=total_size,
            initial=downloaded_size,
            unit='iB',
            unit_scale=True,
            unit_divisor=1024,
        ) as progress_bar:
            for data in response.iter_content(chunk_size=8192):
                size = file.write(data)
                progress_bar.update(size)
        
        # Mover arquivo completo para destino final
        temp_file.rename(destination)
        
        print("‚úÖ Download conclu√≠do com sucesso!")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro no download: {str(e)}")
        if temp_file.exists():
            print(f"üíæ Download parcial salvo em: {temp_file}")
            print("üîÑ Execute novamente para retomar o download")
        return False
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è  Download interrompido pelo usu√°rio")
        if temp_file.exists():
            print(f"üíæ Download parcial salvo em: {temp_file}")
            print("üîÑ Execute novamente para retomar o download")
        return False
    except Exception as e:
        print(f"‚ùå Erro inesperado: {str(e)}")
        return False

def download_shenzhen_dataset(output_dir='./data'):
    """
    Download do dataset Shenzhen Hospital X-ray Set
    """
    print("=" * 70)
    print("DOWNLOAD DO DATASET SHENZHEN HOSPITAL X-RAY SET")
    print("=" * 70)
    
    # Criar diret√≥rio de sa√≠da
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Arquivo zip tempor√°rio
    zip_path = output_path / "shenzhen_dataset.zip"
    zip_part = output_path / "shenzhen_dataset.zip.part"
    
    # Verificar se j√° existe download completo
    if zip_path.exists():
        print(f"\n‚úÖ Arquivo j√° existe: {zip_path}")
        print("üì¶ Pulando download e indo direto para extra√ß√£o...")
        success = True
    else:
        print("\nüì• Tentando baixar dataset...")
        
        if zip_part.exists():
            part_size = zip_part.stat().st_size
            print(f"\nüîÑ Download parcial encontrado: {part_size / (1024*1024):.1f} MB")
            print("Tentando retomar download...\n")
        
        # Tentar cada fonte em ordem de prioridade
        success = False
        for idx, source in enumerate(DATASET_SOURCES, 1):
            print(f"\n{'='*70}")
            print(f"üì° Tentativa {idx}/{len(DATASET_SOURCES)}: {source['name']}")
            print(f"Name: {source['name']}")
            if "url" in source:
                print(f"URL: {source['url']}")
            print(f"{'='*70}\n")
            try:
                if source['type'] == 'kaggle':
                    # Tentar importar kaggle
                    try:
                        import kaggle
                    except ImportError:
                        print(f"‚ö†Ô∏è  Biblioteca 'kaggle' n√£o encontrada. Pulando fonte Kaggle.")
                        continue

                    print(f"üîë Autenticando e baixando do Kaggle ({source['id']})...")
                    print("   (Necessita arquivo kaggle.json configurado ou vari√°veis de ambiente)")
                    
                    # Kaggle baixa um zip com nome diferente as vezes
                    # Vamos baixar para o diret√≥rio
                    kaggle.api.dataset_download_files(source['id'], path=output_path, unzip=False, quiet=False)
                    
                    # Encontrar o zip baixado
                    # O nome geralmente √© o slug do dataset.zip
                    possible_zips = list(output_path.glob("*.zip"))
                    # Se antes n√£o tinha zip (verificado no inicio), o novo √© o nosso
                    # Mas como startamos um loop, melhor procurar pelo mais recente ou pelo nome esperado.
                    # O dataset raddar/tuberculosis... baixa como tuberculosis-chest-xrays-shenzhen.zip
                    
                    downloaded_zip = None
                    for zip_f in possible_zips:
                        if zip_f.name != "shenzhen_dataset.zip": # Ignorar se for o nosso target (que nao existia)
                             downloaded_zip = zip_f
                             break
                    
                    if downloaded_zip and downloaded_zip.exists():
                        print(f"‚úÖ Download Kaggle conclu√≠do: {downloaded_zip.name}")
                        # Renomear para o padr√£o esperado
                        if zip_path.exists(): zip_path.unlink()
                        downloaded_zip.rename(zip_path)
                        success = True
                    else:
                        print("‚ö†Ô∏è  Download Kaggle parece ter falhado (arquivo n√£o encontrado)")
                        success = False

                else:
                    success = download_file(source['url'], zip_path, resume=True)
                
                if success and zip_path.exists():
                    print(f"\n‚úÖ Download bem-sucedido de: {source['name']}")
                    break
                else:
                    print(f"\n‚ö†Ô∏è  Falha ao baixar de: {source['name']}")
                    if idx < len(DATASET_SOURCES):
                        print("üîÑ Tentando pr√≥xima fonte...")
            except Exception as e:
                print(f"\n‚ùå Erro ao baixar de {source['name']}: {str(e)}")
                if idx < len(DATASET_SOURCES):
                    print("üîÑ Tentando pr√≥xima fonte...")
    
    if not success or not zip_path.exists():
        print("\n" + "=" * 70)
        print("‚ùå TODAS AS FONTES DE DOWNLOAD FALHARAM")
        print("=" * 70)
        print("\nüìã INSTRU√á√ïES PARA DOWNLOAD MANUAL:\n")
        print("1. Acesse o site oficial:")
        print("   üëâ https://lhncbc.nlm.nih.gov/LHC-downloads/downloads.html#tuberculosis-image-data-sets\n")
        print("2. Localize 'Shenzhen Hospital X-ray Set' e clique em 'Download'")
        print("3. O arquivo ser√°: ChinaSet_AllFiles.zip (aproximadamente 440 MB)\n")
        print("4. Ap√≥s baixar, coloque o arquivo .zip em:")
        print(f"   üëâ {zip_path.absolute()}\n")
        print("5. Execute novamente este script para extrair e organizar:\n")
        print("   docker-compose run --rm tuberculosis-detection python src/download_data.py\n")
        print("=" * 70)
        return False
    
    # Se chegou aqui, o download foi bem-sucedido
    try:
        print("\n‚úÖ Download conclu√≠do!")
        
        # Extrair arquivos
        print("\nüì¶ Extraindo arquivos...")
        extract_path = output_path / "shenzhen_raw"
        extract_path.mkdir(exist_ok=True)
        
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            members = zip_ref.namelist()
            for member in tqdm(members, desc="Extraindo"):
                zip_ref.extract(member, extract_path)
        
        print("‚úÖ Extra√ß√£o conclu√≠da!")
        
        # Organizar dataset
        print("\nüìÇ Organizando dataset...")
        organize_dataset(extract_path, output_path / "shenzhen")
        
        # Limpar arquivos tempor√°rios
        print("\nüßπ Limpando arquivos tempor√°rios...")
        zip_path.unlink()
        shutil.rmtree(extract_path)
        
        print("\n‚ú® Dataset pronto para uso!")
        print(f"üìç Localiza√ß√£o: {output_path / 'shenzhen'}")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Erro durante a extra√ß√£o/organiza√ß√£o: {str(e)}")
        print("\nPor favor, verifique manualmente o arquivo baixado.")
        return False

def organize_dataset(source_dir, target_dir):
    """
    Organiza o dataset extra√≠do em pastas normal/tuberculosis
    """
    target_path = Path(target_dir)
    source_path = Path(source_dir)
    
    # Criar estrutura de diret√≥rios
    normal_dir = target_path / "normal"
    tb_dir = target_path / "tuberculosis"
    normal_dir.mkdir(parents=True, exist_ok=True)
    tb_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üìÅ Procurando arquivos em: {source_path}")
    
    # O dataset Shenzhen geralmente vem com esta estrutura:
    # ChinaSet_AllFiles/
    #   ‚îú‚îÄ‚îÄ CXR_png/  (imagens)
    #   ‚îî‚îÄ‚îÄ ClinicalReadings/ (metadados)
    
    # Procurar diret√≥rio de imagens
    image_dirs = list(source_path.rglob("**/CXR_png")) or \
                 list(source_path.rglob("**/images")) or \
                 [source_path]
    
    if image_dirs:
        image_dir = image_dirs[0]
        print(f"üì∏ Diret√≥rio de imagens encontrado: {image_dir}")
    else:
        image_dir = source_path
    
    # Procurar arquivo de metadados
    metadata_files = list(source_path.rglob("**/*ClinicalReadings*.txt")) or \
                     list(source_path.rglob("**/metadata*.txt")) or \
                     list(source_path.rglob("*.txt"))
    
    if metadata_files:
        print(f"üìã Arquivo de metadados encontrado: {metadata_files[0].name}")
        organize_from_metadata(image_dir, target_path, metadata_files[0])
    else:
        print("‚ö†Ô∏è  Arquivo de metadados n√£o encontrado.")
        print("üìã Organizando baseado na nomenclatura dos arquivos...")
        organize_by_filename(image_dir, target_path)
    
    # Contar imagens organizadas
    normal_count = len(list(normal_dir.glob("*.png")))
    tb_count = len(list(tb_dir.glob("*.png")))
    
    print(f"‚úÖ Organizados: {normal_count} normais, {tb_count} com tuberculose")
    print(f"üìä Total: {normal_count + tb_count} imagens")

def organize_from_metadata(image_dir, target_dir, metadata_file):
    """
    Organiza dataset usando arquivo de metadados
    """
    normal_dir = target_dir / "normal"
    tb_dir = target_dir / "tuberculosis"
    
    # Ler arquivo de metadados
    try:
        with open(metadata_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            
        # O formato t√≠pico inclui informa√ß√µes sobre cada imagem
        # Procurar padr√µes como "normal", "abnormal", "tuberculosis"
        for img_file in image_dir.glob("*.png"):
            filename = img_file.name
            filename_lower = filename.lower()
            
            # CORRE√á√ÉO: Usar padr√£o do nome do arquivo como m√©todo prim√°rio
            # CHNCXR_xxxx_0.png = normal, CHNCXR_xxxx_1.png = TB
            if filename_lower.endswith('_0.png') or 'normal' in filename_lower:
                shutil.copy2(img_file, normal_dir / filename)
            elif filename_lower.endswith('_1.png'):
                shutil.copy2(img_file, tb_dir / filename)
            else:
                # Apenas se n√£o houver padr√£o claro, usar metadados
                if filename in content:
                    # Extrair a linha relevante
                    for line in content.split('\n'):
                        if filename in line:
                            line_lower = line.lower()
                            if 'normal' in line_lower and 'abnormal' not in line_lower:
                                shutil.copy2(img_file, normal_dir / filename)
                            else:
                                # Assume tuberculose se n√£o for normal
                                shutil.copy2(img_file, tb_dir / filename)
                            break
                else:
                    # Se n√£o encontrado nos metadados e sem padr√£o, assume TB
                    shutil.copy2(img_file, tb_dir / filename)
                    
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao processar metadados: {str(e)}")
        print("üìã Usando organiza√ß√£o por nome de arquivo...")
        organize_by_filename(image_dir, target_dir)

def organize_by_filename(image_dir, target_dir):
    """
    Organiza dataset baseado no nome dos arquivos
    """
    normal_dir = target_dir / "normal"
    tb_dir = target_dir / "tuberculosis"
    
    # Procurar todas as imagens PNG
    for img_file in image_dir.rglob("*.png"):
        filename = img_file.name.lower()
        
        # Baseado na nomenclatura t√≠pica do dataset Shenzhen
        # Imagens normais geralmente cont√™m "normal" no nome
        # ou t√™m IDs espec√≠ficos (CHNCXR_xxxx_0.png = normal, CHNCXR_xxxx_1.png = TB)
        if 'normal' in filename or filename.endswith('_0.png'):
            shutil.copy2(img_file, normal_dir / img_file.name)
        else:
            shutil.copy2(img_file, tb_dir / img_file.name)

def verify_dataset(data_dir='./data/shenzhen'):
    """
    Verifica a integridade do dataset baixado
    """
    data_path = Path(data_dir)
    
    print("\n" + "=" * 70)
    print("üîç VERIFICA√á√ÉO DO DATASET")
    print("=" * 70)
    
    normal_dir = data_path / "normal"
    tb_dir = data_path / "tuberculosis"
    
    if not data_path.exists():
        print(f"‚ùå Diret√≥rio n√£o encontrado: {data_path}")
        print("\nüí° Execute o download primeiro:")
        print("   docker-compose run --rm tuberculosis-detection python src/download_data.py")
        return False
    
    if not normal_dir.exists() or not tb_dir.exists():
        print("‚ùå Estrutura de diret√≥rios incompleta!")
        print(f"\nüìÅ Estrutura esperada:")
        print(f"   {data_path}/")
        print(f"   ‚îú‚îÄ‚îÄ normal/")
        print(f"   ‚îî‚îÄ‚îÄ tuberculosis/")
        return False
    
    normal_images = list(normal_dir.glob("*.png"))
    tb_images = list(tb_dir.glob("*.png"))
    
    print(f"\nüìä Estat√≠sticas do Dataset:")
    print(f"   ‚úÖ Imagens normais: {len(normal_images)}")
    print(f"   ‚úÖ Imagens com TB: {len(tb_images)}")
    print(f"   üìä Total: {len(normal_images) + len(tb_images)} imagens")
    
    # Verificar n√∫meros esperados (aproximados)
    expected_normal = 326
    expected_tb = 240
    total = len(normal_images) + len(tb_images)
    
    print(f"\nüìà Compara√ß√£o com valores esperados:")
    print(f"   Normal: {len(normal_images)}/{expected_normal} ({len(normal_images)/expected_normal*100:.1f}%)")
    print(f"   TB: {len(tb_images)}/{expected_tb} ({len(tb_images)/expected_tb*100:.1f}%)")
    print(f"   Total: {total}/566 ({total/566*100:.1f}%)")
    
    if len(normal_images) == 0 or len(tb_images) == 0:
        print("\n‚ö†Ô∏è  Dataset incompleto! Pelo menos uma categoria est√° vazia.")
        return False
    
    if total < 500:
        print("\n‚ö†Ô∏è  Aviso: N√∫mero de imagens abaixo do esperado (566).")
        print("   Verifique se o download foi completo.")
    
    print("\n‚úÖ Dataset verificado com sucesso!")
    print("=" * 70)
    return True

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Download e prepara√ß√£o do dataset Shenzhen",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:

  # Download e organiza√ß√£o autom√°tica
  python src/download_data.py

  # Especificar diret√≥rio de sa√≠da
  python src/download_data.py --output-dir /caminho/personalizado

  # Apenas verificar dataset existente
  python src/download_data.py --verify-only

  # Organizar dataset baixado manualmente
  python src/download_data.py --organize-only --source /caminho/do/zip/extraido
        """
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='./data',
        help='Diret√≥rio de sa√≠da (padr√£o: ./data)'
    )
    parser.add_argument(
        '--verify-only',
        action='store_true',
        help='Apenas verificar dataset existente'
    )
    parser.add_argument(
        '--organize-only',
        action='store_true',
        help='Apenas organizar dataset j√° baixado'
    )
    parser.add_argument(
        '--source',
        type=str,
        help='Diret√≥rio fonte para organiza√ß√£o (usar com --organize-only)'
    )
    parser.add_argument(
        '--clean',
        action='store_true',
        help='Limpar downloads parciais e recome√ßar do zero'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='For√ßar re-download mesmo se arquivo j√° existir'
    )
    
    args = parser.parse_args()
    
    # Limpar downloads parciais se solicitado
    if args.clean:
        output_path = Path(args.output_dir)
        zip_path = output_path / "shenzhen_dataset.zip"
        zip_part = output_path / "shenzhen_dataset.zip.part"
        
        print("üßπ Limpando downloads parciais...")
        
        if zip_part.exists():
            zip_part.unlink()
            print(f"‚úÖ Removido: {zip_part}")
        
        if args.force and zip_path.exists():
            zip_path.unlink()
            print(f"‚úÖ Removido: {zip_path}")
        
        if not zip_part.exists() and not (args.force and zip_path.exists()):
            print("‚ÑπÔ∏è  Nenhum arquivo para limpar")
        
        print("‚ú® Limpeza conclu√≠da!")
        
        # Se apenas limpar, sair
        if not args.verify_only and not args.organize_only:
            print("\nüí° Execute novamente sem --clean para baixar o dataset")
            exit(0)
    
    if args.verify_only:
        verify_dataset(f"{args.output_dir}/shenzhen")
    elif args.organize_only:
        if not args.source:
            print("‚ùå Erro: --source √© obrigat√≥rio quando usar --organize-only")
            exit(1)
        print(f"üìÇ Organizando dataset de: {args.source}")
        organize_dataset(args.source, f"{args.output_dir}/shenzhen")
        verify_dataset(f"{args.output_dir}/shenzhen")
    else:
        success = download_shenzhen_dataset(args.output_dir)
        if success:
            verify_dataset(f"{args.output_dir}/shenzhen")
