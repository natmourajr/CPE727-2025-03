"""
Script para download e preparaÃ§Ã£o do dataset Shenzhen Hospital X-ray Set
"""
import os
import requests
import zipfile
from pathlib import Path
from tqdm import tqdm
import shutil

# URLs do dataset
SHENZHEN_URL = "https://lhncbc.nlm.nih.gov/LHC-downloads/downloads.html#tuberculosis-image-data-sets"
# Nota: O download direto pode nÃ£o funcionar devido a restriÃ§Ãµes do site NIH
# Mantenha as instruÃ§Ãµes manuais como alternativa
DATASET_ZIP_URL = "https://openi.nlm.nih.gov/imgs/collections/ChinaSet_AllFiles.zip"

def download_file(url, destination, resume=True):
    """
    Download de arquivo com barra de progresso e suporte a resumo
    
    Args:
        url: URL do arquivo
        destination: Path de destino
        resume: Se True, tenta continuar download parcial
    """
    destination = Path(destination)
    temp_file = destination.with_suffix(destination.suffix + '.part')
    
    # Verificar se hÃ¡ download parcial
    downloaded_size = 0
    if resume and temp_file.exists():
        downloaded_size = temp_file.stat().st_size
        print(f"ğŸ“¦ Download parcial encontrado: {downloaded_size / (1024*1024):.1f} MB")
        print("ğŸ”„ Retomando download...")
    
    headers = {}
    if downloaded_size > 0:
        headers['Range'] = f'bytes={downloaded_size}-'
    
    try:
        response = requests.get(url, stream=True, timeout=30, headers=headers)
        
        # Verificar se servidor suporta range requests
        if downloaded_size > 0 and response.status_code not in [206, 200]:
            print("âš ï¸  Servidor nÃ£o suporta resumo, baixando do inÃ­cio...")
            downloaded_size = 0
            headers = {}
            response = requests.get(url, stream=True, timeout=30, headers=headers)
        
        response.raise_for_status()
        
        # Tamanho total do arquivo
        if 'content-length' in response.headers:
            total_size = int(response.headers.get('content-length'))
            if response.status_code == 206:  # Partial content
                total_size = total_size + downloaded_size
        else:
            total_size = downloaded_size
        
        # Modo de abertura do arquivo
        mode = 'ab' if downloaded_size > 0 else 'wb'
        
        with open(temp_file, mode) as file, tqdm(
            desc=destination.name,
            total=total_size,
            initial=downloaded_size,
            unit='iB',
            unit_scale=True,
            unit_divisor=1024,
        ) as progress_bar:
            for data in response.iter_content(chunk_size=8192):
                size = file.write(data)
                progress_bar.update(size)
        
        # Mover arquivo completo para destino final
        temp_file.rename(destination)
        
        print("âœ… Download concluÃ­do com sucesso!")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Erro no download: {str(e)}")
        if temp_file.exists():
            print(f"ğŸ’¾ Download parcial salvo em: {temp_file}")
            print("ğŸ”„ Execute novamente para retomar o download")
        return False
    except KeyboardInterrupt:
        print(f"\nâš ï¸  Download interrompido pelo usuÃ¡rio")
        if temp_file.exists():
            print(f"ğŸ’¾ Download parcial salvo em: {temp_file}")
            print("ğŸ”„ Execute novamente para retomar o download")
        return False
    except Exception as e:
        print(f"âŒ Erro inesperado: {str(e)}")
        return False

def download_shenzhen_dataset(output_dir='./data'):
    """
    Download do dataset Shenzhen Hospital X-ray Set
    """
    print("=" * 70)
    print("DOWNLOAD DO DATASET SHENZHEN HOSPITAL X-RAY SET")
    print("=" * 70)
    
    # Criar diretÃ³rio de saÃ­da
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Arquivo zip temporÃ¡rio
    zip_path = output_path / "shenzhen_dataset.zip"
    zip_part = output_path / "shenzhen_dataset.zip.part"
    
    # Verificar se jÃ¡ existe download completo
    if zip_path.exists():
        print(f"\nâœ… Arquivo jÃ¡ existe: {zip_path}")
        print("ğŸ“¦ Pulando download e indo direto para extraÃ§Ã£o...")
        success = True
    else:
        print("\nğŸ“¥ Tentando baixar dataset automaticamente...")
        print(f"URL: {DATASET_ZIP_URL}")
        print(f"Destino: {zip_path}")
        
        if zip_part.exists():
            part_size = zip_part.stat().st_size
            print(f"\nğŸ”„ Download parcial encontrado: {part_size / (1024*1024):.1f} MB")
            print("Tentando retomar download...\n")
        else:
            print("\nâš ï¸  Nota: O download automÃ¡tico pode falhar devido a restriÃ§Ãµes do site NIH.")
            print("Se falhar, siga as instruÃ§Ãµes de download manual abaixo.\n")
        
        # Tentar download automÃ¡tico com suporte a resumo
        success = download_file(DATASET_ZIP_URL, zip_path, resume=True)
    
    if not success or not zip_path.exists():
        print("\n" + "=" * 70)
        print("âŒ DOWNLOAD AUTOMÃTICO FALHOU")
        print("=" * 70)
        print("\nğŸ“‹ INSTRUÃ‡Ã•ES PARA DOWNLOAD MANUAL:\n")
        print("1. Acesse o site oficial:")
        print("   ğŸ‘‰ https://lhncbc.nlm.nih.gov/LHC-downloads/downloads.html#tuberculosis-image-data-sets\n")
        print("2. Localize 'Shenzhen Hospital X-ray Set' e clique em 'Download'")
        print("3. O arquivo serÃ¡: ChinaSet_AllFiles.zip (aproximadamente 440 MB)\n")
        print("4. ApÃ³s baixar, coloque o arquivo .zip em:")
        print(f"   ğŸ‘‰ {zip_path.absolute()}\n")
        print("5. Execute novamente este script para extrair e organizar:\n")
        print("   docker-compose run --rm tuberculosis-detection python src/download_data.py\n")
        print("=" * 70)
        return False
    
    # Se chegou aqui, o download foi bem-sucedido
    try:
        print("\nâœ… Download concluÃ­do!")
        
        # Extrair arquivos
        print("\nğŸ“¦ Extraindo arquivos...")
        extract_path = output_path / "shenzhen_raw"
        extract_path.mkdir(exist_ok=True)
        
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            members = zip_ref.namelist()
            for member in tqdm(members, desc="Extraindo"):
                zip_ref.extract(member, extract_path)
        
        print("âœ… ExtraÃ§Ã£o concluÃ­da!")
        
        # Organizar dataset
        print("\nğŸ“‚ Organizando dataset...")
        organize_dataset(extract_path, output_path / "shenzhen")
        
        # Limpar arquivos temporÃ¡rios
        print("\nğŸ§¹ Limpando arquivos temporÃ¡rios...")
        zip_path.unlink()
        shutil.rmtree(extract_path)
        
        print("\nâœ¨ Dataset pronto para uso!")
        print(f"ğŸ“ LocalizaÃ§Ã£o: {output_path / 'shenzhen'}")
        return True
        
    except Exception as e:
        print(f"\nâŒ Erro durante a extraÃ§Ã£o/organizaÃ§Ã£o: {str(e)}")
        print("\nPor favor, verifique manualmente o arquivo baixado.")
        return False

def organize_dataset(source_dir, target_dir):
    """
    Organiza o dataset extraÃ­do em pastas normal/tuberculosis
    """
    target_path = Path(target_dir)
    source_path = Path(source_dir)
    
    # Criar estrutura de diretÃ³rios
    normal_dir = target_path / "normal"
    tb_dir = target_path / "tuberculosis"
    normal_dir.mkdir(parents=True, exist_ok=True)
    tb_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ Procurando arquivos em: {source_path}")
    
    # O dataset Shenzhen geralmente vem com esta estrutura:
    # ChinaSet_AllFiles/
    #   â”œâ”€â”€ CXR_png/  (imagens)
    #   â””â”€â”€ ClinicalReadings/ (metadados)
    
    # Procurar diretÃ³rio de imagens
    image_dirs = list(source_path.rglob("**/CXR_png")) or \
                 list(source_path.rglob("**/images")) or \
                 [source_path]
    
    if image_dirs:
        image_dir = image_dirs[0]
        print(f"ğŸ“¸ DiretÃ³rio de imagens encontrado: {image_dir}")
    else:
        image_dir = source_path
    
    # Procurar arquivo de metadados
    metadata_files = list(source_path.rglob("**/*ClinicalReadings*.txt")) or \
                     list(source_path.rglob("**/metadata*.txt")) or \
                     list(source_path.rglob("*.txt"))
    
    if metadata_files:
        print(f"ğŸ“‹ Arquivo de metadados encontrado: {metadata_files[0].name}")
        organize_from_metadata(image_dir, target_path, metadata_files[0])
    else:
        print("âš ï¸  Arquivo de metadados nÃ£o encontrado.")
        print("ğŸ“‹ Organizando baseado na nomenclatura dos arquivos...")
        organize_by_filename(image_dir, target_path)
    
    # Contar imagens organizadas
    normal_count = len(list(normal_dir.glob("*.png")))
    tb_count = len(list(tb_dir.glob("*.png")))
    
    print(f"âœ… Organizados: {normal_count} normais, {tb_count} com tuberculose")
    print(f"ğŸ“Š Total: {normal_count + tb_count} imagens")

def organize_from_metadata(image_dir, target_dir, metadata_file):
    """
    Organiza dataset usando arquivo de metadados
    """
    normal_dir = target_dir / "normal"
    tb_dir = target_dir / "tuberculosis"
    
    # Ler arquivo de metadados
    try:
        with open(metadata_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            
        # O formato tÃ­pico inclui informaÃ§Ãµes sobre cada imagem
        # Procurar padrÃµes como "normal", "abnormal", "tuberculosis"
        for img_file in image_dir.glob("*.png"):
            filename = img_file.name
            
            # Verificar se o arquivo estÃ¡ mencionado nos metadados
            if filename in content:
                # Extrair a linha relevante
                for line in content.split('\n'):
                    if filename in line:
                        line_lower = line.lower()
                        if 'normal' in line_lower and 'abnormal' not in line_lower:
                            shutil.copy2(img_file, normal_dir / filename)
                        else:
                            # Assume tuberculose se nÃ£o for normal
                            shutil.copy2(img_file, tb_dir / filename)
                        break
            else:
                # Se nÃ£o encontrado nos metadados, usar nome do arquivo
                if 'normal' in filename.lower():
                    shutil.copy2(img_file, normal_dir / filename)
                else:
                    shutil.copy2(img_file, tb_dir / filename)
                    
    except Exception as e:
        print(f"âš ï¸  Erro ao processar metadados: {str(e)}")
        print("ğŸ“‹ Usando organizaÃ§Ã£o por nome de arquivo...")
        organize_by_filename(image_dir, target_dir)

def organize_by_filename(image_dir, target_dir):
    """
    Organiza dataset baseado no nome dos arquivos
    """
    normal_dir = target_dir / "normal"
    tb_dir = target_dir / "tuberculosis"
    
    # Procurar todas as imagens PNG
    for img_file in image_dir.rglob("*.png"):
        filename = img_file.name.lower()
        
        # Baseado na nomenclatura tÃ­pica do dataset Shenzhen
        # Imagens normais geralmente contÃªm "normal" no nome
        # ou tÃªm IDs especÃ­ficos (CHNCXR_xxxx_0.png = normal, CHNCXR_xxxx_1.png = TB)
        if 'normal' in filename or filename.endswith('_0.png'):
            shutil.copy2(img_file, normal_dir / img_file.name)
        else:
            shutil.copy2(img_file, tb_dir / img_file.name)

def verify_dataset(data_dir='./data/shenzhen'):
    """
    Verifica a integridade do dataset baixado
    """
    data_path = Path(data_dir)
    
    print("\n" + "=" * 70)
    print("ğŸ” VERIFICAÃ‡ÃƒO DO DATASET")
    print("=" * 70)
    
    normal_dir = data_path / "normal"
    tb_dir = data_path / "tuberculosis"
    
    if not data_path.exists():
        print(f"âŒ DiretÃ³rio nÃ£o encontrado: {data_path}")
        print("\nğŸ’¡ Execute o download primeiro:")
        print("   docker-compose run --rm tuberculosis-detection python src/download_data.py")
        return False
    
    if not normal_dir.exists() or not tb_dir.exists():
        print("âŒ Estrutura de diretÃ³rios incompleta!")
        print(f"\nğŸ“ Estrutura esperada:")
        print(f"   {data_path}/")
        print(f"   â”œâ”€â”€ normal/")
        print(f"   â””â”€â”€ tuberculosis/")
        return False
    
    normal_images = list(normal_dir.glob("*.png"))
    tb_images = list(tb_dir.glob("*.png"))
    
    print(f"\nğŸ“Š EstatÃ­sticas do Dataset:")
    print(f"   âœ… Imagens normais: {len(normal_images)}")
    print(f"   âœ… Imagens com TB: {len(tb_images)}")
    print(f"   ğŸ“Š Total: {len(normal_images) + len(tb_images)} imagens")
    
    # Verificar nÃºmeros esperados (aproximados)
    expected_normal = 326
    expected_tb = 240
    total = len(normal_images) + len(tb_images)
    
    print(f"\nğŸ“ˆ ComparaÃ§Ã£o com valores esperados:")
    print(f"   Normal: {len(normal_images)}/{expected_normal} ({len(normal_images)/expected_normal*100:.1f}%)")
    print(f"   TB: {len(tb_images)}/{expected_tb} ({len(tb_images)/expected_tb*100:.1f}%)")
    print(f"   Total: {total}/566 ({total/566*100:.1f}%)")
    
    if len(normal_images) == 0 or len(tb_images) == 0:
        print("\nâš ï¸  Dataset incompleto! Pelo menos uma categoria estÃ¡ vazia.")
        return False
    
    if total < 500:
        print("\nâš ï¸  Aviso: NÃºmero de imagens abaixo do esperado (566).")
        print("   Verifique se o download foi completo.")
    
    print("\nâœ… Dataset verificado com sucesso!")
    print("=" * 70)
    return True

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Download e preparaÃ§Ã£o do dataset Shenzhen",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:

  # Download e organizaÃ§Ã£o automÃ¡tica
  python src/download_data.py

  # Especificar diretÃ³rio de saÃ­da
  python src/download_data.py --output-dir /caminho/personalizado

  # Apenas verificar dataset existente
  python src/download_data.py --verify-only

  # Organizar dataset baixado manualmente
  python src/download_data.py --organize-only --source /caminho/do/zip/extraido
        """
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='./data',
        help='DiretÃ³rio de saÃ­da (padrÃ£o: ./data)'
    )
    parser.add_argument(
        '--verify-only',
        action='store_true',
        help='Apenas verificar dataset existente'
    )
    parser.add_argument(
        '--organize-only',
        action='store_true',
        help='Apenas organizar dataset jÃ¡ baixado'
    )
    parser.add_argument(
        '--source',
        type=str,
        help='DiretÃ³rio fonte para organizaÃ§Ã£o (usar com --organize-only)'
    )
    parser.add_argument(
        '--clean',
        action='store_true',
        help='Limpar downloads parciais e recomeÃ§ar do zero'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='ForÃ§ar re-download mesmo se arquivo jÃ¡ existir'
    )
    
    args = parser.parse_args()
    
    # Limpar downloads parciais se solicitado
    if args.clean:
        output_path = Path(args.output_dir)
        zip_path = output_path / "shenzhen_dataset.zip"
        zip_part = output_path / "shenzhen_dataset.zip.part"
        
        print("ğŸ§¹ Limpando downloads parciais...")
        
        if zip_part.exists():
            zip_part.unlink()
            print(f"âœ… Removido: {zip_part}")
        
        if args.force and zip_path.exists():
            zip_path.unlink()
            print(f"âœ… Removido: {zip_path}")
        
        if not zip_part.exists() and not (args.force and zip_path.exists()):
            print("â„¹ï¸  Nenhum arquivo para limpar")
        
        print("âœ¨ Limpeza concluÃ­da!")
        
        # Se apenas limpar, sair
        if not args.verify_only and not args.organize_only:
            print("\nğŸ’¡ Execute novamente sem --clean para baixar o dataset")
            exit(0)
    
    if args.verify_only:
        verify_dataset(f"{args.output_dir}/shenzhen")
    elif args.organize_only:
        if not args.source:
            print("âŒ Erro: --source Ã© obrigatÃ³rio quando usar --organize-only")
            exit(1)
        print(f"ğŸ“‚ Organizando dataset de: {args.source}")
        organize_dataset(args.source, f"{args.output_dir}/shenzhen")
        verify_dataset(f"{args.output_dir}/shenzhen")
    else:
        success = download_shenzhen_dataset(args.output_dir)
        if success:
            verify_dataset(f"{args.output_dir}/shenzhen")
