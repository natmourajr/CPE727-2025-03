{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d818ec1",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ae799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767276e6",
   "metadata": {},
   "source": [
    "### Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e89eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para replicabilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class GANConfig:\n",
    "    'Stores all the hyperparameters of the experiment'\n",
    "    data_dim: int = 2       # Saída do Gerador e a entrada do Discriminador\n",
    "    noise_dim: int = 1      # Entrada do Gerador\n",
    "    hidden_dim: int = 128   # Número de neurônios nas camadas ocultas\n",
    "    batch_size: int = 64    # Número de amostras processadas por iteração de treinamento\n",
    "    num_epochs: int = 5000  # Número de vezes que o conjunto passa por todo o modelo\n",
    "    \n",
    "CASE_STABLE = {\n",
    "    'name': 'stable',\n",
    "    'lr_D': 0.00005,\n",
    "    'lr_G': 0.0001\n",
    "}\n",
    "\n",
    "CASE_UNSTABLE = {\n",
    "    'name': 'unstable',\n",
    "    'lr_D': 0.00009,\n",
    "    'lr_G': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f47da",
   "metadata": {},
   "source": [
    "### Distribuições de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51453ee5",
   "metadata": {},
   "source": [
    "`real_data_sampler`: Define a distribuição de dados alvo.\n",
    "\n",
    "$$y = \\sin(x) + \\mathcal{N}(0, 0.1)$$\n",
    "\n",
    "onde $x$ é amostrado uniformemente em $[-3, 3]$. O GAN tentará replicar essa curva senoidal com ruído.\n",
    "\n",
    "`noise_sampler`: É a entrada do Gerador, amostrada de uma distribuição normal padrão (ruído gaussiano).\n",
    "\n",
    "$$\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "O Gerador aprende a mapear esse vetor de ruído (`noise_dim`) para a distribuição de dados de saída (`data_dim=2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cce27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_sampler(num_samples: int) -> torch.Tensor:\n",
    "    x = np.random.uniform(-3, 3, size=(num_samples, 1))     # x ~ Uniform(-3, 3)\n",
    "    noise = np.random.normal(0, 0.1, size=(num_samples, 1)) # Ruído ~ Normal(0, 0.1)\n",
    "    y = np.sin(x) + noise                                   # y = sin(x) + noise\n",
    "    data = np.hstack((x, y))\n",
    "    return torch.tensor(data, dtype=torch.float32)          # Retorna o tensor [x, y]\n",
    " \n",
    "def noise_sampler(num_samples: int) -> torch.Tensor:\n",
    "    # Retorna num_samples vetores de ruído gaussiano (dimensão noise_dim=1)\n",
    "    return torch.randn(num_samples, GANConfig.noise_dim)             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd7f6f",
   "metadata": {},
   "source": [
    "### Arquitetura da rede\n",
    "\n",
    "A rede do Gerador transforma ruído em dados falsos, com a função de ativação ReLU nas camadas ocultas.\n",
    "\n",
    "Já a rede do Classificador classifica a entrada como real (próximo de 1) ou falsa (próximo de 0), usando LeakyReLU nas camadas ocultas e Sigmoid na saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc95945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bab07f",
   "metadata": {},
   "source": [
    "### Classe de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5330c76",
   "metadata": {},
   "source": [
    "A classe `GANTrainer` encapsula todo o ciclo de vida do treinamento de uma GAN.\n",
    "\n",
    "- Inicialização (`__init__`):\n",
    "    - Cria instâncias do **Gerador ($\\text{G}$)** e do **Discriminador ($\\text{D}$)**, que são as duas redes neurais da GAN.\n",
    "    - Define a função de perda (**`nn.BCELoss()`**, ou **Binary Cross-Entropy Loss**), ideal para tarefas de classificação binária (real vs. fake).\n",
    "    - Configura os otimizadores (**`optim.Adam`**) separadamente para $\\text{G}$ e $\\text{D}$, cada um com sua própria taxa de aprendizado ($\\text{lr}_{\\text{D}}$, $\\text{lr}_{\\text{G}}$).\n",
    "    - Prepara o diretório para salvar os **checkpoints** (pesos) do modelo.\n",
    "\n",
    "O método `train` executa o loop de treinamento por um número fixo de épocas, alternando entre a otimização do Discriminador e a do Gerador em cada passo.\n",
    "\n",
    "#### D\n",
    "\n",
    "O objetivo do Discriminador é se tornar bom em distinguir dados reais de dados falsos.\n",
    "\n",
    "- Amostragem de Dados:\n",
    "    - Obtém um lote de dados reais (`real_data`) com rótulos $\\mathbf{1}$ (`real_labels`).\n",
    "    - Gera dados falsos (`fake_data`) a partir do $\\text{G}$ usando ruído aleatório ($\\mathbf{z}$), e os rotula como $\\mathbf{0}$ (`fake_labels`).\n",
    "    - Importante: $\\text{G}$ é desconectado do gráfico computacional (`.detach()`) para que os gradientes calculados no $\\text{D}$ não sejam propagados de volta para o $\\text{G}$ neste passo.\n",
    "- Cálculo e Otimização:\n",
    "    - Os dados reais e falsos são concatenados (`all_data`).\n",
    "    - A $\\text{D}$ avalia esses dados (`D_output`).\n",
    "    - Calcula-se a perda ($\\text{loss}_{\\text{D}}$) comparando as previsões de $\\text{D}$ com os rótulos verdadeiros (1s e 0s).\n",
    "    - O D é atualizado para minimizar essa perda, ou seja, para melhorar sua capacidade de classificar corretamente os dados.\n",
    "\n",
    "#### G\n",
    "\n",
    "O objetivo do Gerador é produzir dados que sejam convincentes o suficiente para enganar o Discriminador.\n",
    "\n",
    "- Geração de Dados:\n",
    "    - $\\text{G}$ gera um novo lote de dados falsos (`fake_data`) a partir de ruído ($\\mathbf{z}$).\n",
    "    - Define-se o rótulo-alvo como $\\mathbf{1}$ (`target_labels`), indicando que o $\\text{G}$ está sendo treinado para que o $\\text{D}$ classifique sua saída como **real**.\n",
    "- Cálculo e Otimização:\n",
    "    - $\\text{D}$ avalia os dados gerados (`G_output`).\n",
    "    - Calcula-se a perda ($\\text{loss}_{\\text{G}}$) comparando as previsões de $\\text{D}$ com o rótulo-alvo **$\\mathbf{1}$**.\n",
    "    - O **Gerador** é atualizado para **minimizar** essa perda, ou seja, para fazer com que o $\\text{D}$ atribua uma probabilidade próxima de $\\mathbf{1}$ aos dados que ele gera.\n",
    "\n",
    "Eu adicionei um processo de checkpoints e uma parte de monitoramento. Então a norma do gradiente do $\\text{G}$ (`G_grad_norms`) é calculada e armazenada para monitorar a estabilidade do treinamento. A cada 500 épocas, o código imprime as perdas atuais do $\\text{D}$ e do $\\text{G}$. Em épocas predefinidas (`self.checkpoints`), os estados (pesos) das redes $\\text{G}$ e $\\text{D}$ são salvos (checkpoint) no disco. O método retorna as listas de perdas ($\\text{D}$ e $\\text{G}$), as normas dos gradientes de $\\text{G}$, as épocas de checkpoint e o diretório de salvamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANTrainer:\n",
    "    'Encapsulates model initialization, the training loop, and checkpoint saving'\n",
    "    def __init__(self, lr_D: float, lr_G: float, case_name: str):\n",
    "        self.config = GANConfig()\n",
    "        self.case_name = case_name\n",
    "        self.checkpoint_dir = os.path.join('..', 'results', '_1_gan', f'gan_checkpoints_{case_name}')\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        self.G = Generator(self.config.noise_dim, self.config.data_dim, self.config.hidden_dim)\n",
    "        self.D = Discriminator(self.config.data_dim, self.config.hidden_dim)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer_D = optim.Adam(self.D.parameters(), lr=lr_D)\n",
    "        self.optimizer_G = optim.Adam(self.G.parameters(), lr=lr_G)\n",
    "\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.checkpoints: List[int] = [\n",
    "            int(self.config.num_epochs * 0.25),\n",
    "            int(self.config.num_epochs * 0.50),\n",
    "            int(self.config.num_epochs * 0.75),\n",
    "            self.config.num_epochs\n",
    "        ]\n",
    "\n",
    "    def train(self) -> Tuple[List[float], List[float], List[float], List[int], str]:\n",
    "        D_losses: List[float] = []\n",
    "        G_losses: List[float] = []\n",
    "        G_grad_norms: List[float] = []\n",
    "        \n",
    "        print(f\"\\nStarting Training: {self.case_name.upper()}\")\n",
    "\n",
    "        for epoch in range(1, self.config.num_epochs + 1):\n",
    "            # D\n",
    "            real_data = real_data_sampler(self.config.batch_size)\n",
    "            real_labels = torch.ones(self.config.batch_size, 1)\n",
    "            z = noise_sampler(self.config.batch_size)\n",
    "            fake_data = self.G(z).detach()\n",
    "            fake_labels = torch.zeros(self.config.batch_size, 1)\n",
    "            all_data = torch.cat((real_data, fake_data))\n",
    "            all_labels = torch.cat((real_labels, fake_labels))\n",
    "            D_output = self.D(all_data)\n",
    "            loss_D = self.criterion(D_output, all_labels)\n",
    "            \n",
    "            self.D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            self.optimizer_D.step()\n",
    "            D_losses.append(loss_D.item())\n",
    "\n",
    "            # G\n",
    "            z = noise_sampler(self.config.batch_size)\n",
    "            fake_data = self.G(z)\n",
    "            target_labels = torch.ones(self.config.batch_size, 1)\n",
    "            G_output = self.D(fake_data)\n",
    "            loss_G = self.criterion(G_output, target_labels)\n",
    "            \n",
    "            self.G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            \n",
    "            # Cálculo e Armazenamento da Norma do Gradiente\n",
    "            grad_norm = 0.0\n",
    "            for p in self.G.parameters():\n",
    "                if p.grad is not None:\n",
    "                    grad_norm += (p.grad.norm(2).item() ** 2)\n",
    "            G_grad_norms.append(grad_norm ** 0.5)\n",
    "\n",
    "            self.optimizer_G.step()\n",
    "            G_losses.append(loss_G.item())\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch}/{self.config.num_epochs} | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f}\")\n",
    "            \n",
    "            if epoch in self.checkpoints:\n",
    "                torch.save(self.G.state_dict(), os.path.join(self.checkpoint_dir, f'generator_epoch_{epoch}.pth'))\n",
    "                torch.save(self.D.state_dict(), os.path.join(self.checkpoint_dir, f'discriminator_epoch_{epoch}.pth'))\n",
    "                print(f\"Checkpoints saved for epoch {epoch}.\")\n",
    "\n",
    "        return D_losses, G_losses, G_grad_norms, self.checkpoints, self.checkpoint_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77cf1a",
   "metadata": {},
   "source": [
    "### Classe de Visualização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89a70a",
   "metadata": {},
   "source": [
    "Separei alguns gráficos que acho interessante serem usados para avaliar. O objetivo aqui é monitorar o comportamento interno da GAN durante o processo de otimização.\n",
    "\n",
    "##### `plot_losses()`: Perdas do Discriminador e do Gerador\n",
    "\n",
    "A finalidade é visualizar o equilíbrio e a convergência do processo adversarial.\n",
    "Então iremos imprimir a $\\text{Loss}_{\\text{D}}$ e a $\\text{Loss}_{\\text{G}}$ ao longo das iterações (epochs).\n",
    "\n",
    "Um cenário ideal mostra ambas as perdas estabilizando em um valor de equilíbrio.\n",
    "    - Se a $\\text{Loss}_{\\text{D}}$ cair para zero, o Discriminador é muito bom e o G não consegue mais aprender (falha de G).\n",
    "    - Se a $\\text{Loss}_{\\text{G}}$ cair para zero, o G está enganando facilmente o Discriminador, mas isso não garante a qualidade dos dados (pode indicar mode collapse).\n",
    "    - Oscilações extremas indicam instabilidade no treinamento.\n",
    "\n",
    "##### `plot_gradient_norm()`: Norma do Gradiente do Gerador\n",
    "\n",
    "A finalidade aqui já é monitorar o fluxo de gradiente no G para detectar problemas como vanishing gradient ou exploding gradient.\n",
    "Nesse caso nós imprimimos a $\\text{Norma } \\mathbf{L2}$ do gradiente do G ao longo das iterações.\n",
    "Uma norma de gradiente que se aproxima de zero indica que o G está aprendendo muito pouco ou nada, sugerindo o problema de vanishing gradient.\n",
    "Normas de gradiente muito altas indicam instabilidade ou a necessidade de weight clipping.\n",
    "\n",
    "##### `plot_data_evolution()`: Evolução dos Pontos de Dados Gerados\n",
    "\n",
    "O foco é visualizar diretamente a evolução da capacidade de geração ao longo do treinamento (usando checkpoints). É especialmente útil para dados 2D.\n",
    "Então temos um gráfico de dispersão (scatter plot) em 2D que compara os dados reais com os dados falsos em diferentes estágios do treinamento.\n",
    "No início, os pontos falsos estarão dispersos ou muito distantes dos reais. À medida que o treinamento avança, os pontos falsos devem se sobrepor e imitar a forma e a distribuição dos pontos reais, indicando convergência.\n",
    "\n",
    "##### `plot_diversity_histogram()`: Histograma de Diversidade (Eixo X)\n",
    "\n",
    "Aqui eu busquei avaliar a diversidade e a cobertura de modo do G. O código usa o eixo X como exemplo.\n",
    "Compara os histogramas de densidade da coordenada X dos dados Reais e dos dados Falsos em diferentes checkpoints.\n",
    "\n",
    "Se o histograma dos dados Falsos (vermelho) cobre e se assemelha ao histograma dos dados Reais (azul) em todos os picos e vales da distribuição, o Gerador está capturando bem a diversidade. Se o histograma Falso tiver picos apenas em algumas regiões e ignorar outras (os \"modos\"), isso indica (mode collapse), um problema comum onde a GAN falha em gerar toda a variedade de dados.\n",
    "\n",
    "##### `plot_boundary_evolution()`: Evolução da Fronteira de Decisão\n",
    "\n",
    "O objetivo principal foi mostrar como o D aprende a separar e o Gerador aprende a enganar ao longo do tempo.\n",
    "A imagem ilustra fronteira de decisão do D (a curva onde a probabilidade de ser real é de $\\mathbf{0.5}$) e as regiões classificadas como Real (vermelho/quente) ou Falso (azul/frio). Os dados reais e falsos gerados também são plotados.\n",
    "\n",
    "No início, a fronteira de decisão (linha preta) pode ser aleatória ou simples. À medida que o treinamento avança, o Discriminador tenta traçar uma fronteira complexa para separar os pontos azuis (reais) dos vermelhos (falsos). No equilíbrio, a fronteira deve se tornar ambígua ou muito complexa, com os dados reais e falsos se **misturando** na região de $\\mathbf{0.5}$ (a $\\text{D}$ não consegue mais distinguir perfeitamente).\n",
    "\n",
    "##### `plot_kde_evolution()`: Evolução da Densidade (KDE)\n",
    "\n",
    "Aqui a ideia era fornecer uma visão suavizada da distribuição de probabilidade dos dados gerados, confirmando a semelhança com a distribuição real.\n",
    "Então tentei imprimir uma Estimativa de Densidade de Kernel (KDE) para os dados Reais (azul) e Falsos (vermelho) em diferentes checkpoints.\n",
    "\n",
    "Semelhante ao histograma e ao `plot_data_evolution`, se as áreas de densidade Falsa (vermelho) se sobrepuserem e replicarem fielmente as áreas de densidade Real (azul), a GAN está funcionando bem e o Gerador está capturando a distribuição subjacente dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANVisualizer:\n",
    "    'It encapsulates all the logic for plotting and visualizing the results'\n",
    "    def __init__(self, \n",
    "                 D_losses: List[float], \n",
    "                 G_losses: List[float], \n",
    "                 G_grad_norms: List[float], \n",
    "                 checkpoints: List[int], \n",
    "                 checkpoint_dir: str, \n",
    "                 case_name: str):\n",
    "        \n",
    "        self.config = GANConfig()\n",
    "        self.D_losses = D_losses\n",
    "        self.G_losses = G_losses\n",
    "        self.G_grad_norms = G_grad_norms\n",
    "        self.checkpoints = checkpoints\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.case_name = case_name\n",
    "        self.filename_suffix = case_name.lower()\n",
    "        self.num_epochs = self.config.num_epochs\n",
    "        \n",
    "        self.image_output_dir = os.path.join('..', 'results', '_1_gan', 'images')\n",
    "        os.makedirs(self.image_output_dir, exist_ok=True)\n",
    "\n",
    "    def _load_model(self, epoch: int) -> Tuple[Generator, Discriminator]:\n",
    "        'Auxiliary function for loading the G and D of a checkpoint'\n",
    "        G_checkpoint = Generator(self.config.noise_dim, self.config.data_dim, self.config.hidden_dim)\n",
    "        D_checkpoint = Discriminator(self.config.data_dim, self.config.hidden_dim)\n",
    "        \n",
    "        G_path = os.path.join(self.checkpoint_dir, f'generator_epoch_{epoch}.pth')\n",
    "        D_path = os.path.join(self.checkpoint_dir, f'discriminator_epoch_{epoch}.pth') \n",
    "        \n",
    "        if not os.path.exists(G_path) or not os.path.exists(D_path):\n",
    "            raise FileNotFoundError(f\"Checkpoints not found for epoch {epoch}.\")\n",
    "\n",
    "        G_checkpoint.load_state_dict(torch.load(G_path))\n",
    "        D_checkpoint.load_state_dict(torch.load(D_path))\n",
    "        G_checkpoint.eval()\n",
    "        D_checkpoint.eval()\n",
    "        return G_checkpoint, D_checkpoint\n",
    "\n",
    "    def plot_losses(self) -> str:\n",
    "        plt.figure(figsize=(10, 5)) \n",
    "        plt.plot(self.D_losses, label='Discriminator Loss', color='blue')\n",
    "        plt.plot(self.G_losses, label='Generator Loss', color='red')\n",
    "        plt.title(f'GAN Training Losses ({self.case_name.upper()})') \n",
    "        plt.xlabel('Iteration') \n",
    "        plt.ylabel('Loss') \n",
    "        plt.legend() \n",
    "        plt.grid(True) \n",
    "        filename = f'gan_losses_evolution_{self.filename_suffix}.png'\n",
    "        full_path = os.path.join(self.image_output_dir, filename)\n",
    "        plt.savefig(full_path) \n",
    "        plt.close()\n",
    "        return filename\n",
    "\n",
    "    def plot_gradient_norm(self) -> str:\n",
    "        plt.figure(figsize=(10, 5)) \n",
    "        plt.plot(self.G_grad_norms, label='Generator Gradient L2 Norm', color='green')\n",
    "        plt.title(f'Generator Gradient Norm Evolution ({self.case_name.upper()} - Vanishing Gradient)') \n",
    "        plt.xlabel('Iteration') \n",
    "        plt.ylabel('Gradient L2 Norm') \n",
    "        plt.legend() \n",
    "        plt.grid(True) \n",
    "        filename = f'gan_grad_norm_evolution_{self.filename_suffix}.png'\n",
    "        full_path = os.path.join(self.image_output_dir, filename)\n",
    "        plt.savefig(full_path)  \n",
    "        plt.close()\n",
    "        return filename\n",
    "\n",
    "    def plot_data_evolution(self) -> str:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "        axes = axes.flatten()\n",
    "        num_samples = 1000\n",
    "        real_data = real_data_sampler(num_samples).numpy()\n",
    "        \n",
    "        for i, epoch in enumerate(self.checkpoints):\n",
    "            try:\n",
    "                G_checkpoint, _ = self._load_model(epoch)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            z = noise_sampler(num_samples)\n",
    "            fake_data = G_checkpoint(z).detach().numpy()\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.scatter(real_data[:, 0], real_data[:, 1], s=5, alpha=0.6, label='Real data', color='blue')\n",
    "            ax.scatter(fake_data[:, 0], fake_data[:, 1], s=5, alpha=0.6, color='red', label='Fake data')\n",
    "            \n",
    "            percent = int(epoch / self.num_epochs * 100)\n",
    "            ax.set_title(f'{self.case_name.upper()} Data Evolution - {percent}% ({epoch} Epochs)')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'gan_data_evolution_{self.filename_suffix}.png'\n",
    "        full_path = os.path.join(self.image_output_dir, filename)\n",
    "        plt.savefig(full_path) \n",
    "        plt.close()\n",
    "        return filename\n",
    "\n",
    "    def plot_diversity_histogram(self) -> str:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "        axes = axes.flatten()\n",
    "        num_samples = 5000 \n",
    "        real_data_x = real_data_sampler(num_samples).numpy()[:, 0]\n",
    "        x_bins = np.linspace(-3.5, 3.5, 50) \n",
    "        \n",
    "        for i, epoch in enumerate(self.checkpoints):\n",
    "            try:\n",
    "                G_checkpoint, _ = self._load_model(epoch)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            z = noise_sampler(num_samples)\n",
    "            fake_data_x = G_checkpoint(z).detach().numpy()[:, 0]\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.hist(real_data_x, bins=x_bins, density=True, alpha=0.5, color='blue', label='Real Data (Target Uniform)')\n",
    "            ax.hist(fake_data_x, bins=x_bins, density=True, alpha=0.7, color='red', label='Fake Data (Generated)')\n",
    "            \n",
    "            percent = int(epoch / self.num_epochs * 100)\n",
    "            ax.set_title(f'{self.case_name.upper()} X-Diversity - {percent}% ({epoch} Epochs)')\n",
    "            ax.set_xlabel('X Coordinate')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'gan_x_diversity_histogram_{self.filename_suffix}.png'\n",
    "        full_path = os.path.join(self.image_output_dir, filename)\n",
    "        plt.savefig(full_path) \n",
    "        plt.close()\n",
    "        return filename\n",
    "\n",
    "    def plot_boundary_evolution(self) -> str:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "        axes = axes.flatten()\n",
    "        num_samples = 1000\n",
    "        real_data = real_data_sampler(num_samples).numpy()\n",
    "        \n",
    "        for i, epoch in enumerate(self.checkpoints):\n",
    "            try:\n",
    "                G_checkpoint, D_checkpoint = self._load_model(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Skipping epoch {epoch}: Checkpoint files not found.\")\n",
    "                continue\n",
    "\n",
    "            self._plot_decision_boundary(G_checkpoint, D_checkpoint, axes[i], epoch, self.num_epochs, real_data, self.case_name)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'gan_boundary_evolution_{self.filename_suffix}.png'\n",
    "        full_path = os.path.join(self.image_output_dir, filename)\n",
    "        plt.savefig(full_path) \n",
    "        plt.close()\n",
    "        return filename\n",
    "    \n",
    "    def _plot_decision_boundary(self, G_checkpoint, D_checkpoint, ax, epoch, num_epochs, real_data, case_name):\n",
    "        x_min, x_max = -4, 4\n",
    "        y_min, y_max = -2, 2\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                            np.arange(y_min, y_max, 0.05))\n",
    "        \n",
    "        grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Z = D_checkpoint(grid_points).numpy().reshape(xx.shape)\n",
    "\n",
    "        ax.contourf(xx, yy, Z, levels=np.linspace(0, 1, 11), cmap=plt.cm.RdBu, alpha=0.4)\n",
    "        ax.contour(xx, yy, Z, levels=[0.5], linewidths=2, colors='k')\n",
    "\n",
    "        num_samples = 1000\n",
    "        z = noise_sampler(num_samples)\n",
    "        fake_data = G_checkpoint(z).detach().numpy()\n",
    "\n",
    "        ax.scatter(real_data[:, 0], real_data[:, 1], s=5, alpha=0.6, label='Real Data', color='blue')\n",
    "        ax.scatter(fake_data[:, 0], fake_data[:, 1], s=5, alpha=0.6, color='red', label='Fake Data (Geração)')\n",
    "\n",
    "        percent = int(epoch / num_epochs * 100)\n",
    "        ax.set_title(f'{case_name.upper()} Boundary - {percent}% ({epoch} Epochs)')\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    def plot_kde_evolution(self) -> str:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "        axes = axes.flatten()\n",
    "        num_samples = 1000\n",
    "        real_data = real_data_sampler(num_samples).numpy()\n",
    "        \n",
    "        for i, epoch in enumerate(self.checkpoints):\n",
    "            try:\n",
    "                G_checkpoint, _ = self._load_model(epoch)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            z = noise_sampler(num_samples)\n",
    "            fake_data = G_checkpoint(z).detach().numpy()\n",
    "            \n",
    "            ax = axes[i]\n",
    "            sns.kdeplot(x=real_data[:, 0], y=real_data[:, 1], ax=ax, cmap=\"Blues\", fill=True, alpha=0.5, label='Real Data Density')\n",
    "            sns.kdeplot(x=fake_data[:, 0], y=fake_data[:, 1], ax=ax, cmap=\"Reds\", fill=True, alpha=0.5, label='Fake Data Density')\n",
    "\n",
    "            percent = int(epoch / self.num_epochs * 100)\n",
    "            ax.set_title(f'{self.case_name.upper()} KDE Density - {percent}% ({epoch} Epochs)')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_xlim(-4, 4)\n",
    "            ax.set_ylim(-2, 2)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'gan_kde_evolution_{self.filename_suffix}.png'\n",
    "        full_path = os.path.join(self.image_output_dir, filename)\n",
    "        plt.savefig(full_path) \n",
    "        plt.close()\n",
    "        return filename\n",
    "\n",
    "    def generate_all_plots(self):\n",
    "        'Public method for generating all graphs'\n",
    "        self.plot_losses()\n",
    "        self.plot_gradient_norm()\n",
    "        self.plot_data_evolution()\n",
    "        self.plot_diversity_histogram()\n",
    "        self.plot_boundary_evolution()\n",
    "        self.plot_kde_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6df8b",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855b962",
   "metadata": {},
   "source": [
    "Uma pergunta que pode surgir: **Qual motivo do learning_rate sempre ser menor no D do que no G?**\n",
    "\n",
    "É uma heurística de treinamento comum para ajudar a estabilizar o processo de treinamento devido à natureza de jogo de soma zero.\n",
    "\n",
    "O D tem o objetivo de distinguir dados reais (da distribuição $y = \\sin(x) + \\text{noise}$) de dados falsos gerados. Se o D for treinado muito rapidamente, ele pode se tornar \"muito bom, muito rápido\". Isso significa que a sua saída para os dados falsos será sempre muito próxima de 0, e para os dados reais, muito próxima de 1. \n",
    "\n",
    "Nessa situação, o G receberia gradientes muito pequenos e de baixa qualidade (ou muito fortes, *vanishing/exploding gradients*) do D, tornando extremamente difícil para ele aprender a melhorar. O G pode ficar estagnado porque o sinal de erro (gradiente) do D é muito fraco ou inconsistente. A ideia é que o G deve estar sempre \"um pouco à frente\" para forçar o D a continuar a melhorar sua capacidade de detecção. \n",
    "\n",
    "Então precisamos atrasar intencionalmente o aprendizado de D (não sobrou nada para o beta...) para garantir que G tenha uma chance de acompanhar e que o treinamento não entre em colapso.\n",
    "\n",
    "Se $LR_D \\ge LR_G$, D pode ganhar o jogo rapidamente e reportar consistentemente que as amostras de G são falsas, o que leva ao problema de Vanishing Gradient, Mode Collapse ou à falha total do treinamento.\n",
    "\n",
    "A proporção $LR_D = \\frac{1}{2} LR_G$ é uma configuração **empírica**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c89071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "\n",
      "Iniciando Treinamento: STABLE\n",
      "Epoch 500/5000 | Loss D: 0.5182 | Loss G: 1.0186\n",
      "Epoch 1000/5000 | Loss D: 0.7174 | Loss G: 0.8638\n",
      "Checkpoints saved for epoch 1250.\n",
      "Epoch 1500/5000 | Loss D: 0.6925 | Loss G: 0.6715\n",
      "Epoch 2000/5000 | Loss D: 0.6871 | Loss G: 0.6907\n",
      "Epoch 2500/5000 | Loss D: 0.6946 | Loss G: 0.6887\n",
      "Checkpoints saved for epoch 2500.\n",
      "Epoch 3000/5000 | Loss D: 0.6856 | Loss G: 0.7092\n",
      "Epoch 3500/5000 | Loss D: 0.6958 | Loss G: 0.6848\n",
      "Checkpoints saved for epoch 3750.\n",
      "Epoch 4000/5000 | Loss D: 0.6916 | Loss G: 0.6873\n",
      "Epoch 4500/5000 | Loss D: 0.6937 | Loss G: 0.6698\n",
      "Epoch 5000/5000 | Loss D: 0.6879 | Loss G: 0.6883\n",
      "Checkpoints saved for epoch 5000.\n",
      "\n",
      "Iniciando Treinamento: UNSTABLE\n",
      "Epoch 500/5000 | Loss D: 0.7012 | Loss G: 0.6593\n",
      "Epoch 1000/5000 | Loss D: 0.6929 | Loss G: 0.6949\n",
      "Checkpoints saved for epoch 1250.\n",
      "Epoch 1500/5000 | Loss D: 0.6799 | Loss G: 0.7171\n",
      "Epoch 2000/5000 | Loss D: 0.6904 | Loss G: 0.7028\n",
      "Epoch 2500/5000 | Loss D: 0.6956 | Loss G: 0.7074\n",
      "Checkpoints saved for epoch 2500.\n",
      "Epoch 3000/5000 | Loss D: 0.7004 | Loss G: 0.7092\n",
      "Epoch 3500/5000 | Loss D: 0.6931 | Loss G: 0.6981\n",
      "Checkpoints saved for epoch 3750.\n",
      "Epoch 4000/5000 | Loss D: 0.6928 | Loss G: 0.6934\n",
      "Epoch 4500/5000 | Loss D: 0.6949 | Loss G: 0.6760\n",
      "Epoch 5000/5000 | Loss D: 0.6952 | Loss G: 0.6934\n",
      "Checkpoints saved for epoch 5000.\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start!\")\n",
    "\n",
    "# Caso Estável\n",
    "trainer_s = GANTrainer(\n",
    "    lr_D=CASE_STABLE['lr_D'], \n",
    "    lr_G=CASE_STABLE['lr_G'], \n",
    "    case_name=CASE_STABLE['name']\n",
    ")\n",
    "D_losses_s, G_losses_s, G_grad_norms_s, checkpoints_s, dir_s = trainer_s.train()\n",
    "\n",
    "visualizer_s = GANVisualizer(\n",
    "    D_losses_s, G_losses_s, G_grad_norms_s, checkpoints_s, dir_s, CASE_STABLE['name']\n",
    ")\n",
    "visualizer_s.generate_all_plots()\n",
    "\n",
    "# Caso Instável\n",
    "trainer_u = GANTrainer(\n",
    "    lr_D=CASE_UNSTABLE['lr_D'], \n",
    "    lr_G=CASE_UNSTABLE['lr_G'], \n",
    "    case_name=CASE_UNSTABLE['name']\n",
    ")\n",
    "D_losses_u, G_losses_u, G_grad_norms_u, checkpoints_u, dir_u = trainer_u.train()\n",
    "\n",
    "visualizer_u = GANVisualizer(\n",
    "    D_losses_u, G_losses_u, G_grad_norms_u, checkpoints_u, dir_u, CASE_UNSTABLE['name']\n",
    ")\n",
    "visualizer_u.generate_all_plots()\n",
    "\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63ad0d",
   "metadata": {},
   "source": [
    "# Análise dos resultados\n",
    "\n",
    "## Perdas\n",
    "\n",
    "Caso estável: Treinamento ideal. As perdas (D e G) convergem e se estabilizam rapidamente em torno de 0.69 (≈-ln(0.5)), indicando que D e G atingiram um equilíbrio onde D classifica as amostras falsas com probabilidade de 50%\n",
    "\n",
    "Caso instável: Treinamento volátil. As perdas oscilam drasticamente no início e continuam com variações de alta frequência ao longo de todo o treinamento, indicando que o equilíbrio é frágil e as redes estão se sobrepondo constantemente\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"../results/_1_gan/images/gan_losses_evolution_stable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 1\" style=\"width: 50%; padding-right: 5px;\" />\n",
    "    <img src=\"../results/_1_gan/images/gan_losses_evolution_unstable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 2\" style=\"width: 50%; padding-left: 5px;\" />\n",
    "</div>\n",
    "\n",
    "## Norma do Gradiente\n",
    "\n",
    "Caso estável: Após uma fase inicial turbulenta, a norma do gradiente de G diminui e se estabiliza em torno de zero, mas com pequenas oscilações. Sugere que o G está recebendo gradientes mais consistentes (estabilidade)\n",
    "\n",
    "Caso instável: Maior volatilidade. A norma do gradiente de G apresenta picos recorrentes e uma oscilação geral maior do que no cenário estável. Isso sugere que a G tem dificuldade em receber feedback consistente\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"../results/_1_gan/images/gan_grad_norm_evolution_stable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 1\" style=\"width: 50%; padding-right: 5px;\" />\n",
    "    <img src=\"../results/_1_gan/images/gan_grad_norm_evolution_unstable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 2\" style=\"width: 50%; padding-left: 5px;\" />\n",
    "</div>\n",
    "\n",
    "## Evolução dos Dados\n",
    "\n",
    "Caso estável: Excelente convergência. O Gerador (pontos vermelhos) imita com precisão a forma da distribuição real (pontos azuis) a partir de 75% do treinamento.\n",
    "\n",
    "Caso instável: Geração imperfeita. Embora a G aprenda a forma básica, há pontos falsos mais dispersos nas extremidades, e o Gerador não cobre totalmente o domínio dos dados reais, indicando uma falha menor na cobertura de modo ou qualidade\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"../results/_1_gan/images/gan_data_evolution_stable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 1\" style=\"width: 50%; padding-right: 5px;\" />\n",
    "    <img src=\"../results/_1_gan/images/gan_data_evolution_unstable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 2\" style=\"width: 50%; padding-left: 5px;\" />\n",
    "</div>\n",
    "\n",
    "## Fronteira\n",
    "\n",
    "Caso estável: Fronteira adaptável. O Discriminador cria uma fronteira de decisão complexa (linha preta) que se adapta perfeitamente à distribuição real. Na época final (100%), a região classificada como \"\"real\"\" (azul claro) envolve a curva de dados, indicando que G está gerando dados indistinguíveis\n",
    "\n",
    "Caso instável: Fronteira \"\"tentativa-e-erro\"\". A linha de decisão (preta) é extremamente irregular e instável, indicando que o Discriminador nunca se acalma e está constantemente aprendendo e esquecendo a fronteira correta devido à natureza volátil dos dados gerados\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"../results/_1_gan/images/gan_boundary_evolution_stable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 1\" style=\"width: 50%; padding-right: 5px;\" />\n",
    "    <img src=\"../results/_1_gan/images/gan_boundary_evolution_unstable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 2\" style=\"width: 50%; padding-left: 5px;\" />\n",
    "</div>\n",
    "\n",
    "## KDE\n",
    "\n",
    "Caso estável: Distribuições alinhadas. A densidade dos dados gerados (vermelho) cobre e replica a densidade dos dados reais (azul), confirmando que a distribuição de probabilidade foi aprendida\n",
    "\n",
    "Caso instável: Dificuldade na Convergência. A sobreposição das densidades é boa, mas em 75% e 100%, a densidade do Gerador (vermelho) parece menos suave e ligeiramente deslocada da densidade real (azul) na extremidade esquerda, refletindo a instabilidade e o ruído\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"../results/_1_gan/images/gan_kde_evolution_stable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 1\" style=\"width: 50%; padding-right: 5px;\" />\n",
    "    <img src=\"../results/_1_gan/images/gan_kde_evolution_unstable.png\" alt=\"Gráfico de Perdas do GAN - Imagem 2\" style=\"width: 50%; padding-left: 5px;\" />\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
