{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8cc6c0",
   "metadata": {},
   "source": [
    "# Demonstração de Overfitting vs Dropout Regularization\n",
    "\n",
    "Este notebook demonstra o efeito do overfitting em redes neurais profundas e como a técnica de **Dropout** pode mitigar esse problema.\n",
    "\n",
    "## Objetivo\n",
    "Treinar duas redes neurais idênticas no dataset MNIST:\n",
    "1. **Sem Dropout**: Propenso a overfitting\n",
    "2. **Com Dropout**: Regularização para melhor generalização\n",
    "\n",
    "## Configuração\n",
    "- Dataset: MNIST (subset de 1000 amostras de treino)\n",
    "- Arquitetura: 9 camadas densas (2048→1024→1024→512→512→256→256→128→64→10)\n",
    "- Dropout rate: 0.3\n",
    "- Épocas: 100\n",
    "- Batch size: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64968e9",
   "metadata": {},
   "source": [
    "## 1. Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Devices: {tf.config.list_physical_devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e3315",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Preparação dos Dados\n",
    "\n",
    "Carregamos o dataset MNIST e criamos um **subset pequeno** de apenas 1000 amostras de treino para facilitar o overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar MNIST\n",
    "(x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizar para [0, 1]\n",
    "x_train_full = x_train_full.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten: 28x28 -> 784\n",
    "x_train_full = x_train_full.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# Criar subset pequeno para facilitar overfitting\n",
    "subset_size = 1000\n",
    "x_train_subset, _, y_train_subset, _ = train_test_split(\n",
    "    x_train_full[:subset_size], \n",
    "    y_train_full[:subset_size], \n",
    "    test_size=0.0, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training subset: {x_train_subset.shape}, {y_train_subset.shape}\")\n",
    "print(f\"Test set: {x_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2d5ec",
   "metadata": {},
   "source": [
    "## 3. Definição dos Modelos\n",
    "\n",
    "Criamos duas arquiteturas **idênticas**:\n",
    "- **Modelo 1**: Sem Dropout (propenso a overfitting)\n",
    "- **Modelo 2**: Com Dropout (rate=0.2) para regularização\n",
    "\n",
    "Ambos usam 9 camadas densas com ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_without_dropout():\n",
    "    \"\"\"Modelo sem Dropout - propenso a overfitting\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(784,)),\n",
    "        Dense(2048, activation='relu'),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_model_with_dropout():\n",
    "    \"\"\"Modelo com Dropout - regularização para melhor generalização\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(784,)),\n",
    "        Dense(2048, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Modelos definidos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ac1e5",
   "metadata": {},
   "source": [
    "## 4. Treinamento dos Modelos\n",
    "\n",
    "Treinamos ambos os modelos por 50 épocas e coletamos as métricas de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe041ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENTO 1: Modelo SEM Dropout\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_no_dropout = create_model_without_dropout()\n",
    "history_no_dropout = model_no_dropout.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENTO 2: Modelo COM Dropout\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_with_dropout = create_model_with_dropout()\n",
    "history_with_dropout = model_with_dropout.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df0425",
   "metadata": {},
   "source": [
    "## 5. Visualização dos Resultados\n",
    "\n",
    "Criamos 5 gráficos para analisar o desempenho:\n",
    "1. Loss do modelo sem dropout\n",
    "2. Acurácia do modelo sem dropout\n",
    "3. Loss do modelo com dropout\n",
    "4. Acurácia do modelo com dropout\n",
    "5. Comparação final das métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair métricas finais\n",
    "train_acc_no_dropout = history_no_dropout.history['accuracy'][-1]\n",
    "test_acc_no_dropout = history_no_dropout.history['val_accuracy'][-1]\n",
    "train_loss_no_dropout = history_no_dropout.history['loss'][-1]\n",
    "test_loss_no_dropout = history_no_dropout.history['val_loss'][-1]\n",
    "\n",
    "train_acc_with_dropout = history_with_dropout.history['accuracy'][-1]\n",
    "test_acc_with_dropout = history_with_dropout.history['val_accuracy'][-1]\n",
    "train_loss_with_dropout = history_with_dropout.history['loss'][-1]\n",
    "test_loss_with_dropout = history_with_dropout.history['val_loss'][-1]\n",
    "\n",
    "# Criar visualizações\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Loss sem dropout\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.plot(history_no_dropout.history['loss'], label='Treino', linewidth=2)\n",
    "ax1.plot(history_no_dropout.history['val_loss'], label='Teste', linewidth=2)\n",
    "ax1.set_xlabel('Época', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Experimento 1: Loss (Sem Dropout)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy sem dropout\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax2.plot(history_no_dropout.history['accuracy'], label='Treino', linewidth=2)\n",
    "ax2.plot(history_no_dropout.history['val_accuracy'], label='Teste', linewidth=2)\n",
    "ax2.set_xlabel('Época', fontsize=12)\n",
    "ax2.set_ylabel('Acurácia', fontsize=12)\n",
    "ax2.set_title('Experimento 1: Acurácia (Sem Dropout)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss com dropout\n",
    "ax3 = plt.subplot(2, 3, 4)\n",
    "ax3.plot(history_with_dropout.history['loss'], label='Treino', linewidth=2)\n",
    "ax3.plot(history_with_dropout.history['val_loss'], label='Teste', linewidth=2)\n",
    "ax3.set_xlabel('Época', fontsize=12)\n",
    "ax3.set_ylabel('Loss', fontsize=12)\n",
    "ax3.set_title('Experimento 2: Loss (Com Dropout)', fontsize=14, fontweight='bold')\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Accuracy com dropout\n",
    "ax4 = plt.subplot(2, 3, 5)\n",
    "ax4.plot(history_with_dropout.history['accuracy'], label='Treino', linewidth=2)\n",
    "ax4.plot(history_with_dropout.history['val_accuracy'], label='Teste', linewidth=2)\n",
    "ax4.set_xlabel('Época', fontsize=12)\n",
    "ax4.set_ylabel('Acurácia', fontsize=12)\n",
    "ax4.set_title('Experimento 2: Acurácia (Com Dropout)', fontsize=14, fontweight='bold')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Comparação final\n",
    "ax5 = plt.subplot(1, 3, 3)\n",
    "x_pos = np.arange(4)\n",
    "width = 0.35\n",
    "\n",
    "bars_no_dropout = [train_acc_no_dropout, test_acc_no_dropout, \n",
    "                   train_loss_no_dropout, test_loss_no_dropout]\n",
    "bars_with_dropout = [train_acc_with_dropout, test_acc_with_dropout, \n",
    "                     train_loss_with_dropout, test_loss_with_dropout]\n",
    "\n",
    "bar1 = ax5.bar(x_pos - width/2, bars_no_dropout, width, \n",
    "               label='Sem Dropout', alpha=0.8, color='#e74c3c')\n",
    "bar2 = ax5.bar(x_pos + width/2, bars_with_dropout, width, \n",
    "               label='Com Dropout', alpha=0.8, color='#3498db')\n",
    "\n",
    "ax5.set_xlabel('Métrica', fontsize=12)\n",
    "ax5.set_ylabel('Valor', fontsize=12)\n",
    "ax5.set_title('Comparação Final das Métricas', fontsize=14, fontweight='bold')\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(['Acc Treino', 'Acc Teste', 'Loss Treino', 'Loss Teste'], \n",
    "                     rotation=15, ha='right')\n",
    "ax5.legend(fontsize=11)\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bars in [bar1, bar2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('overfitting_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTADOS FINAIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSEM DROPOUT:\")\n",
    "print(f\"  Acurácia Treino: {train_acc_no_dropout:.3f}\")\n",
    "print(f\"  Acurácia Teste:  {test_acc_no_dropout:.3f}\")\n",
    "print(f\"  Gap (Overfitting): {train_acc_no_dropout - test_acc_no_dropout:.3f}\")\n",
    "print(f\"  Loss Treino: {train_loss_no_dropout:.3f}\")\n",
    "print(f\"  Loss Teste:  {test_loss_no_dropout:.3f}\")\n",
    "\n",
    "print(f\"\\nCOM DROPOUT:\")\n",
    "print(f\"  Acurácia Treino: {train_acc_with_dropout:.3f}\")\n",
    "print(f\"  Acurácia Teste:  {test_acc_with_dropout:.3f}\")\n",
    "print(f\"  Gap (Overfitting): {train_acc_with_dropout - test_acc_with_dropout:.3f}\")\n",
    "print(f\"  Loss Treino: {train_loss_with_dropout:.3f}\")\n",
    "print(f\"  Loss Teste:  {test_loss_with_dropout:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANÁLISE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Redução no Gap de Acurácia: \"\n",
    "      f\"{((train_acc_no_dropout - test_acc_no_dropout) - (train_acc_with_dropout - test_acc_with_dropout)):.3f}\")\n",
    "print(f\"Melhoria na Acurácia de Teste: \"\n",
    "      f\"{(test_acc_with_dropout - test_acc_no_dropout):.3f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78b229",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "\n",
    "Este experimento demonstra claramente o fenômeno de **overfitting** e como o **Dropout** pode mitigá-lo:\n",
    "\n",
    "### Observações Principais:\n",
    "1. **Overfitting Sem Dropout**: O modelo sem dropout alcança alta acurácia no conjunto de treino, mas desempenho significativamente inferior no conjunto de teste (gap grande)\n",
    "2. **Regularização com Dropout**: O modelo com dropout apresenta menor gap entre treino e teste, indicando melhor generalização\n",
    "3. **Trade-off**: O dropout pode reduzir ligeiramente a acurácia de treino, mas melhora a capacidade de generalização\n",
    "\n",
    "### Métricas-Chave:\n",
    "- **Gap de Acurácia**: Diferença entre acurácia de treino e teste (quanto menor, melhor)\n",
    "- **Acurácia de Teste**: Métrica mais importante para avaliar generalização\n",
    "- **Loss**: Comportamento da função de perda ao longo das épocas\n",
    "\n",
    "### Próximos Passos:\n",
    "- Experimentar diferentes dropout rates (0.1, 0.3, 0.5)\n",
    "- Adicionar outras técnicas de regularização (L1, L2, Batch Normalization)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
