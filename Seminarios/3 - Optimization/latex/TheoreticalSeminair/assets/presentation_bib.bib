@misc{gradientDescent,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.04747}, 
}

@misc{kingma2014adam,
  title        = {Adam: A Method for Stochastic Optimization},
  author       = {Diederik P. Kingma and Jimmy Ba},
  year         = {2014},
  eprint       = {1412.6980},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/1412.6980}
}

@misc{loshchilov2017decoupled,
  title        = {Decoupled Weight Decay Regularization},
  author       = {Ilya Loshchilov and Frank Hutter},
  year         = {2017},
  eprint       = {1711.05101},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/1711.05101}
}

@misc{reddi2019amsgrad,
  title        = {On the Convergence of Adam and Beyond},
  author       = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  year         = {2019},
  eprint       = {1904.09237},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/1904.09237}
}

@misc{liu2019radam,
  title        = {On the Variance of the Adaptive Learning Rate and Beyond},
  author       = {Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
  year         = {2019},
  eprint       = {1908.03265},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/1908.03265}
}

@misc{xie2022adan,
  title        = {Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models},
  author       = {Junxiong Xie and Xiaoxia Pan and Zhuoran Yu and Yue Wu},
  year         = {2022},
  eprint       = {2208.06677},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2208.06677}
}

@misc{chen2023lion,
  title        = {Symbolic Discovery of Optimization Algorithms},
  author       = {Zhuang Chen and Chen Sun and Dhruv Tirumala and Pieter Abbeel and Igor Mordatch and Deepak Pathak},
  year         = {2023},
  eprint       = {2302.06675},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2302.06675}
}

@misc{ding2023adamfamily,
  title        = {Adam-family Methods with Decoupled Weight Decay in Deep Learning},
  author       = {Kai Ding and Nan Xiao and Kim-Chuan Toh},
  year         = {2023},
  eprint       = {2310.08858},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2310.08858}
}

@misc{wang2024adamwdecay,
  title        = {How to Set AdamW's Weight Decay as You Scale Model and Dataset Size},
  author       = {Xingyu Wang and Laurence Aitchison},
  year         = {2024},
  eprint       = {2405.13698},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2405.13698}
}

@manual{pytorchdocs,
  title        = {PyTorch Documentation},
  author       = {{PyTorch Core Team}},
  year         = {2025},
  url          = {https://docs.pytorch.org/docs/main/optim.html},
  note         = {Accessed: October 2025}
}
@misc{lu2024gradientdescentstochasticoptimization,
      title={Gradient Descent, Stochastic Optimization, and Other Tales}, 
      author={Jun Lu},
      year={2024},
      eprint={2205.00832},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.00832}, 
}
@misc{chandra2022gradientdescentultimateoptimizer,
      title={Gradient Descent: The Ultimate Optimizer}, 
      author={Kartik Chandra and Audrey Xie and Jonathan Ragan-Kelley and Erik Meijer},
      year={2022},
      eprint={1909.13371},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.13371}, 
}
@misc{ruder2017overviewgradientdescentoptimization,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.04747}, 
}
@misc{bottou1998online,
  title={Online algorithms and stochastic approximations},
  author={Bottou, L{\'e}on},
  journal={Online learning in neural networks},
  year={1998},
  publisher={Cambridge University Press}
}
@misc{khirirat2017mini,
  title={Mini-batch gradient descent: Faster convergence under data sparsity},
  author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  booktitle={2017 IEEE 56th Annual Conference on Decision and Control (CDC)},
  pages={2880--2887},
  year={2017},
  organization={IEEE}
}
@misc{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
  pages={177--186},
  year={2010},
  organization={Springer}
}
@misc{jothimurugesan2018variance,
  title={Variance-reduced stochastic gradient descent on streaming data},
  author={Jothimurugesan, Ellango and Tahmasbi, Ashraf and Gibbons, Phillip and Tirthapura, Srikanta},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{lei2019stochastic,
  title={Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
  author={Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={10},
  pages={4394--4400},
  year={2019},
  publisher={IEEE}
}

@book{boyd2004convex,
  title     = {Convex Optimization},
  author    = {Boyd, Stephen and Vandenberghe, Lieven},
  year      = {2004},
  publisher = {Cambridge University Press}
}

@book{goodfellow2016deep,
  title     = {Deep Learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  publisher = {MIT Press}
}

@misc{newton-imagem,
  author       = {Umam, Ardian},
  title        = {Newtonâ€™s method optimization: Derivation and how it works},
  year         = {2017},
  howpublished = {\url{https://ardianumam.wordpress.com/2017/09/27/newtons-method-optimization-derivation-and-how-it-works/}},
  note         = {Accessed: 21 October 2025},
  journal      = {Ardian Umam Blog}
}

@manual{lbfgs,
  title        = {LBFGS - PyTorch Documentation},
  author       = {{PyTorch Core Team}},
  year         = {2025},
  url          = {https://docs.pytorch.org/docs/main/generated/torch.optim.lbfgs.LBFGS.html},
  note         = {Accessed: October 2025}
}

@inproceedings{martens2010deep,
  title={Deep learning via hessian-free optimization.},
  author={Martens, James and others},
  booktitle={Icml},
  volume={27},
  pages={735--742},
  year={2010}
}

@inproceedings{ba2017distributed,
  title={Distributed second-order optimization using kronecker-factored approximations},
  author={Ba, Jimmy and Grosse, Roger and Martens, James},
  booktitle={International conference on learning representations},
  year={2017}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{erd1970new,
  title={On a new law of large numbers},
  author={Erd, Paul},
  journal={J. Anal. Muth},
  volume={22},
  pages={103--l},
  year={1970}
}