# Experimento: SAE

Este diret√≥rio cont√©m o script principal para executar o pipeline completo de treinamento e avalia√ß√£o para a arquitetura **SAE** no dataset de Tuberculose.

## ‚öôÔ∏è Par√¢metros da Arquitetura (config.yaml)

Este modelo √© configurado dinamicamente a partir do arquivo `config.yaml` localizado na raiz do projeto. Os par√¢metros espec√≠ficos para esta arquitetura, encontrados sob a chave `architectures:`, s√£o:

```yaml
# ===================================================================
# 1. PAR√ÇMETROS DE DADOS E VALIDA√á√ÉO
# ===================================================================
dataset:
  raw_file_name: "Obfuscated-MalMem2022.csv"
  random_seed: 117

cross_validation:
  n_splits: 10 
  test_size: 0.2
  
# ===================================================================
# 2. PAR√ÇMETROS DE TREINAMENTO
# ===================================================================
training:
  optimizer: 'Adam'
  learning_rate: 0.00005
  weight_decay: 0.0001
  batch_size: 64
  epochs: 500
  early_stopping_patience: 25
  dropout_rate: 0.5

# ===================================================================
# 3. ARQUITETURAS DOS MODELOS
# ===================================================================
architectures:

  cnn:
    dropout_rate: 0.25
    cnn_channels: [3, 16, 32,64,128] # (Entrada, Conv1, Conv2, Conv3, Conv4)
    kernel_size: 3

  DeepNN_MLP: 
    dropout_rate: 0.5 
    hidden_layers: [512, 256, 128, 64] 

  Autoencoder_SAE: 
    pretrain_epochs: 25
    

  DeepNN_SAE_Classifier:
    dropout_rate: 0.5

  DBN_RBM: 
    pretrain_epochs: 50
    pretrain_lr: 0.001
    dropout_rate: 0.5
  
```

## üöÄ Como Executar
Este script foi projetado para ser executado a partir do diret√≥rio raiz do projeto, para que todos os imports de m√≥dulos (`modules/`, `dataloaders/`, `models/`) funcionem corretamente.

1. Verifique a Configura√ß√£o:

Antes de executar, confirme se os par√¢metros da arquitetura (acima) e, :

2. Execute o Script:

A partir do diret√≥rio raiz do projeto, execute o seguinte comando:

```Bash
python src/experiments/CIC_MalMem_2022_SAE/run_experiment.py
```


## üî¨ O que este script faz?
O `run_experiment.py` automatiza todo o pipeline de avalia√ß√£o robusta que definimos:

Carrega as configura√ß√µes do `config.yaml`.

Separa um conjunto de teste final (Hold-Out) estratificado e) do restante dos dados.

Executa uma Valida√ß√£o Cruzada de K-Folds (K=10) no restante dos dados (conjunto de Desenvolvimento).

Para cada fold:

Treina o modelo `SAE`.

Usa `early_stopping_patience` para salvar o melhor checkpoint com base na perda de valida√ß√£o.

Avalia o melhor modelo do fold no conjunto de valida√ß√£o com base na acur√°cia.

Ao final dos K-folds, ele seleciona o "modelo campe√£o" (o modelo do fold com a maior acur√°cia).

Realiza uma avalia√ß√£o final, √∫nica e imparcial deste modelo campe√£o no conjunto Hold-Out.

## üìä Sa√≠das (Resultados)
Todos os artefatos deste experimento ser√£o salvos na pasta raiz `results/` em um diret√≥rio √∫nico com timestamp, seguindo o padr√£o:

`results/CIC_MALMEM_2022_SAE/[YYYYMMDD_HHMMSS]/`

Este diret√≥rio conter√°:

Subpastas para cada `fold_...` com logs e gr√°ficos de perda.

A pasta `holdout_results/` com os gr√°ficos ROC finais.

O modelo campe√£o salvo: `best_overall_model.pt`.

O resumo completo das m√©tricas (com dados brutos dos folds): `summary_results.json.`